<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hypothesis Testing</title>
    <url>/2020/03/04/2020-3-3-Hypothesis_Testing-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="Hypothesis-Testing"><a href="#Hypothesis-Testing" class="headerlink" title="Hypothesis Testing"></a>Hypothesis Testing</h1><p>是推断统计的最后一步，是根据一定的假设条件由假设条件由样本推断总体的一种方法</p>
<p>首先提出你的假设</p>
<p>其实检验假设其实就是假设和检验两步，先提出假设，之后在验证假设时候合理</p>
<p>4、显著水平<br>总共猜10次，那么是出现7次猜对，可以认为有特殊能力，还是9次猜对之后我才能确认有特殊能力，这是一个较为主观的标准。</p>
<p>我们一般认为</p>
<p>P-value&lt;=0.05</p>
<p>就可以认为假设是不正确的。</p>
<p>0.05这个标准就是显著水平，当然选择多少作为显著水平也是主观的。</p>
<p>比如，我们猜奶茶的例子，如果取单侧P值，那么根据我们的计算，如果10次猜对9次：</p>
<h2 id="P-value-P-9-lt-X-lt-10-0-01-lt-0-05"><a href="#P-value-P-9-lt-X-lt-10-0-01-lt-0-05" class="headerlink" title="P-value=P(9&lt;=X&lt;=10)=0.01&lt;=0.05"></a>P-value=P(9&lt;=X&lt;=10)=0.01&lt;=0.05</h2><p><strong>检验统计量</strong>是用于假设检验计算的统计量。在零假设情况下，这项统计量服从一个给定的概率分布，而这在另一种假设下则不然。从而若检验统计量的值落在上述分布的临界值之外，则可认为前述零假设未必正确。统计学中，用于检验假设量是否正确的量。常用的检验统计量有t统计量，Z统计量等。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>数理统计</category>
      </categories>
      <tags>
        <tag>Hypothesis Testing</tag>
      </tags>
  </entry>
  <entry>
    <title>Linear Regression for Classification</title>
    <url>/2020/03/04/2018-12-10-linear_regression-2018/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="多元线性回归问题"><a href="#多元线性回归问题" class="headerlink" title="多元线性回归问题"></a>多元线性回归问题</h1><ol>
<li><p>代价函数(整体数据集)，损失函数(对于每个数据的误差)</p>
</li>
<li><p>learning_rate(从小的尝试比如0.0006)</p>
</li>
<li><p><strong>scaling the features(特征缩放)—适用与梯度下降改进 -Mean normalization(均值归一化)</strong></p>
<ol>
<li>min-max标准化</li>
</ol>
<p>X(每个数据) - U(特征值均值))/(Max-Min)</p>
<ol>
<li>Z-score标准化方法</li>
</ol>
<p>X(每个数据) - U(特征值均值))/(std)</p>
<ol>
<li>归一化</li>
</ol>
<p>把数据变成(0,1)或者(1,1)之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速 </p>
</li>
<li><p><strong> normal equation(正规方程) or Gradient Descent </strong></p>
<ol>
<li>normal equation is suitable for samll features(计算量比较大)</li>
<li>Gradient descent is suitable for a large number for features(<br>  <img src="/images/feature_scaling.png" alt="">)<br>但是受数据的影响较大，所以对与Gradient descent来说需要进行feature scaling)</li>
<li><p>normalize equation(正规方程)的公式推导</p>
<p>代价函数</p>
</li>
</ol>
<p>​    <script type="math/tex">J(\theta)=J(\theta_0,\ldots,\theta_n)=\frac {1} {2m} \sum_{i=1}^{m} {(h_\theta(x^{(i)})-y^{(i)})^2}</script></p>
<p>​    <script type="math/tex">J(\theta)=\frac {1}{2m}(X\theta-y)^T(X\theta-y)</script></p>
<p>​    <script type="math/tex">J(\theta)=\theta^TX^TX\theta-(X\theta)^Ty-y^TX\theta+y^Ty</script></p>
<p>​    <script type="math/tex">J(\theta)=\theta^TX^TX\theta-2(X\theta)^Ty+y^Ty</script></p>
<p>​    <script type="math/tex">\frac {\partial}{\partial\theta}J(\theta)=2X^TX\theta-2X^Ty=0</script></p>
<p>​    <script type="math/tex">\theta=(X^TX)^{-1}X^Ty</script></p>
</li>
<li><p>矩阵不可逆：</p>
<p>1.矩阵存在线性相关的特征之值</p>
<p>2.矩阵所对应的行列式为0</p>
</li>
<li><p>homework(python)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line">data = pd.read_table(<span class="string">"ex1data1.txt"</span>, header=<span class="literal">None</span>, delimiter=<span class="string">","</span>, encoding=<span class="string">"gb2312"</span>)</span><br><span class="line">m = len(data)</span><br><span class="line">data_x = np.array(data)[:, <span class="number">0</span>].reshape(m, <span class="number">1</span>)</span><br><span class="line">data_y = np.array(data)[:, <span class="number">1</span>].reshape(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the first task</span></span><br><span class="line">print(np.eye(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the second task</span></span><br><span class="line"><span class="comment"># plt.scatter(data_x, data_y, color='r', marker='x',label="Training Data")</span></span><br><span class="line"><span class="comment"># plt.ylabel('Profit in $10,000s')</span></span><br><span class="line"><span class="comment"># plt.xlabel('Population of City in 10,000s')</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the third task</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    inner = np.power(((X * theta) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner / (<span class="number">2</span> * len(X)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X,y,theta,alpha,epoch)</span>:</span></span><br><span class="line">    cost = np.zeros(epoch)</span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">        temp = theta - (alpha / m) * ((X * theta - y).T * X).T</span><br><span class="line">        theta = temp</span><br><span class="line">        cost[i] = computeCost(X,y,theta)</span><br><span class="line">    <span class="keyword">return</span>  theta,cost</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_equations</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    theta = np.linalg.inv(X.T*X)*X.T*y</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">X = np.matrix(np.concatenate((np.ones((m, <span class="number">1</span>)), data_x), axis=<span class="number">1</span>))</span><br><span class="line">y = np.matrix(data_y)</span><br><span class="line">theta = np.matrix(np.zeros([<span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">epoch = <span class="number">1000</span></span><br><span class="line">final_theta,cost = gradientDescent(X,y,theta,alpha,epoch)</span><br><span class="line">print(final_theta)</span><br><span class="line">final_theta = normal_equations(X,y)</span><br><span class="line">print(final_theta)</span><br><span class="line">x = np.array(list(data_x[:,<span class="number">0</span>]))</span><br><span class="line">f = final_theta[<span class="number">0</span>,<span class="number">0</span>] + final_theta[<span class="number">1</span>,<span class="number">0</span>] * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.scatter(data_x, data_y, color='r', marker='x',label="Training Data")</span></span><br><span class="line"><span class="comment"># plt.ylabel('Profit in $10,000s')</span></span><br><span class="line"><span class="comment"># plt.xlabel('Population of City in 10,000s')</span></span><br><span class="line"><span class="comment"># plt.plot(x,f,label="Prediction")</span></span><br><span class="line"><span class="comment"># plt.legend()</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.plot(np.arange(epoch),cost,'r')</span></span><br><span class="line"><span class="comment"># plt.xlabel("Iteration")</span></span><br><span class="line"><span class="comment"># plt.ylabel("Cost")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">theta0_vals = np.linspace(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">100</span>)</span><br><span class="line">theta1_vals = np.linspace(<span class="number">-1</span>,<span class="number">4</span>,<span class="number">100</span>)</span><br><span class="line">J_vals = np.zeros((len(theta0_vals),len(theta1_vals)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta0_vals)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(theta1_vals)):</span><br><span class="line">        t = np.matrix(np.array([theta0_vals[i],theta1_vals[j]]).reshape((<span class="number">2</span>,<span class="number">1</span>)))</span><br><span class="line">        J_vals[i,j] = computeCost(X,y,t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################## 3D图 #######################################</span></span><br><span class="line"><span class="comment"># fig = plt.figure()</span></span><br><span class="line"><span class="comment"># ax = Axes3D(fig)</span></span><br><span class="line"><span class="comment"># ax.plot_surface(theta0_vals,theta1_vals,J_vals,rstride=1,cstride=1,cmap="rainbow")</span></span><br><span class="line"><span class="comment"># plt.xlabel('theta_0')</span></span><br><span class="line"><span class="comment"># plt.ylabel('theta_1')</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment">######################################## 3D图 #######################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################## 等高线图 #######################################</span></span><br><span class="line"><span class="comment"># plt.figure()</span></span><br><span class="line"><span class="comment"># plt.contourf(theta0_vals, theta1_vals, J_vals, 20, alpha = 0.6, cmap = plt.cm.hot)</span></span><br><span class="line"><span class="comment"># a = plt.contour(theta0_vals, theta1_vals, J_vals, colors = 'black')</span></span><br><span class="line"><span class="comment"># plt.clabel(a,inline=1,fontsize=10)</span></span><br><span class="line"><span class="comment"># plt.plot(theta[0,0],theta[1,0],'r',marker='x')</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment">######################################## 等高线图 #######################################</span></span><br><span class="line"></span><br><span class="line">data2 = pd.read_table(<span class="string">"ex1data2.txt"</span>, header=<span class="literal">None</span>, delimiter=<span class="string">","</span>, encoding=<span class="string">"gb2312"</span>)</span><br><span class="line"></span><br><span class="line">data2 = (data2 - data2.mean()) / data2.std()</span><br><span class="line">m = len(data2)</span><br><span class="line">data_x = np.array(data2)[:, <span class="number">0</span>:<span class="number">2</span>].reshape(m, <span class="number">2</span>)</span><br><span class="line">data_y = np.array(data2)[:, <span class="number">2</span>].reshape(m, <span class="number">1</span>)</span><br><span class="line">data_x = np.concatenate((np.zeros((m,<span class="number">1</span>)),data_x),axis=<span class="number">1</span>)</span><br><span class="line">X = np.matrix(data_x)</span><br><span class="line">y = np.matrix(data_y)</span><br><span class="line">theta = np.matrix(np.zeros((<span class="number">3</span>,<span class="number">1</span>)))</span><br><span class="line">final_theta,cost = gradientDescent(X,y,theta,alpha,epoch)</span><br><span class="line"></span><br><span class="line">plt.plot(np.arange(epoch),cost,<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>multi-regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Classifition metrics</title>
    <url>/2020/03/04/2020-3-3-classifition_metrics-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="Classifition-metrics"><a href="#Classifition-metrics" class="headerlink" title="Classifition metrics"></a>Classifition metrics</h1><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">confusion_matrix(y_train_5, y_train_pred)</span><br><span class="line">array([[<span class="number">53272</span>, <span class="number">1307</span>],</span><br><span class="line">        [ <span class="number">1077</span>, <span class="number">4344</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="准确率和召回率"><a href="#准确率和召回率" class="headerlink" title="准确率和召回率"></a>准确率和召回率</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>precision_score(y_train_5, y_pred) <span class="comment"># == 4344 / (4344 + 1307)</span></span><br><span class="line"><span class="number">0.76871350203503808</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>recall_score(y_train_5, y_train_pred) <span class="comment"># == 4344 / (4344 + 1077)</span></span><br><span class="line"><span class="number">0.7913669064748201</span></span><br></pre></td></tr></table></figure>
<h3 id="F1-score"><a href="#F1-score" class="headerlink" title="F1 score"></a>F1 score</h3><p>通常结合准确率和召回率会更加方便，这个指标叫做“F1 值”，特别是当你需要一个简单的方法去比较两个分类器的优劣的时候。F1 值是准确率和召回率的调和平均。普通的平均值平等地看待所有的值，而调和平均会给小的值更大的权重。所以，要想分类器得到一个高的 F1 值，需要召回率和准确率同时高。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f1_score(y_train_5, y_train_pred)</span><br><span class="line"><span class="number">0.78468208092485547</span></span><br></pre></td></tr></table></figure>
<h3 id="准确率和召回率的关系"><a href="#准确率和召回率的关系" class="headerlink" title="准确率和召回率的关系"></a>准确率和召回率的关系</h3><p>Scikit-Learn 不让你直接设置阈值，但是它给你提供了设置决策分数的方法，这个决策分数可以用来产生预测。它不是调用分类器的<code>predict()</code>方法，而是调用<code>decision_function()</code>方法。这个方法返回每一个样例的分数值，然后基于这个分数值，使用你想要的任何阈值做出预测</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt; y_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_scores</span><br><span class="line">array([ <span class="number">161855.74572176</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>threshold = <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_some_digit_pred = (y_scores &gt; threshold)</span><br><span class="line">array([ <span class="literal">True</span>], dtype=bool)</span><br><span class="line"><span class="string">"""SGDClassifier用了一个等于 0 的阈值，所以前面的代码返回了跟predict()方法一样的结果（都返回了true）。让我们提高这个阈值"""</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>threshold = <span class="number">200000</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_some_digit_pred = (y_scores &gt; threshold)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_some_digit_pred</span><br><span class="line">array([<span class="literal">False</span>], dtype=bool)</span><br><span class="line"></span><br><span class="line"><span class="string">"""使用cross_val_predict()得到每一个样例的分数值，但是这一次指定返回一个决策分数，而不是预测值。"""</span></span><br><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, </span><br><span class="line">                            method=<span class="string">"decision_function"</span>)</span><br><span class="line"><span class="comment"># y_scores返回和样本数量相同的决策分数</span></span><br><span class="line"><span class="keyword">if</span> y_scores.ndim == <span class="number">2</span>:</span><br><span class="line">    y_scores = y_scores[:, <span class="number">1</span>]</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line"></span><br><span class="line">precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)</span><br><span class="line"><span class="comment"># 绘制precisoin和recall曲线</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_recall_vs_threshold</span><span class="params">(precisions, recalls, thresholds)</span>:</span></span><br><span class="line">    plt.plot(thresholds, precisions[:<span class="number">-1</span>], <span class="string">"b--"</span>, label=<span class="string">"Precision"</span>)</span><br><span class="line">    plt.plot(thresholds, recalls[:<span class="number">-1</span>], <span class="string">"g-"</span>, label=<span class="string">"Recall"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Threshold"</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">"upper left"</span>)</span><br><span class="line">    plt.ylim([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plot_precision_recall_vs_threshold(precisions, recalls, thresholds)</span><br><span class="line">plt.show()    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_vs_recall</span><span class="params">(precisions, recalls)</span>:</span></span><br><span class="line">    plt.plot(recalls, precisions, <span class="string">"b-"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Recall"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Precision"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plot_precision_vs_recall(precisions, recalls)</span><br><span class="line">save_fig(<span class="string">"precision_vs_recall_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://hand2st.apachecn.org/images/chapter_3/chapter3.4.jpeg" alt="图3-4 准确率和召回率对决策阈值"></p>
<p><img src="https://hand2st.apachecn.org/images/chapter_3/chapter3.5.jpeg" alt="图3-5 准确率对召回率"></p>
<h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>受试者工作特征（ROC）曲线是另一个二分类器常用的工具。它非常类似与准确率/召回率曲线，但不是画出准确率对召回率的曲线，ROC 曲线是真正例率（true positive rate，另一个名字叫做召回率）对假正例率（false positive rate, FPR）的曲线。FPR 是反例被错误分成正例的比率。它等于 1 减去真反例率（true negative rate， TNR）。TNR是反例被正确分类的比率。TNR也叫做特异性。所以 ROC 曲线画出召回率对（1 减特异性）的曲线。</p>
<p><img src="https://hand2st.apachecn.org/images/chapter_3/chapter3.6.jpeg" alt="图3-6 ROC曲线"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line"><span class="string">"""使用cross_val_predict()得到每一个样例的分数值，但是这一次指定返回一个决策分数，而不是预测值。"""</span></span><br><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, </span><br><span class="line">                            method=<span class="string">"decision_function"</span>)</span><br><span class="line"><span class="comment"># y_scores返回和样本数量相同的决策分数</span></span><br><span class="line"><span class="keyword">if</span> y_scores.ndim == <span class="number">2</span>:</span><br><span class="line">    y_scores = y_scores[:, <span class="number">1</span>]</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_roc_curve</span><span class="params">(fpr, tpr, label=None)</span>:</span></span><br><span class="line">    plt.plot(fpr, tpr, linewidth=<span class="number">2</span>, label=label)</span><br><span class="line">    plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'k--'</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>)</span><br><span class="line">plot_roc_curve(fpr, tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>这里同样存在折衷的问题：召回率（TPR）越高，分类器就会产生越多的假正例（FPR）。图中的点线是一个完全随机的分类器生成的 ROC 曲线；一个好的分类器的 ROC 曲线应该尽可能远离这条线（即向左上角方向靠拢）。</p>
<p>一个比较分类器之间优劣的方法是：测量ROC曲线下的面积（AUC）。一个完美的分类器的 ROC AUC 等于 1，而一个纯随机分类器的 ROC AUC 等于 0.5。Scikit-Learn 提供了一个函数来计算 ROC AUC：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>roc_auc_score(y_train_5, y_scores)</span><br><span class="line"><span class="number">0.97061072797174941</span></span><br></pre></td></tr></table></figure>
<p>因为 ROC 曲线跟准确率/召回率曲线（或者叫 PR）很类似，你或许会好奇如何决定使用哪一个曲线呢？一个笨拙的规则是，优先使用 PR 曲线当正例很少，或者当你关注假正例多于假反例的时候。其他情况使用 ROC 曲线。举例子，回顾前面的 ROC 曲线和 ROC AUC 数值，你或许认为这个分类器很棒。但是这几乎全是因为只有少数正例（“是 5”），而大部分是反例（“非 5”）。相反，PR 曲线清楚显示出这个分类器还有很大的改善空间（PR 曲线应该尽可能地靠近右上角）。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machin Learning metrics</tag>
      </tags>
  </entry>
  <entry>
    <title>损失函数（loss function）</title>
    <url>/2020/03/04/2020-3-3-cost_function-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="损失函数（loss-function）"><a href="#损失函数（loss-function）" class="headerlink" title="损失函数（loss function）"></a>损失函数（loss function）</h1><p>损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是<strong>经验风险函数</strong>的核心部分，也是<strong>结构风险函数</strong>重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子：</p>
<script type="math/tex; mode=display">
\theta^* = \arg \min_\theta \frac{1}{N}{}\sum_{i=1}^{N} L(y_i, f(x_i; \theta)) + \lambda\  \Phi(\theta)</script><p>其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的Φ是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是L1，也可以是L2，或者其他的正则函数。整个式子表示的意思是<strong>找到使目标函数最小时的θθ值</strong>。下面主要列出几种常见的损失函数。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title>Cross Validation</title>
    <url>/2020/03/04/2020-3-3-cross_valadation_sklearn-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h1><h2 id="有关交叉验证的sklearn的方法和原理"><a href="#有关交叉验证的sklearn的方法和原理" class="headerlink" title="有关交叉验证的sklearn的方法和原理"></a>有关交叉验证的sklearn的方法和原理</h2><h3 id="cross-val-score"><a href="#cross-val-score" class="headerlink" title="cross_val_score"></a>cross_val_score</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cross_val_score 用训练集来评估模型的好坏，注意交叉验证都是使用的是训练集来进行测验，而不是测试集</span></span><br><span class="line"><span class="comment"># 其返回的是，几折交叉验证就返回几个模型训练的评估分数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment"># 这里的model(),传入的是个模型实例，后传入的两个数据集合，cv表示交叉验证的次数，scoring表示评价方法</span></span><br><span class="line">cross_val_score(model(),X_train,y_train,cv=<span class="number">3</span>,scoring=<span class="string">"accuracy"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面则使用python实现一个cross_val_score函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklean.base <span class="keyword">import</span> clone</span><br><span class="line"><span class="comment"># 将数据集划分为几份</span></span><br><span class="line">skfolds = StratifiedKFold(n_splits=<span class="number">3</span>,random_state=<span class="number">42</span>)</span><br><span class="line"><span class="keyword">for</span> train_index,test_index <span class="keyword">in</span> skfolds.split(X_train,y_train):</span><br><span class="line">    <span class="comment"># 克隆原来模型</span></span><br><span class="line">    clone_clf = clone(model())</span><br><span class="line">    X_train_folds = X_train[trian_index]</span><br><span class="line">    y_train_folds = y_train[train_index]</span><br><span class="line">    X_test_folds = X_train[test_index]</span><br><span class="line">    y_test_folds= y_train[test_index]</span><br><span class="line">    clone_clf.fit(X_train_folds,y_train_folds)</span><br><span class="line">    y_pred = clone_clf.predict(X_test_folds)</span><br><span class="line">    n_correct = sum(y_pred == y_test_folds)</span><br><span class="line">    print(n_correct / len(y_pred) )</span><br></pre></td></tr></table></figure>
<h3 id="cross-val-predict"><a href="#cross-val-predict" class="headerlink" title="cross_val_predict"></a>cross_val_predict</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">就像 cross_val_score(),cross_val_predict()也使用 K 折交叉验证。它不是返回一个评估分数，而是返回基于每一个测试折做出的一个预测值。这意味着，对于每一个训练集的样例，你得到一个干净的预测（“干净”是说一个模型在训练过程当中没有用到测试集的数据）。"""</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict</span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>scikit-learn</tag>
      </tags>
  </entry>
  <entry>
    <title>Ensemble Learning</title>
    <url>/2020/03/04/2020-3-3-enseemble_learning-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="Ensemble-Learning"><a href="#Ensemble-Learning" class="headerlink" title="Ensemble Learning"></a>Ensemble Learning</h1><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h3 id="集成学习概述"><a href="#集成学习概述" class="headerlink" title="集成学习概述"></a>集成学习概述</h3><p>从下图，我们可以对集成学习的思想做一个概括。对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的</p>
<p>也就是说，集成学习有两个主要的问题需要解决，第一是如何得到若干个个体学习器，第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器</p>
<h3 id="集成学习之个体学习器"><a href="#集成学习之个体学习器" class="headerlink" title="集成学习之个体学习器"></a>集成学习之个体学习器</h3><p>上一节我们讲到，集成学习的第一个问题就是如何得到若干个个体学习器。这里我们有两种选择。</p>
<p>　　　　第一种就是所有的个体学习器都是一个种类的，或者说是同质的。比如都是决策树个体学习器，或者都是神经网络个体学习器。第二种是所有的个体学习器不全是一个种类的，或者说是异质的。比如我们有一个分类问题，对训练集采用支持向量机个体学习器，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。</p>
<p>　　　　目前来说，同质个体学习器的应用是最广泛的，一般我们常说的集成学习的方法都是指的同质个体学习器。而同质个体学习器使用最多的模型是CART决策树和神经网络。同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类，第一个是个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是boosting系列算法，第二个是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。下面就分别对这两类算法做一个概括总结。</p>
<h3 id="boostsing集成学习"><a href="#boostsing集成学习" class="headerlink" title="boostsing集成学习"></a>boostsing集成学习</h3><p>　从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p>
<p>Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)</p>
<p>这里对Adaboost算法的优缺点做一个总结。</p>
<p>Adaboost的主要优点有：</p>
<p>1）Adaboost作为分类器时，分类精度很高</p>
<p>2）在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。</p>
<p>3）作为简单的二元分类器时，构造简单，结果可理解。</p>
<p>4）不容易发生过拟合</p>
<p>Adaboost的主要缺点有：</p>
<p>1）对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</p>
<p>由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。</p>
<p>最后总结下GBDT的优缺点。</p>
<p>GBDT主要的优点有：</p>
<p>1) 可以灵活处理各种类型的数据，包括连续值和离散值。</p>
<p>2) 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。</p>
<p>3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</p>
<p>GBDT的主要缺点有：</p>
<p>1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</p>
<h3 id="集成学习之bagging"><a href="#集成学习之bagging" class="headerlink" title="集成学习之bagging"></a>集成学习之bagging</h3><p>Bagging的算法原理和 boosting不同，它的弱学习器之间没有依赖关系，可以并行生成，我们可以用一张图做一个概括如下：从上图可以看出，bagging的个体弱学习器的训练集是通过随机采样得到的。通过T次的随机采样，我们就可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。</p>
<p>　　　　对于这里的随机采样有必要做进一步的介绍，这里一般采用的是自助采样法（Bootstrap sampling）,即对于m个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。</p>
<p>　　　　随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。bagging和随机森林算法的原理在后面的文章中会专门来讲。</p>
<h3 id="集成学习之结合策略"><a href="#集成学习之结合策略" class="headerlink" title="集成学习之结合策略"></a>集成学习之结合策略</h3><ol>
<li><p>平均法 </p>
</li>
<li><p>投票法 （投票法也可以设置权重）</p>
</li>
<li><p>学习法 （代表的方法是stacking）当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。</p>
<p>　　　　在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Ensemble Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Cross Validation 详解</title>
    <url>/2020/03/04/2020-3-3-cross_validation-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="Cross-Validation-详解"><a href="#Cross-Validation-详解" class="headerlink" title="Cross Validation 详解"></a>Cross Validation 详解</h1><p><img src="https://github.com/LelandYan/lelandyan.github.io/raw/master/img/cross_validation.png" alt="Image text"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line"><span class="comment"># cv参数为交叉时候分成几份（k-folds）</span></span><br><span class="line">scores = cross_val_score(knn_clf,x_train,y_train,cv=<span class="number">10</span>)</span><br><span class="line">scores = np.mean(scores)</span><br></pre></td></tr></table></figure>
<h2 id="交叉验证的目的就是找到最好的超参数所以传入的数据只是train的数据"><a href="#交叉验证的目的就是找到最好的超参数所以传入的数据只是train的数据" class="headerlink" title="交叉验证的目的就是找到最好的超参数所以传入的数据只是train的数据"></a>交叉验证的目的就是找到最好的超参数所以传入的数据只是train的数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="comment"># 事实上网格搜受已经使用了交叉验证了</span></span><br><span class="line">para,_grid = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'weights'</span>:[<span class="string">'uniform'</span>],</span><br><span class="line">        <span class="string">'n_neighbors'</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>)]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'weights'</span>:[<span class="string">'distance'</span>],</span><br><span class="line">        <span class="string">'n_neighbors'</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>)],</span><br><span class="line">        <span class="string">'p'</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>)]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"><span class="comment"># n_jobs表示的处理的计算的核数,verbose可以在运行的程序的时候，可以显示信息</span></span><br><span class="line">knn_clf = KneighborsClassifier()</span><br><span class="line">grid_search = GridSearchCV(knn_clf,param_grid,n_jobs=<span class="number">-1</span>，verbose=<span class="number">2</span>)</span><br><span class="line">grid_search.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># 获取训练集的最好结果</span></span><br><span class="line">grid_search.best_score_</span><br><span class="line"><span class="comment"># 获取训练集的最好超参数</span></span><br><span class="line">grid_search.best_params_</span><br><span class="line"><span class="comment"># 获得最好的分类器</span></span><br><span class="line">grid_search.best_estimator_</span><br></pre></td></tr></table></figure>
<h3 id="留一法-LOO-CV-虽然准确，但是计算量巨大"><a href="#留一法-LOO-CV-虽然准确，但是计算量巨大" class="headerlink" title="留一法(LOO-CV) 虽然准确，但是计算量巨大"></a>留一法(LOO-CV) 虽然准确，但是计算量巨大</h3>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>scikit-learn</tag>
        <tag>cross validation</tag>
        <tag>validation</tag>
      </tags>
  </entry>
  <entry>
    <title>如何处理多分类问题</title>
    <url>/2020/03/04/2020-3-3-mult_classifition-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="如何处理多分类问题"><a href="#如何处理多分类问题" class="headerlink" title="如何处理多分类问题"></a>如何处理多分类问题</h1><h2 id="有关多分类问题有两种方法"><a href="#有关多分类问题有两种方法" class="headerlink" title="有关多分类问题有两种方法"></a>有关多分类问题有两种方法</h2><p>二分类器只能区分两个类，而多类分类器（也被叫做多项式分类器）可以区分多于两个类。</p>
<p>一些算法（比如随机森林分类器或者朴素贝叶斯分类器）可以直接处理多类分类问题。其他一些算法（比如 SVM 分类器或者线性分类器）则是严格的二分类器。然后，有许多策略可以让你用二分类器去执行多类分类。</p>
<p>举例子，创建一个可以将图片分成 10 类（从 0 到 9）的系统的一个方法是：训练10个二分类器，每一个对应一个数字（探测器 0，探测器 1，探测器 2，以此类推）。然后当你想对某张图片进行分类的时候，让每一个分类器对这个图片进行分类，选出决策分数最高的那个分类器。这叫做“一对所有”（OvA）策略（也被叫做“一对其他”）。</p>
<p>另一个策略是对每一对数字都训练一个二分类器：一个分类器用来处理数字 0 和数字 1，一个用来处理数字 0 和数字 2，一个用来处理数字 1 和 2，以此类推。这叫做“一对一”（OvO）策略。如果有 N 个类。你需要训练<code>N*(N-1)/2</code>个分类器。对于 MNIST 问题，需要训练 45 个二分类器！当你想对一张图片进行分类，你必须将这张图片跑在全部45个二分类器上。然后看哪个类胜出。OvO 策略的主要优点是：每个分类器只需要在训练集的部分数据上面进行训练。这部分数据是它所需要区分的那两个类对应的数据。</p>
<p>一些算法（比如 SVM 分类器）在训练集的大小上很难扩展，所以对于这些算法，OvO 是比较好的，因为它可以在小的数据集上面可以更多地训练，较之于巨大的数据集而言。但是，对于大部分的二分类器来说，OvA 是更好的选择。</p>
<p>Scikit-Learn 可以探测出你想使用一个二分类器去完成多分类的任务，它会自动地执行 OvA（除了 SVM 分类器，它使用 OvO）。让我们试一下<code>SGDClassifier</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.fit(X_train, y_train) <span class="comment"># y_train, not y_train_5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.predict([some_digit])</span><br><span class="line">array([ <span class="number">5.</span>])</span><br></pre></td></tr></table></figure>
<p>很容易。上面的代码在训练集上训练了一个<code>SGDClassifier</code>。这个分类器处理原始的目标class，从 0 到 9（<code>y_train</code>），而不是仅仅探测是否为 5 （<code>y_train_5</code>）。然后它做出一个判断（在这个案例下只有一个正确的数字）。在幕后，Scikit-Learn 实际上训练了 10 个二分类器，每个分类器都产到一张图片的决策数值，选择数值最高的那个类。</p>
<p>为了证明这是真实的，你可以调用<code>decision_function()</code>方法。不是返回每个样例的一个数值，而是返回 10 个数值，一个数值对应于一个类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_digit_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_digit_scores</span><br><span class="line">array([[<span class="number">-311402.62954431</span>, <span class="number">-363517.28355739</span>, <span class="number">-446449.5306454</span> ,</span><br><span class="line">        <span class="number">-183226.61023518</span>, <span class="number">-414337.15339485</span>, <span class="number">161855.74572176</span>,</span><br><span class="line">        <span class="number">-452576.39616343</span>, <span class="number">-471957.14962573</span>, <span class="number">-518542.33997148</span>,</span><br><span class="line">        <span class="number">-536774.63961222</span>]])</span><br></pre></td></tr></table></figure>
<p>最高数值是对应于类别 5 ：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.argmax(some_digit_scores)</span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.classes_</span><br><span class="line">array([ <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.classes_[<span class="number">5</span>]</span><br><span class="line"><span class="number">5.0</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>一个分类器被训练好了之后，它会保存目标类别列表到它的属性<code>classes_</code> 中去，按照值排序。在本例子当中，在<code>classes_</code> 数组当中的每个类的索引方便地匹配了类本身，比如，索引为 5 的类恰好是类别 5 本身。但通常不会这么幸运。</p>
</blockquote>
<p>如果你想强制 Scikit-Learn 使用 OvO 策略或者 OvA 策略，你可以使用<code>OneVsOneClassifier</code>类或者<code>OneVsRestClassifier</code>类。创建一个样例，传递一个二分类器给它的构造函数。举例子，下面的代码会创建一个多类分类器，使用 OvO 策略，基于<code>SGDClassifier</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsOneClassifier</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=<span class="number">42</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ovo_clf.fit(X_train, y_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ovo_clf.predict([some_digit])</span><br><span class="line">array([ <span class="number">5.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(ovo_clf.estimators_)</span><br><span class="line"><span class="number">45</span></span><br></pre></td></tr></table></figure>
<p>训练一个<code>RandomForestClassifier</code>同样简单：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_clf.fit(X_train, y_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_clf.predict([some_digit])</span><br><span class="line">array([ <span class="number">5.</span>])</span><br></pre></td></tr></table></figure>
<p>这次 Scikit-Learn 没有必要去运行 OvO 或者 OvA，因为随机森林分类器能够直接将一个样例分到多个类别。你可以调用<code>predict_proba()</code>，得到样例对应的类别的概率值的列表：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_clf.predict_proba([some_digit])</span><br><span class="line">array([[ <span class="number">0.1</span>, <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.1</span>, <span class="number">0.</span> , <span class="number">0.8</span>, <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> ]])</span><br></pre></td></tr></table></figure>
<p>你可以看到这个分类器相当确信它的预测：在数组的索引 5 上的 0.8，意味着这个模型以 80% 的概率估算这张图片代表数字 5。它也认为这个图片可能是数字 0 或者数字 3，分别都是 10% 的几率。</p>
<p>现在当然你想评估这些分类器。像平常一样，你想使用交叉验证。让我们用<code>cross_val_score()</code>来评估<code>SGDClassifier</code>的精度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(sgd_clf, X_train, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line">array([ <span class="number">0.84063187</span>, <span class="number">0.84899245</span>, <span class="number">0.86652998</span>])</span><br></pre></td></tr></table></figure>
<p>在所有测试折（test fold）上，它有 84% 的精度。如果你是用一个随机的分类器，你将会得到 10% 的正确率。所以这不是一个坏的分数，但是你可以做的更好。举例子，简单将输入正则化，将会提高精度到 90% 以上。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler = StandardScaler()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line">array([ <span class="number">0.91011798</span>, <span class="number">0.90874544</span>, <span class="number">0.906636</span> ])</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>scikit-learn</tag>
        <tag>multi-classifition</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Procedure</title>
    <url>/2020/03/04/2020-3-3-machine_learning_procedure-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="Machine-Learning-Procedure"><a href="#Machine-Learning-Procedure" class="headerlink" title="Machine Learning Procedure"></a>Machine Learning Procedure</h1><h2 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="数据预览"><a href="#数据预览" class="headerlink" title="数据预览"></a>数据预览</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = pd.DataFrame(X)</span><br><span class="line"><span class="comment"># 取前10行数据</span></span><br><span class="line">X.head(n=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 取数据中任意的10行</span></span><br><span class="line">X.sample(n=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 查看数据的大体数据分布以及大小</span></span><br><span class="line">X.describe()</span><br></pre></td></tr></table></figure>
<h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><p>通过箱线图可以发现异常值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.plot(kind=<span class="string">'box'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>通过条形图可以看出数据结构的分布特征，是否满足正太分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.hist(figsize=(<span class="number">12</span>,<span class="number">5</span>),xlabelsize=<span class="number">1</span>,ylabelsize=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>通过折线图可以看出数据值的大小密度的分布情况</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.plot(kind=<span class="string">"density"</span>,subplot=<span class="literal">True</span>,layout=(<span class="number">4</span>,<span class="number">4</span>),figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>通过特征相关图我们能够知道哪些特征存在明显的相关性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.scatter_matrix(X,figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>通过热力图可以更加清晰的看出各个特征之间的关系</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">cax = ax.matshow(X.corr(),vmin=<span class="number">-1</span>,vmax=<span class="number">1</span>,interploation=<span class="string">"none"</span>)</span><br><span class="line">fig.colorbar(cax)</span><br><span class="line"><span class="comment"># 这里为数据特征关联的分布大小</span></span><br><span class="line">ticks = np.arange(<span class="number">0</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">ax.set_xticks(ticks)</span><br><span class="line">ax.set_yticks(ticks)</span><br><span class="line"><span class="comment"># 这里的col_name为特征的名称</span></span><br><span class="line">ax.set_xticklabels(col_name)</span><br><span class="line">ax.set_yticklabels(col_name)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="查找最优模型"><a href="#查找最优模型" class="headerlink" title="查找最优模型"></a>查找最优模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, cross_val_score,GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">models = []</span><br><span class="line">models.append((<span class="string">"AB"</span>, AdaBoostClassifier()))</span><br><span class="line">models.append((<span class="string">"GBM"</span>, GradientBoostingClassifier()))</span><br><span class="line">models.append((<span class="string">"RF"</span>, RandomForestClassifier()))</span><br><span class="line">models.append((<span class="string">"ET"</span>, ExtraTreesClassifier()))</span><br><span class="line">models.append((<span class="string">"SVC"</span>, SVC()))</span><br><span class="line">models.append((<span class="string">"KNN"</span>, KNeighborsClassifier()))</span><br><span class="line">models.append((<span class="string">"LR"</span>, LogisticRegression()))</span><br><span class="line">models.append((<span class="string">"GNB"</span>, GaussianNB()))</span><br><span class="line">models.append((<span class="string">"LDA"</span>, LinearDiscriminantAnalysis()))</span><br><span class="line"></span><br><span class="line">names = []</span><br><span class="line">results = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, model <span class="keyword">in</span> models:</span><br><span class="line">    kfold = KFold(n_splits=<span class="number">5</span>, random_state=<span class="number">42</span>)</span><br><span class="line">    result = cross_val_score(model, X, y, scoring=<span class="string">'accuracy'</span>, cv=kfold)</span><br><span class="line">    names.append(name)</span><br><span class="line">    results.append(result)</span><br><span class="line">    print(<span class="string">"&#123;&#125; Mean:&#123;:.4f&#125;(Std:&#123;:.4f&#125;)"</span>.format(name, result.mean(), result.std()))</span><br></pre></td></tr></table></figure>
<h2 id="使用Pipeline"><a href="#使用Pipeline" class="headerlink" title="使用Pipeline"></a>使用Pipeline</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">pipeline = []</span><br><span class="line">pipeline.append((<span class="string">"ScalerET"</span>, Pipeline([(<span class="string">"Scaler"</span>,StandardScaler()),</span><br><span class="line"> (<span class="string">"ET"</span>,ExtraTreesClassifier())])))</span><br><span class="line">pipeline.append((<span class="string">"ScalerGBM"</span>, Pipeline([(<span class="string">"Scaler"</span>,StandardScaler()),</span><br><span class="line">   (<span class="string">"GBM"</span>,GradientBoostingClassifier())])))</span><br><span class="line">pipeline.append((<span class="string">"ScalerRF"</span>, Pipeline([(<span class="string">"Scaler"</span>,StandardScaler()),</span><br><span class="line"> (<span class="string">"RF"</span>,RandomForestClassifier())])))</span><br><span class="line"></span><br><span class="line">names = []</span><br><span class="line">results = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, model <span class="keyword">in</span> pipeline:</span><br><span class="line">    kfold = KFold(n_splits=<span class="number">5</span>, random_state=<span class="number">42</span>)</span><br><span class="line">    result = cross_val_score(model, X, y, scoring=<span class="string">'accuracy'</span>, cv=kfold)</span><br><span class="line">    names.append(name)</span><br><span class="line">    results.append(result)</span><br><span class="line">    print(<span class="string">"&#123;&#125; Mean:&#123;:.4f&#125;(Std:&#123;:.4f&#125;)"</span>.format(name, result.mean(), result.std()))</span><br></pre></td></tr></table></figure>
<h2 id="模型调节"><a href="#模型调节" class="headerlink" title="模型调节"></a>模型调节</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">"C"</span>:[<span class="number">0.1</span>,<span class="number">0.3</span>,<span class="number">0.5</span>,<span class="number">0.7</span>,<span class="number">0.9</span>,<span class="number">1.0</span>,<span class="number">1.3</span>,<span class="number">1.5</span>,<span class="number">1.7</span>,<span class="number">2.0</span>],</span><br><span class="line">    <span class="string">"kernel"</span>:[<span class="string">"linear"</span>,<span class="string">"poly"</span>,<span class="string">"rbf"</span>,<span class="string">"sigmoid"</span>]</span><br><span class="line">&#125;</span><br><span class="line">model = SVC()</span><br><span class="line">kfold = KFold(n_splits=<span class="number">5</span>,random_state=<span class="number">42</span>)</span><br><span class="line">grid = GridSearchCV(estimator=model,param_grid=param_grid,scoring=<span class="string">"accuracy"</span>,cv=kfold)</span><br><span class="line">grid_result = grid.fit(X,y)</span><br><span class="line">print(<span class="string">"Best: &#123;&#125; using &#123;&#125;"</span>.format(grid_result.best_score_,grid_result.best_params_))</span><br><span class="line">means = grid_result.cv_results_[<span class="string">"mean_test_score"</span>]</span><br><span class="line">stds = grid_result.cv_results_[<span class="string">"std_test_score"</span>]</span><br><span class="line">params = grid_result.cv_results_[<span class="string">"params"</span>]</span><br><span class="line"><span class="keyword">for</span> mean,stdev,param <span class="keyword">in</span> zip(means,stds,params):</span><br><span class="line">    print(<span class="string">"&#123;&#125; (&#123;&#125;) with &#123;&#125;"</span>.format(mean,stdev,param))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 使用随机梯度下降法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line">param_distribs = &#123;</span><br><span class="line">        <span class="string">'n_estimators'</span>: randint(low=<span class="number">1</span>, high=<span class="number">200</span>),</span><br><span class="line">        <span class="string">'max_features'</span>: randint(low=<span class="number">1</span>, high=<span class="number">8</span>),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">forest_reg = RandomForestRegressor(random_state=<span class="number">42</span>)</span><br><span class="line">rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,</span><br><span class="line">                                n_iter=<span class="number">10</span>, cv=<span class="number">5</span>, scoring=<span class="string">'neg_mean_squared_error'</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rnd_search.fit(housing_prepared, housing_labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># svm 调参</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> expon, reciprocal</span><br><span class="line"></span><br><span class="line"><span class="comment"># see https://docs.scipy.org/doc/scipy/reference/stats.html</span></span><br><span class="line"><span class="comment"># for `expon()` and `reciprocal()` documentation and more probability distribution functions.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: gamma is ignored when kernel is "linear"</span></span><br><span class="line">param_distribs = &#123;</span><br><span class="line">        <span class="string">'kernel'</span>: [<span class="string">'linear'</span>, <span class="string">'rbf'</span>],</span><br><span class="line">        <span class="string">'C'</span>: reciprocal(<span class="number">20</span>, <span class="number">200000</span>),</span><br><span class="line">        <span class="string">'gamma'</span>: expon(scale=<span class="number">1.0</span>),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">svm_reg = SVR()</span><br><span class="line">rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,</span><br><span class="line">                                n_iter=<span class="number">50</span>, cv=<span class="number">5</span>, scoring=<span class="string">'neg_mean_squared_error'</span>,</span><br><span class="line">                                verbose=<span class="number">2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rnd_search.fit(housing_prepared, housing_labels)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>softmaxa Regression</title>
    <url>/2020/03/04/2020-3-3-softmax_function-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="softmaxa-Regression"><a href="#softmaxa-Regression" class="headerlink" title="softmaxa Regression"></a>softmaxa Regression</h1><p>为LR在多分类上的推广，与LR一样，同属于广义线性模型（Generalized Linear Model）</p>
<p>​    </p>
<script type="math/tex; mode=display">
S_i=\frac{e^{V_i}}{\sum_i^Ce^{V_i}}</script><p>其中，Vi 是分类器前级输出单元的输出。i 表示类别索引，总的类别个数为 C。Si 表示的是当前元素的指数与所有元素指数和的比值。Softmax 将多分类的输出数值转化为相对概率，更容易理解和比较。我们来看下面这个例子</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 什么是Softmax函数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">实际应用中，使用 Softmax 需要注意数值溢出的问题。因为有指数运算，</span></span><br><span class="line"><span class="string">如果 V 数值很大，经过指数运算后的数值往往可能有溢出的可能。</span></span><br><span class="line"><span class="string">所以，需要对 V 进行一些数值处理：即 V 中的每个元素减去 V 中的最大值。"""</span></span><br><span class="line">scores = np.array([<span class="number">123</span>,<span class="number">456</span>,<span class="number">789</span>])</span><br><span class="line">scores -= np.max(scores)</span><br><span class="line">p = np.exp(scores) / np.sum(np.exp(scores))</span><br><span class="line"><span class="comment"># print(p)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Softmax损失函数</span></span><br><span class="line"><span class="comment"># 其中，Syi是正确类别对应的线性得分函数，Si 是正确类别对应的 Softmax输出。</span></span><br><span class="line"><span class="comment"># 由于 log 运算符不会影响函数的单调性，我们对 Si 进行 log 操作：</span></span><br><span class="line"><span class="comment"># 我们希望 Si 越大越好，即正确类别对应的相对概率越大越好，那么就可以对 Si 前面加个负号，来表示损失函数</span></span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
L_i=-log\frac{e^{S_{y_i}}}{\sum_{j=1}^Ce^{S_j}}=-(s_{y_i}-log\sum_{j=1}^Ce^{s_j})=-s_{y_i}+log\sum_{j=1}^Ce^{s_j}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W,X,y,reg)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Softmax loss function ,naive implementation(with loops)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs have dimension D,there are C classes,and we operate on minibatches of N examples</span></span><br><span class="line"><span class="string">    :param W: A numpy array of shape(D,C) containing weights</span></span><br><span class="line"><span class="string">    :param X: A numpy array of shaep(N,D) containing a minibatch of data</span></span><br><span class="line"><span class="string">    :param y:A numpy array of shape(N,) containing training,labels,y[i] = c means</span></span><br><span class="line"><span class="string">    :param reg:(float) regularization strength</span></span><br><span class="line"><span class="string">    :return: A tuple of (loss as single float, gradient with respect to weights W,an array of same shape as  W)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># initialize the loss and gradient to zero</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</span><br><span class="line">        scores = X[i,:].dot(W)</span><br><span class="line">        scores_shift = scores - np.max(scores)</span><br><span class="line">        right_class = y[i]</span><br><span class="line">        loss += (-scores_shift[right_class] + np.log(np.sum(np.exp(scores_shift))))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):</span><br><span class="line">            softmax_output = np.exp(scores_shift[j]) / np.sum(np.exp(scores_shift))</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                dW[:,j] += (<span class="number">-1</span> + softmax_output) * X[i,:]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dW[:,j] += softmax_output * X[i,:]</span><br><span class="line">    loss /= num_train</span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.sum(W*W)</span><br><span class="line">    dW /= num_train</span><br><span class="line">    dW += reg * W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss,dW</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W,X,y,reg)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Softmax loss function,vectorized version</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as softmax_loss_naive</span></span><br><span class="line"><span class="string">    :param W:</span></span><br><span class="line"><span class="string">    :param X:</span></span><br><span class="line"><span class="string">    :param y:</span></span><br><span class="line"><span class="string">    :param reg:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得样本的数量</span></span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 获取样本的分裂数量</span></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 获取到每个样本所对应的分类的得分</span></span><br><span class="line">    scores = X.dot(W)</span><br><span class="line">    scores_shift = scores - np.max(scores, axis=<span class="number">1</span>)</span><br><span class="line">    softmax_output = np.exp(scores_shift) / np.sum(np.exp(scores_shift), axis=<span class="number">1</span>)</span><br><span class="line">    loss = -np.sum(np.log(softmax_output[range(num_train), list(y)]))</span><br><span class="line">    loss /= num_train</span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line">    dS = softmax_output.copy()</span><br><span class="line">    dS[range(num_train), list(y)] += <span class="number">-1</span></span><br><span class="line">    dW = (X.T).dot(dS)</span><br><span class="line">    dW = dW / num_train + reg * W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>multi-classifition</tag>
        <tag>softmax</tag>
      </tags>
  </entry>
  <entry>
    <title>如何处理不平衡的数据</title>
    <url>/2020/03/04/2020-3-3-sciPy-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="2-5-SciPy中稀疏矩阵"><a href="#2-5-SciPy中稀疏矩阵" class="headerlink" title="2.5 SciPy中稀疏矩阵"></a>2.5 SciPy中稀疏矩阵</h1><p>In [3]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import numpy as np</span><br></pre></td></tr></table></figure>
<h2 id="2-5-1-介绍"><a href="#2-5-1-介绍" class="headerlink" title="2.5.1 介绍"></a>2.5.1 介绍</h2><p>(密集) 矩阵是:</p>
<ul>
<li>数据对象</li>
<li>存储二维值数组的数据结构</li>
</ul>
<p>重要特征:</p>
<ul>
<li>一次分配所有项目的内存<ul>
<li>通常是一个连续组块，想一想Numpy数组</li>
</ul>
</li>
<li><em>快速</em>访问个项目(*)</li>
</ul>
<h3 id="2-5-1-1-为什么有稀疏矩阵？"><a href="#2-5-1-1-为什么有稀疏矩阵？" class="headerlink" title="2.5.1.1 为什么有稀疏矩阵？"></a>2.5.1.1 为什么有稀疏矩阵？</h3><ul>
<li>内存，增长是n**2</li>
<li>小例子（双精度矩阵）:</li>
</ul>
<p>In [5]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">x &#x3D; np.linspace(0, 1e6, 10)</span><br><span class="line">plt.plot(x, 8.0 * (x**2) &#x2F; 1e6, lw&#x3D;5)   </span><br><span class="line">plt.xlabel(&#39;size n&#39;)</span><br><span class="line">plt.ylabel(&#39;memory [MB]&#39;)</span><br></pre></td></tr></table></figure>
<p>Out[5]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;matplotlib.text.Text at 0x105b08dd0&gt;</span><br></pre></td></tr></table></figure>
<p><img src="https://wizardforcel.gitbooks.io/scipy-lecture-notes/content/img/C3AAAAAElFTkSuQmCC.png" alt="img"></p>
<h3 id="2-5-1-2-稀疏矩阵-vs-稀疏矩阵存储方案"><a href="#2-5-1-2-稀疏矩阵-vs-稀疏矩阵存储方案" class="headerlink" title="2.5.1.2 稀疏矩阵 vs. 稀疏矩阵存储方案"></a>2.5.1.2 稀疏矩阵 vs. 稀疏矩阵存储方案</h3><ul>
<li>稀疏矩阵是一个矩阵，巨大多数是空的</li>
<li>存储所有的0是浪费 -&gt; 只存储非0项目</li>
<li>想一下<strong>压缩</strong></li>
<li>有利: 巨大的内存节省</li>
<li>不利: 依赖实际的存储方案, (*) 通常并不能满足</li>
</ul>
<h3 id="2-5-1-3-典型应用"><a href="#2-5-1-3-典型应用" class="headerlink" title="2.5.1.3 典型应用"></a>2.5.1.3 典型应用</h3><ul>
<li>偏微分方程（PDES）的解<ul>
<li>有限元素法</li>
<li>机械工程、电子、物理…</li>
</ul>
</li>
<li>图论<ul>
<li>（i，j）不是0表示节点i与节点j是联接的</li>
</ul>
</li>
<li>…</li>
</ul>
<h3 id="2-5-1-4-先决条件"><a href="#2-5-1-4-先决条件" class="headerlink" title="2.5.1.4 先决条件"></a>2.5.1.4 先决条件</h3><p>最新版本的</p>
<ul>
<li><code>numpy</code></li>
<li><code>scipy</code></li>
<li><code>matplotlib</code> (可选)</li>
<li><code>ipython</code> (那些增强很方便)</li>
</ul>
<h3 id="2-5-1-5-稀疏结构可视化"><a href="#2-5-1-5-稀疏结构可视化" class="headerlink" title="2.5.1.5 稀疏结构可视化"></a>2.5.1.5 稀疏结构可视化</h3><ul>
<li>matplotlib中的<code>spy()</code></li>
<li>样例绘图:</li>
</ul>
<p><img src="http://scipy-lectures.github.io/_images/graph.png" alt="img"> <img src="http://scipy-lectures.github.io/_images/graph_g.png" alt="img"> <img src="http://scipy-lectures.github.io/_images/graph_rcm.png" alt="img"></p>
<h2 id="2-5-2-存储机制"><a href="#2-5-2-存储机制" class="headerlink" title="2.5.2 存储机制"></a>2.5.2 存储机制</h2><ul>
<li>scipy.sparse中有七类稀疏矩阵:<ol>
<li>csc_matrix: 压缩列格式</li>
<li>csr_matrix: 压缩行格式</li>
<li>bsr_matrix: 块压缩行格式</li>
<li>lil_matrix: 列表的列表格式</li>
<li>dok_matrix: 值的字典格式</li>
<li>coo_matrix: 座标格式 (即 IJV, 三维格式)</li>
<li>dia_matrix: 对角线格式</li>
</ol>
</li>
<li>每一个类型适用于一些任务</li>
<li>许多都利用了由Nathan Bell提供的稀疏工具 C ++ 模块</li>
<li>假设导入了下列模块:</li>
</ul>
<p>In [1]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import scipy.sparse as sparse</span><br><span class="line">import matplotlib.pyplot as plt</span><br></pre></td></tr></table></figure>
<ul>
<li><p>给Numpy用户的</p>
<p>warning</p>
<p>:</p>
<ul>
<li>使用’<em>‘的乘是</em>矩阵相乘* (点积)</li>
<li>并不是Numpy的一部分!<ul>
<li>向Numpy函数传递一个稀疏矩阵希望一个ndarray/矩阵是没用的</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-5-2-1-通用方法"><a href="#2-5-2-1-通用方法" class="headerlink" title="2.5.2.1 通用方法"></a>2.5.2.1 通用方法</h3><ul>
<li>所有scipy.sparse类都是spmatrix的子类<ul>
<li>算术操作的默认实现<ul>
<li>通常转化为CSR</li>
<li>为了效率而子类覆盖</li>
</ul>
</li>
<li>形状、数据类型设置/获取</li>
<li>非0索引</li>
<li>格式转化、与Numpy交互(toarray(), todense())</li>
<li>…</li>
</ul>
</li>
<li>属性:<ul>
<li>mtx.A - 与mtx.toarray()相同</li>
<li>mtx.T - 转置 (与mtx.transpose()相同)</li>
<li>mtx.H - Hermitian (列举) 转置</li>
<li>mtx.real - 复矩阵的真部</li>
<li>mtx.imag - 复矩阵的虚部</li>
<li>mtx.size - 非零数 (与self.getnnz()相同)</li>
<li>mtx.shape - 行数和列数 (元组)</li>
</ul>
</li>
<li>数据通常储存在Numpy数组中</li>
</ul>
<h3 id="2-5-2-2-稀疏矩阵类"><a href="#2-5-2-2-稀疏矩阵类" class="headerlink" title="2.5.2.2 稀疏矩阵类"></a>2.5.2.2 稀疏矩阵类</h3><h4 id="2-5-2-2-1-对角线格式-DIA"><a href="#2-5-2-2-1-对角线格式-DIA" class="headerlink" title="2.5.2.2.1 对角线格式 (DIA))"></a>2.5.2.2.1 对角线格式 (DIA))</h4><ul>
<li>非常简单的格式</li>
<li>形状 (n_diag, length) 的密集Numpy数组的对角线<ul>
<li>固定长度 -&gt; 当离主对角线比较远时会浪费空间</li>
<li>_data_matrix的子类 (带数据属性的稀疏矩阵类)</li>
</ul>
</li>
<li>每个对角线的偏移<ul>
<li>0 是主对角线</li>
<li>负偏移 = 下面</li>
<li>正偏移 = 上面</li>
</ul>
</li>
<li>快速矩阵 * 向量 (sparsetools)</li>
<li>快速方便的关于项目的操作<ul>
<li>直接操作数据数组 (快速的NumPy机件)</li>
</ul>
</li>
<li>构建器接受 :<ul>
<li>密集矩阵 (数组)</li>
<li>稀疏矩阵</li>
<li>形状元组 (创建空矩阵)</li>
<li>(数据, 偏移) 元组</li>
</ul>
</li>
<li>没有切片、没有单个项目访问</li>
<li>用法 :<ul>
<li>非常专业</li>
<li>通过有限微分解偏微分方程</li>
<li>有一个迭代求解器 ##### 2.5.2.2.1.1 示例</li>
</ul>
</li>
<li>创建一些DIA矩阵 :</li>
</ul>
<p>In [3]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data &#x3D; np.array([[1, 2, 3, 4]]).repeat(3, axis&#x3D;0)</span><br><span class="line">data</span><br></pre></td></tr></table></figure>
<p>Out[3]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[1, 2, 3, 4],</span><br><span class="line">       [1, 2, 3, 4],</span><br><span class="line">       [1, 2, 3, 4]])</span><br></pre></td></tr></table></figure>
<p>In [6]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">offsets &#x3D; np.array([0, -1, 2])</span><br><span class="line">mtx &#x3D; sparse.dia_matrix((data, offsets), shape&#x3D;(4, 4))</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure>
<p>Out[6]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;4x4 sparse matrix of type &#39;&lt;type &#39;numpy.int64&#39;&gt;&#39;</span><br><span class="line">    with 9 stored elements (3 diagonals) in DIAgonal format&gt;</span><br></pre></td></tr></table></figure>
<p>In [7]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure>
<p>Out[7]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[1, 0, 3, 0],</span><br><span class="line">        [1, 2, 0, 4],</span><br><span class="line">        [0, 2, 3, 0],</span><br><span class="line">        [0, 0, 3, 4]])</span><br></pre></td></tr></table></figure>
<p>In [9]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data &#x3D; np.arange(12).reshape((3, 4)) + 1</span><br><span class="line">data</span><br></pre></td></tr></table></figure>
<p>Out[9]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[ 1,  2,  3,  4],</span><br><span class="line">       [ 5,  6,  7,  8],</span><br><span class="line">       [ 9, 10, 11, 12]])</span><br></pre></td></tr></table></figure>
<p>In [10]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx &#x3D; sparse.dia_matrix((data, offsets), shape&#x3D;(4, 4))</span><br><span class="line">mtx.data</span><br></pre></td></tr></table></figure>
<p>Out[10]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[ 1,  2,  3,  4],</span><br><span class="line">       [ 5,  6,  7,  8],</span><br><span class="line">       [ 9, 10, 11, 12]])</span><br></pre></td></tr></table></figure>
<p>In [11]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.offsets</span><br></pre></td></tr></table></figure>
<p>Out[11]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([ 0, -1,  2], dtype&#x3D;int32)</span><br></pre></td></tr></table></figure>
<p>In [12]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print mtx</span><br><span class="line">  (0, 0)    1</span><br><span class="line">  (1, 1)    2</span><br><span class="line">  (2, 2)    3</span><br><span class="line">  (3, 3)    4</span><br><span class="line">  (1, 0)    5</span><br><span class="line">  (2, 1)    6</span><br><span class="line">  (3, 2)    7</span><br><span class="line">  (0, 2)    11</span><br><span class="line">  (1, 3)    12</span><br></pre></td></tr></table></figure>
<p>In [13]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure>
<p>Out[13]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[ 1,  0, 11,  0],</span><br><span class="line">        [ 5,  2,  0, 12],</span><br><span class="line">        [ 0,  6,  3,  0],</span><br><span class="line">        [ 0,  0,  7,  4]])</span><br></pre></td></tr></table></figure>
<ul>
<li>机制的解释 :</li>
</ul>
<p>偏移: 行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> 2:  9</span><br><span class="line"> 1:  --10------</span><br><span class="line"> 0:  1  . 11  .</span><br><span class="line">-1:  5  2  . 12</span><br><span class="line">-2:  .  6  3  .</span><br><span class="line">-3:  .  .  7  4</span><br><span class="line">     ---------8</span><br></pre></td></tr></table></figure>
<ul>
<li>矩阵-向量相乘</li>
</ul>
<p>In [15]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vec &#x3D; np.ones((4, ))</span><br><span class="line">vec</span><br></pre></td></tr></table></figure>
<p>Out[15]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([ 1.,  1.,  1.,  1.])</span><br></pre></td></tr></table></figure>
<p>In [16]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx * vec</span><br></pre></td></tr></table></figure>
<p>Out[16]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([ 12.,  19.,   9.,  11.])</span><br></pre></td></tr></table></figure>
<p>In [17]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.toarray() * vec</span><br></pre></td></tr></table></figure>
<p>Out[17]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[  1.,   0.,  11.,   0.],</span><br><span class="line">       [  5.,   2.,   0.,  12.],</span><br><span class="line">       [  0.,   6.,   3.,   0.],</span><br><span class="line">       [  0.,   0.,   7.,   4.]])</span><br></pre></td></tr></table></figure>
<h4 id="2-5-2-2-2-列表的列表格式-LIL"><a href="#2-5-2-2-2-列表的列表格式-LIL" class="headerlink" title="2.5.2.2.2 列表的列表格式 (LIL))"></a>2.5.2.2.2 列表的列表格式 (LIL))</h4><ul>
<li>基于行的联接列表<ul>
<li>每一行是一个Python列表（排序的）非零元素的列索引</li>
<li>行存储在Numpy数组中 (dtype=np.object)</li>
<li>非零值也近似存储</li>
</ul>
</li>
<li>高效增量构建稀疏矩阵</li>
<li>构建器接受 :<ul>
<li>密集矩阵 (数组)</li>
<li>稀疏矩阵</li>
<li>形状元组 (创建一个空矩阵)</li>
</ul>
</li>
<li>灵活切片、高效改变稀疏结构</li>
<li>由于是基于行的，算术和行切片慢</li>
<li>用途 :<ul>
<li>当稀疏模式并不是已知的逻辑或改变</li>
<li>例子：从一个文本文件读取稀疏矩阵 ##### 2.5.2.2.2.1 示例</li>
</ul>
</li>
<li>创建一个空的LIL矩阵 :</li>
</ul>
<p>In [2]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx &#x3D; sparse.lil_matrix((4, 5))</span><br></pre></td></tr></table></figure>
<ul>
<li>准备随机数据:</li>
</ul>
<p>In [4]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from numpy.random import rand</span><br><span class="line">data &#x3D; np.round(rand(2, 3))</span><br><span class="line">data</span><br></pre></td></tr></table></figure>
<p>Out[4]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[ 0.,  0.,  0.],</span><br><span class="line">       [ 1.,  0.,  0.]])</span><br></pre></td></tr></table></figure>
<ul>
<li>使用象征所以分配数据:</li>
</ul>
<p>In [6]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx[:2, [1, 2, 3]] &#x3D; data</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure>
<p>Out[6]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;4x5 sparse matrix of type &#39;&lt;type &#39;numpy.float64&#39;&gt;&#39;</span><br><span class="line">    with 3 stored elements in LInked List format&gt;</span><br></pre></td></tr></table></figure>
<p>In [7]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print mtx</span><br><span class="line">  (0, 1)    1.0</span><br><span class="line">  (0, 3)    1.0</span><br><span class="line">  (1, 2)    1.0</span><br></pre></td></tr></table></figure>
<p>In [8]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure>
<p>Out[8]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[ 0.,  1.,  0.,  1.,  0.],</span><br><span class="line">        [ 0.,  0.,  1.,  0.,  0.],</span><br><span class="line">        [ 0.,  0.,  0.,  0.,  0.],</span><br><span class="line">        [ 0.,  0.,  0.,  0.,  0.]])</span><br></pre></td></tr></table></figure>
<p>In [9]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.toarray()</span><br></pre></td></tr></table></figure>
<p>Out[9]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([[ 0.,  1.,  0.,  1.,  0.],</span><br><span class="line">       [ 0.,  0.,  1.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  0.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  0.,  0.,  0.]])</span><br></pre></td></tr></table></figure>
<p>更多的切片和索引:</p>
<p>In [10]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx &#x3D; sparse.lil_matrix([[0, 1, 2, 0], [3, 0, 1, 0], [1, 0, 0, 1]])</span><br><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure>
<p>Out[10]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[0, 1, 2, 0],</span><br><span class="line">        [3, 0, 1, 0],</span><br><span class="line">        [1, 0, 0, 1]])</span><br></pre></td></tr></table></figure>
<p>In [11]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print mtx</span><br><span class="line">  (0, 1)    1</span><br><span class="line">  (0, 2)    2</span><br><span class="line">  (1, 0)    3</span><br><span class="line">  (1, 2)    1</span><br><span class="line">  (2, 0)    1</span><br><span class="line">  (2, 3)    1</span><br></pre></td></tr></table></figure>
<p>In [12]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx[:2, :]</span><br></pre></td></tr></table></figure>
<p>Out[12]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;2x4 sparse matrix of type &#39;&lt;type &#39;numpy.int64&#39;&gt;&#39;</span><br><span class="line">    with 4 stored elements in LInked List format&gt;</span><br></pre></td></tr></table></figure>
<p>In [13]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx[:2, :].todense()</span><br></pre></td></tr></table></figure>
<p>Out[13]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[0, 1, 2, 0],</span><br><span class="line">        [3, 0, 1, 0]])</span><br></pre></td></tr></table></figure>
<p>In [14]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx[1:2, [0,2]].todense()</span><br></pre></td></tr></table></figure>
<p>Out[14]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[3, 1]])</span><br></pre></td></tr></table></figure>
<p>In [15]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure>
<p>Out[15]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[0, 1, 2, 0],</span><br><span class="line">        [3, 0, 1, 0],</span><br><span class="line">        [1, 0, 0, 1]])</span><br></pre></td></tr></table></figure>
<h4 id="2-5-2-2-3-值的字典格式-DOK"><a href="#2-5-2-2-3-值的字典格式-DOK" class="headerlink" title="2.5.2.2.3 值的字典格式 (DOK))"></a>2.5.2.2.3 值的字典格式 (DOK))</h4><ul>
<li>Python字典的子类<ul>
<li>键是 (行, 列) 索引元组 (不允许重复的条目)</li>
<li>值是对应的非零值</li>
</ul>
</li>
<li>高效增量构建稀疏矩阵</li>
<li>构建器支持:<ul>
<li>密集矩阵 (数组)</li>
<li>稀疏矩阵</li>
<li>形状元组 (创建空矩阵)</li>
</ul>
</li>
<li>高效 O(1) 对单个元素的访问</li>
<li>灵活索引，改变稀疏结构是高效</li>
<li>一旦创建完成后可以被高效转换为coo_matrix</li>
<li>算术很慢 (循环用<code>dict.iteritems()</code>)</li>
<li>用法:<ul>
<li>当稀疏模式是未知的假设或改变时</li>
</ul>
</li>
</ul>
<h5 id="2-5-2-2-3-1-示例"><a href="#2-5-2-2-3-1-示例" class="headerlink" title="2.5.2.2.3.1 示例"></a>2.5.2.2.3.1 示例</h5><ul>
<li>逐个元素创建一个DOK矩阵:</li>
</ul>
<p>In [16]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx &#x3D; sparse.dok_matrix((5, 5), dtype&#x3D;np.float64)</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure>
<p>Out[16]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;5x5 sparse matrix of type &#39;&lt;type &#39;numpy.float64&#39;&gt;&#39;</span><br><span class="line">    with 0 stored elements in Dictionary Of Keys format&gt;</span><br></pre></td></tr></table></figure>
<p>In [17]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for ir in range(5):</span><br><span class="line">    for ic in range(5):</span><br><span class="line">        mtx[ir, ic] &#x3D; 1.0 * (ir !&#x3D; ic)</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure>
<p>Out[17]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;5x5 sparse matrix of type &#39;&lt;type &#39;numpy.float64&#39;&gt;&#39;</span><br><span class="line">    with 20 stored elements in Dictionary Of Keys format&gt;</span><br></pre></td></tr></table></figure>
<p>In [18]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure>
<p>Out[18]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[ 0.,  1.,  1.,  1.,  1.],</span><br><span class="line">        [ 1.,  0.,  1.,  1.,  1.],</span><br><span class="line">        [ 1.,  1.,  0.,  1.,  1.],</span><br><span class="line">        [ 1.,  1.,  1.,  0.,  1.],</span><br><span class="line">        [ 1.,  1.,  1.,  1.,  0.]])</span><br></pre></td></tr></table></figure>
<ul>
<li>切片与索引:</li>
</ul>
<p>In [19]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx[1, 1]</span><br></pre></td></tr></table></figure>
<p>Out[19]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.0</span><br></pre></td></tr></table></figure>
<p>In [20]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx[1, 1:3]</span><br></pre></td></tr></table></figure>
<p>Out[20]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;1x2 sparse matrix of type &#39;&lt;type &#39;numpy.float64&#39;&gt;&#39;</span><br><span class="line">    with 1 stored elements in Dictionary Of Keys format&gt;</span><br></pre></td></tr></table></figure>
<p>In [21]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx[1, 1:3].todense()</span><br></pre></td></tr></table></figure>
<p>Out[21]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[ 0.,  1.]])</span><br></pre></td></tr></table></figure>
<p>In [22]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx[[2,1], 1:3].todense()</span><br></pre></td></tr></table></figure>
<p>Out[22]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[ 1.,  0.],</span><br><span class="line">        [ 0.,  1.]])</span><br></pre></td></tr></table></figure>
<h4 id="2-5-2-2-4-座标格式-COO"><a href="#2-5-2-2-4-座标格式-COO" class="headerlink" title="2.5.2.2.4 座标格式 (COO))"></a>2.5.2.2.4 座标格式 (COO))</h4><ul>
<li>也被称为 ‘ijv’ 或 ‘triplet’ 格式<ul>
<li>三个NumPy数组: row, col, data</li>
<li><code>data[i]</code>是在 (row[i], col[i]) 位置的值</li>
<li>允许重复值</li>
</ul>
</li>
<li><code>\_data\_matrix</code>的子类 (带有<code>data</code>属性的稀疏矩阵类)</li>
<li>构建稀疏矩阵的高速模式</li>
<li>构建器接受:<ul>
<li>密集矩阵 (数组)</li>
<li>稀疏矩阵</li>
<li>形状元组 (创建空数组)</li>
<li><code>(data, ij)</code>元组</li>
</ul>
</li>
<li>与CSR/CSC格式非常快的互相转换</li>
<li>快速的矩阵 * 向量 (sparsetools)</li>
<li>快速而简便的逐项操作<ul>
<li>直接操作数据数组 (快速NumPy机制)</li>
</ul>
</li>
<li>没有切片，没有算术 (直接)</li>
<li>使用:<ul>
<li>在各种稀疏格式间的灵活转换</li>
<li>当转化到其他形式 (通常是 CSR 或 CSC), 重复的条目被加总到一起<ul>
<li>有限元素矩阵的快速高效创建</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="2-5-2-2-4-1-示例"><a href="#2-5-2-2-4-1-示例" class="headerlink" title="2.5.2.2.4.1 示例"></a>2.5.2.2.4.1 示例</h5><ul>
<li>创建空的COO矩阵:</li>
</ul>
<p>In [23]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx &#x3D; sparse.coo_matrix((3, 4), dtype&#x3D;np.int8)</span><br><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure>
<p>Out[23]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0]], dtype&#x3D;int8)</span><br></pre></td></tr></table></figure>
<ul>
<li>用 (data, ij) 元组创建:</li>
</ul>
<p>In [24]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">row &#x3D; np.array([0, 3, 1, 0])</span><br><span class="line">col &#x3D; np.array([0, 3, 1, 2])</span><br><span class="line">data &#x3D; np.array([4, 5, 7, 9])</span><br><span class="line">mtx &#x3D; sparse.coo_matrix((data, (row, col)), shape&#x3D;(4, 4))</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure>
<p>Out[24]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;4x4 sparse matrix of type &#39;&lt;type &#39;numpy.int64&#39;&gt;&#39;</span><br><span class="line">    with 4 stored elements in COOrdinate format&gt;</span><br></pre></td></tr></table></figure>
<p>In [25]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure>
<p>Out[25]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[4, 0, 9, 0],</span><br><span class="line">        [0, 7, 0, 0],</span><br><span class="line">        [0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 5]])</span><br></pre></td></tr></table></figure>
<h4 id="2-5-2-2-5-压缩稀疏行格式-CSR"><a href="#2-5-2-2-5-压缩稀疏行格式-CSR" class="headerlink" title="2.5.2.2.5 压缩稀疏行格式 (CSR))"></a>2.5.2.2.5 压缩稀疏行格式 (CSR))</h4><ul>
<li><p>面向行</p>
<ul>
<li>三个Numpy数组:</li>
</ul>
</li>
</ul>
<pre><code><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">indices</span><br></pre></td></tr></table></figure>

,



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">indptr</span><br></pre></td></tr></table></figure>

,



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>

- `indices`是列索引的数组
- `data`是对应的非零值数组
- `indptr`指向行开始的所以和数据
- 长度是`n_row + 1`, 最后一个项目 = 值数量 = `indices`和`data`的长度
- i-th行的非零值是列索引为`indices[indptr[i]:indptr[i+1]]`的`data[indptr[i]:indptr[i+1]]`
- 项目 (i, j) 可以通过`data[indptr[i]+k]`, k是j在`indices[indptr[i]:indptr[i+1]]`的位置来访问
</code></pre><ul>
<li><p><code>_cs_matrix</code> (常规 CSR/CSC 功能) 的子类</p>
</li>
<li><p><code>_data_matrix</code> (带有<code>data</code>属性的稀疏矩阵类) 的子类</p>
</li>
</ul>
<ul>
<li><p>快速矩阵向量相乘和其他算术 (sparsetools)</p>
</li>
<li><p>构建器接受:</p>
<ul>
<li>密集矩阵 (数组)</li>
<li>稀疏矩阵</li>
<li>形状元组 (创建空矩阵)</li>
<li><code>(data, ij)</code> 元组</li>
<li><code>(data, indices, indptr)</code> 元组</li>
</ul>
</li>
<li><p>高效行切片，面向行的操作</p>
</li>
<li><p>较慢的列切片，改变稀疏结构代价昂贵</p>
</li>
<li><p>用途:</p>
<ul>
<li>实际计算 (大多数线性求解器都支持这个格式)</li>
</ul>
</li>
</ul>
<h5 id="2-5-2-2-5-1-示例"><a href="#2-5-2-2-5-1-示例" class="headerlink" title="2.5.2.2.5.1 示例"></a>2.5.2.2.5.1 示例</h5><ul>
<li>创建空的CSR矩阵:</li>
</ul>
<p>In [26]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx &#x3D; sparse.csr_matrix((3, 4), dtype&#x3D;np.int8)</span><br><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure>
<p>Out[26]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0]], dtype&#x3D;int8)</span><br></pre></td></tr></table></figure>
<ul>
<li>用<code>(data, ij)</code>元组创建:</li>
</ul>
<p>In [27]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">row &#x3D; np.array([0, 0, 1, 2, 2, 2])</span><br><span class="line">col &#x3D; np.array([0, 2, 2, 0, 1, 2])</span><br><span class="line">data &#x3D; np.array([1, 2, 3, 4, 5, 6])</span><br><span class="line">mtx &#x3D; sparse.csr_matrix((data, (row, col)), shape&#x3D;(3, 3))</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure>
<p>Out[27]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;3x3 sparse matrix of type &#39;&lt;type &#39;numpy.int64&#39;&gt;&#39;</span><br><span class="line">    with 6 stored elements in Compressed Sparse Row format&gt;</span><br></pre></td></tr></table></figure>
<p>In [28]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure>
<p>Out[28]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[1, 0, 2],</span><br><span class="line">        [0, 0, 3],</span><br><span class="line">        [4, 5, 6]])</span><br></pre></td></tr></table></figure>
<p>In [29]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.data</span><br></pre></td></tr></table></figure>
<p>Out[29]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([1, 2, 3, 4, 5, 6])</span><br></pre></td></tr></table></figure>
<p>In [30]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.indices</span><br></pre></td></tr></table></figure>
<p>Out[30]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([0, 2, 2, 0, 1, 2], dtype&#x3D;int32)</span><br></pre></td></tr></table></figure>
<p>In [31]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mtx.indptr</span><br></pre></td></tr></table></figure>
<p>Out[31]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([0, 2, 3, 6], dtype&#x3D;int32)</span><br></pre></td></tr></table></figure>
<p>用<code>(data, indices, indptr)</code>元组创建:</p>
<p>In [32]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data &#x3D; np.array([1, 2, 3, 4, 5, 6])</span><br><span class="line">indices &#x3D; np.array([0, 2, 2, 0, 1, 2])</span><br><span class="line">indptr &#x3D; np.array([0, 2, 3, 6])</span><br><span class="line">mtx &#x3D; sparse.csr_matrix((data, indices, indptr), shape&#x3D;(3, 3))</span><br><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure>
<p>Out[32]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matrix([[1, 0, 2],</span><br><span class="line">        [0, 0, 3],</span><br><span class="line">        [4, 5, 6]])</span><br></pre></td></tr></table></figure>
<h4 id="2-5-2-2-6-压缩稀疏列格式-CSC"><a href="#2-5-2-2-6-压缩稀疏列格式-CSC" class="headerlink" title="2.5.2.2.6 压缩稀疏列格式 (CSC))"></a>2.5.2.2.6 压缩稀疏列格式 (CSC))</h4><ul>
<li><p>面向列</p>
<ul>
<li><p>三个Numpy数组: <code>indices</code>、<code>indptr</code>、<code>data</code></p>
</li>
<li><p><code>indices</code>是行索引的数组</p>
</li>
<li><p><code>data</code>是对应的非零值</p>
</li>
<li><p><code>indptr</code>指向<code>indices</code>和<code>data</code>开始的列</p>
</li>
<li><p>长度是<code>n_col + 1</code>, 最后一个条目 = 值数量 = <code>indices</code>和<code>data</code>的长度</p>
</li>
<li><p>第i列的非零值是行索引为<code>indices[indptr[i]:indptr[i+1]]</code>的<code>data[indptr[i]:indptr[i+1]]</code></p>
</li>
<li><p>项目 (i, j) 可以作为<code>data[indptr[j]+k]</code>访问, k是i在<code>indices[indptr[j]:indptr[j+1]]</code>的位置</p>
</li>
<li><p>```<br>_cs_matrix</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    的子类 (通用的 CSR&#x2F;CSC 功能性)</span><br><span class="line"></span><br><span class="line">    - &#96;_data_matrix&#96;的子类 (带有&#96;data&#96;属性的稀疏矩阵类)</span><br><span class="line"></span><br><span class="line">- 快速的矩阵和向量相乘及其他数学 (sparsetools)</span><br><span class="line"></span><br><span class="line">- 构建器接受：</span><br><span class="line"></span><br><span class="line">  - 密集矩阵 (数组)</span><br><span class="line">  - 稀疏矩阵</span><br><span class="line">  - 形状元组 (创建空矩阵)</span><br><span class="line">  - &#96;(data, ij)&#96;元组</span><br><span class="line">  - &#96;(data, indices, indptr)&#96;元组</span><br><span class="line"></span><br><span class="line">- 高效列切片、面向列的操作</span><br><span class="line"></span><br><span class="line">- 较慢的行切片、改变稀疏结构代价昂贵</span><br><span class="line"></span><br><span class="line">- 用途:</span><br><span class="line"></span><br><span class="line">  - 实际计算 (巨大多数线性求解器支持这个格式)</span><br><span class="line"></span><br><span class="line">##### 2.5.2.2.6.1 示例</span><br><span class="line"></span><br><span class="line">- 创建空CSC矩阵:</span><br><span class="line"></span><br><span class="line">In [33]:</span><br></pre></td></tr></table></figure>
<p>mtx = sparse.csc_matrix((3, 4), dtype=np.int8)<br>mtx.todense()</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[33]:</span><br></pre></td></tr></table></figure>
<p>matrix([[0, 0, 0, 0],</p>
<pre><code>[0, 0, 0, 0],
[0, 0, 0, 0]], dtype=int8)
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 用&#96;(data, ij)&#96;元组创建:</span><br><span class="line"></span><br><span class="line">In [34]:</span><br></pre></td></tr></table></figure>
<p>row = np.array([0, 0, 1, 2, 2, 2])<br>col = np.array([0, 2, 2, 0, 1, 2])<br>data = np.array([1, 2, 3, 4, 5, 6])<br>mtx = sparse.csc_matrix((data, (row, col)), shape=(3, 3))<br>mtx</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[34]:</span><br></pre></td></tr></table></figure>
<p><3x3 sparse matrix of type '<type 'numpy.int64'>'
with 6 stored elements in Compressed Sparse Column format></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">In [35]:</span><br></pre></td></tr></table></figure>
<p>mtx.todense()</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[35]:</span><br></pre></td></tr></table></figure>
<p>matrix([[1, 0, 2],</p>
<pre><code>[0, 0, 3],
[4, 5, 6]])
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">In [36]:</span><br></pre></td></tr></table></figure>
<p>mtx.data</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[36]:</span><br></pre></td></tr></table></figure>
<p>array([1, 4, 5, 2, 3, 6])</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">In [37]:</span><br></pre></td></tr></table></figure>
<p>mtx.indices</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[37]:</span><br></pre></td></tr></table></figure>
<p>array([0, 2, 2, 0, 1, 2], dtype=int32)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">In [38]:</span><br></pre></td></tr></table></figure>
<p>mtx.indptr</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[38]:</span><br></pre></td></tr></table></figure>
<p>array([0, 2, 3, 6], dtype=int32)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 用&#96;(data, indices, indptr)&#96;元组创建:</span><br><span class="line"></span><br><span class="line">In [39]:</span><br></pre></td></tr></table></figure>
<p>data = np.array([1, 4, 5, 2, 3, 6])<br>indices = np.array([0, 2, 2, 0, 1, 2])<br>indptr = np.array([0, 2, 3, 6])<br>mtx = sparse.csc_matrix((data, indices, indptr), shape=(3, 3))<br>mtx.todense()</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[39]:</span><br></pre></td></tr></table></figure>
<p>matrix([[1, 0, 2],</p>
<pre><code>[0, 0, 3],
[4, 5, 6]])
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.5.2.2.7 块压缩行格式 (BSR))</span><br><span class="line"></span><br><span class="line">- 本质上，CSR带有密集的固定形状的子矩阵而不是纯量的项目</span><br><span class="line"></span><br><span class="line">  - 块大小&#96;(R, C)&#96;必须可以整除矩阵的形状&#96;(M, N)&#96;</span><br><span class="line"></span><br><span class="line">  - 三个Numpy数组:</span><br></pre></td></tr></table></figure>
<p>indices</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">、</span><br></pre></td></tr></table></figure>
<p>indptr</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">、</span><br></pre></td></tr></table></figure>
<p>data</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    - &#96;indices&#96;是每个块列索引的数组</span><br><span class="line">    - &#96;data&#96;是形状为(nnz, R, C)对应的非零值</span><br><span class="line">    - ...</span><br><span class="line"></span><br><span class="line">  - &#96;_cs_matrix&#96;的子类 (通用的CSR&#x2F;CSC功能性)</span><br><span class="line"></span><br><span class="line">  - &#96;_data_matrix&#96;的子类 (带有&#96;data&#96;属性的稀疏矩阵类)</span><br><span class="line"></span><br><span class="line">- 快速矩阵向量相乘和其他的算术 (sparsetools)</span><br><span class="line"></span><br><span class="line">- 构建器接受:</span><br><span class="line"></span><br><span class="line">  - 密集矩阵 (数组)</span><br><span class="line">  - 稀疏矩阵</span><br><span class="line">  - 形状元组 (创建空的矩阵)</span><br><span class="line">  - &#96;(data, ij)&#96;元组</span><br><span class="line">  - &#96;(data, indices, indptr)&#96;元组</span><br><span class="line"></span><br><span class="line">- 许多对于带有密集子矩阵的稀疏矩阵算术操作比CSR更高效很多</span><br><span class="line"></span><br><span class="line">- 用途:</span><br><span class="line"></span><br><span class="line">  - 类似CSR</span><br><span class="line">  - 有限元素向量值离散化 ##### 2.5.2.2.7.1 示例</span><br><span class="line"></span><br><span class="line">- 创建空的&#96;(1, 1)&#96;块大小的（类似CSR...）的BSR矩阵:</span><br><span class="line"></span><br><span class="line">In [40]:</span><br></pre></td></tr></table></figure>
<p>mtx = sparse.bsr_matrix((3, 4), dtype=np.int8)<br>mtx</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[40]:</span><br></pre></td></tr></table></figure>
<p><3x4 sparse matrix of type '<type 'numpy.int8'>'
with 0 stored elements (blocksize = 1x1) in Block Sparse Row format></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">In [41]:</span><br></pre></td></tr></table></figure>
<p>mtx.todense()</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[41]:</span><br></pre></td></tr></table></figure>
<p>matrix([[0, 0, 0, 0],</p>
<pre><code>[0, 0, 0, 0],
[0, 0, 0, 0]], dtype=int8)
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 创建块大小&#96;(3, 2)&#96;的空BSR矩阵:</span><br><span class="line"></span><br><span class="line">In [42]:</span><br></pre></td></tr></table></figure>
<p>mtx = sparse.bsr_matrix((3, 4), blocksize=(3, 2), dtype=np.int8)<br>mtx</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[42]:</span><br></pre></td></tr></table></figure>
<3x4 sparse matrix of type '<type 'numpy.int8'>'
with 0 stored elements (blocksize = 3x2) in Block Sparse Row format></li>
</ul>
</li>
<li><p>一个bug?</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 用&#96;(1, 1)&#96;块大小 (类似 CSR...)&#96;(data, ij)&#96;的元组创建:</span><br><span class="line"></span><br><span class="line">In [43]:</span><br></pre></td></tr></table></figure>
<p>row = np.array([0, 0, 1, 2, 2, 2])<br>col = np.array([0, 2, 2, 0, 1, 2])<br>data = np.array([1, 2, 3, 4, 5, 6])<br>mtx = sparse.bsr_matrix((data, (row, col)), shape=(3, 3))<br>mtx</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[43]:</span><br></pre></td></tr></table></figure>
<p><3x3 sparse matrix of type '<type 'numpy.int64'>'
  with 6 stored elements (blocksize = 1x1) in Block Sparse Row format></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">In [44]:</span><br></pre></td></tr></table></figure>
<p>mtx.todense()</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[44]:</span><br></pre></td></tr></table></figure>
<p>matrix([[1, 0, 2],</p>
<pre><code>  [0, 0, 3],
  [4, 5, 6]])
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">In [45]:</span><br></pre></td></tr></table></figure>
<p>mtx.indices</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[45]:</span><br></pre></td></tr></table></figure>
<p>array([0, 2, 2, 0, 1, 2], dtype=int32)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">In [46]:</span><br></pre></td></tr></table></figure>
<p>mtx.indptr</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[46]:</span><br></pre></td></tr></table></figure>
<p>array([0, 2, 3, 6], dtype=int32)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 用&#96;(2, 1)&#96;块大小&#96;(data, indices, indptr)&#96;的元组创建:</span><br><span class="line"></span><br><span class="line">In [47]:</span><br></pre></td></tr></table></figure>
<p>indptr = np.array([0, 2, 3, 6])<br>indices = np.array([0, 2, 2, 0, 1, 2])<br>data = np.array([1, 2, 3, 4, 5, 6]).repeat(4).reshape(6, 2, 2)<br>mtx = sparse.bsr_matrix((data, indices, indptr), shape=(6, 6))<br>mtx.todense()</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[47]:</span><br></pre></td></tr></table></figure>
<p>matrix([[1, 1, 0, 0, 2, 2],</p>
<pre><code>  [1, 1, 0, 0, 2, 2],
  [0, 0, 0, 0, 3, 3],
  [0, 0, 0, 0, 3, 3],
  [4, 4, 5, 5, 6, 6],
  [4, 4, 5, 5, 6, 6]])
</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">In [48]:</span><br></pre></td></tr></table></figure>
<p>data</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Out[48]:</span><br></pre></td></tr></table></figure>
<p>array([[[1, 1],</p>
<pre><code>  [1, 1]],

 [[2, 2],
  [2, 2]],

 [[3, 3],
  [3, 3]],

 [[4, 4],
  [4, 4]],

 [[5, 5],
  [5, 5]],

 [[6, 6],
  [6, 6]]])
</code></pre><p>```</p>
</li>
</ul>
<h3 id="2-5-2-3-总结"><a href="#2-5-2-3-总结" class="headerlink" title="2.5.2.3 总结"></a>2.5.2.3 总结</h3><p>存储机制的总结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>格式</th>
<th>矩阵 * 向量</th>
<th>提取项目</th>
<th>灵活提取</th>
<th>设置项目</th>
<th>灵活设置</th>
<th>求解器</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>DIA</td>
<td>sparsetools</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>迭代</td>
<td>有数据数组，专门化</td>
</tr>
<tr>
<td>LIL</td>
<td>通过 CSR</td>
<td>是</td>
<td>是</td>
<td>是</td>
<td>是</td>
<td>迭代</td>
<td>通过CSR的算术, 增量构建</td>
</tr>
<tr>
<td>DOK</td>
<td>python</td>
<td>是</td>
<td>只有一个轴</td>
<td>是</td>
<td>是</td>
<td>迭代</td>
<td><code>O(1)</code>条目访问, 增量构建</td>
</tr>
<tr>
<td>COO</td>
<td>sparsetools</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>迭代</td>
<td>有数据数组, 便利的快速转换</td>
</tr>
<tr>
<td>CSR</td>
<td>sparsetools</td>
<td>是</td>
<td>是</td>
<td>慢</td>
<td>.</td>
<td>任何</td>
<td>有数据数组, 快速以行为主的操作</td>
</tr>
<tr>
<td>CSC</td>
<td>sparsetools</td>
<td>是</td>
<td>是</td>
<td>慢</td>
<td>.</td>
<td>任何</td>
<td>有数据数组, 快速以列为主的操作</td>
</tr>
<tr>
<td>BSR</td>
<td>sparsetools</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>专门化</td>
<td>有数据数组，专门化</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>scikit-learn</tag>
        <tag>scipy</tag>
      </tags>
  </entry>
  <entry>
    <title>sklearn稀疏矩阵</title>
    <url>/2020/03/04/2020-3-3-spare_matrix-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="sklearn稀疏矩阵"><a href="#sklearn稀疏矩阵" class="headerlink" title="sklearn稀疏矩阵"></a>sklearn稀疏矩阵</h1><h3 id="lil-matrix-使用两个列表储存非0元素，data保存每行中的非零元素-rows保存非零元素所在的列。这种格式也很适合逐个添加元素，并且能快速获取行相关的数据。"><a href="#lil-matrix-使用两个列表储存非0元素，data保存每行中的非零元素-rows保存非零元素所在的列。这种格式也很适合逐个添加元素，并且能快速获取行相关的数据。" class="headerlink" title="lil_matrix 使用两个列表储存非0元素，data保存每行中的非零元素,rows保存非零元素所在的列。这种格式也很适合逐个添加元素，并且能快速获取行相关的数据。"></a>lil_matrix 使用两个列表储存非0元素，data保存每行中的非零元素,rows保存非零元素所在的列。这种格式也很适合逐个添加元素，并且能快速获取行相关的数据。</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> lil_matrix</span><br><span class="line">l = lil_matrix((<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line">l[<span class="number">2</span>,<span class="number">3</span>] = <span class="number">1</span></span><br><span class="line">l[<span class="number">3</span>,<span class="number">4</span>] = <span class="number">2</span></span><br><span class="line">l[<span class="number">3</span>,<span class="number">2</span>] = <span class="number">3</span></span><br><span class="line">print(l.toarray())</span><br><span class="line">[[ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">3.</span>  <span class="number">0.</span>  <span class="number">2.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> l.data</span><br><span class="line">[[] [] [<span class="number">1.0</span>] [<span class="number">3.0</span>, <span class="number">2.0</span>] [] []]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> l.rows</span><br><span class="line">[[] [] [<span class="number">3</span>] [<span class="number">2</span>, <span class="number">4</span>] [] []]</span><br></pre></td></tr></table></figure>
<h3 id="dok-matrix和lil-matrix适用的场景是逐渐添加矩阵的元素。doc-matrix的策略是采用字典来记录矩阵中不为0的元素。自然，字典的key存的是记录元素的位置信息的元祖，value是记录元素的具体值。"><a href="#dok-matrix和lil-matrix适用的场景是逐渐添加矩阵的元素。doc-matrix的策略是采用字典来记录矩阵中不为0的元素。自然，字典的key存的是记录元素的位置信息的元祖，value是记录元素的具体值。" class="headerlink" title="dok_matrix和lil_matrix适用的场景是逐渐添加矩阵的元素。doc_matrix的策略是采用字典来记录矩阵中不为0的元素。自然，字典的key存的是记录元素的位置信息的元祖，value是记录元素的具体值。"></a>dok_matrix和lil_matrix适用的场景是逐渐添加矩阵的元素。doc_matrix的策略是采用字典来记录矩阵中不为0的元素。自然，字典的key存的是记录元素的位置信息的<a href="https://www.baidu.com/s?wd=%E5%85%83%E7%A5%96&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">元祖</a>，value是记录元素的具体值。</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> dok_matrix</span><br><span class="line">S = dok_matrix((<span class="number">5</span>,<span class="number">5</span>), dtype=np.float32)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        S[i,j] = i + j</span><br><span class="line"><span class="comment"># 返回的是非零元素的row_index,column_index</span></span><br><span class="line">S.nonzero()</span><br><span class="line">print(S.toarray())</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Machine Learnin</category>
      </categories>
      <tags>
        <tag>scipy</tag>
        <tag>稀疏矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title>如何处理不平衡的数据</title>
    <url>/2020/03/04/2020-3-3-unbalance_data-2020/</url>
    <content><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote>
<p>Machine Learning notes</p>
</blockquote>
<h1 id="如何处理不平衡的数据"><a href="#如何处理不平衡的数据" class="headerlink" title="如何处理不平衡的数据"></a>如何处理不平衡的数据</h1><p>对于不平衡数据集，一般的分类算法都倾向于将样本划分到多数类，体现在模型整体的准确率很好。但是模型并不是很好。常用的分类算法一般假设不同类的比例是均衡的，现实生活中经常遇到不平衡的数据集，比如广告点击预测（点击转化率一般都很小）、商品推荐（推荐的商品被购买的比例很低）、信用卡欺诈检测（欺诈的总数小数）等。</p>
<h4 id="在类别不平衡的情况下，对模型使用F值或者AUC值是更好的选择。"><a href="#在类别不平衡的情况下，对模型使用F值或者AUC值是更好的选择。" class="headerlink" title="在类别不平衡的情况下，对模型使用F值或者AUC值是更好的选择。"></a>在类别不平衡的情况下，对模型使用F值或者AUC值是更好的选择。</h4><h4 id="处理不平衡的数据，可以从两个方面考虑"><a href="#处理不平衡的数据，可以从两个方面考虑" class="headerlink" title="处理不平衡的数据，可以从两个方面考虑"></a>处理不平衡的数据，可以从两个方面考虑</h4><ol>
<li>是改变数据分布，从数据层面使得类别更为平衡<ol>
<li>欠采样 Undersampling 减少多数类样本的数量</li>
<li>过采样 Oversampling 增加少数样本的数据量</li>
<li>总和采样：将过采样和欠采样的结合</li>
</ol>
</li>
<li>改变分类算法，在传统分类基础上对不同的类别采用不同的加权方式，使得模型更加看重少数类。</li>
</ol>
<h2 id="欠采样"><a href="#欠采样" class="headerlink" title="欠采样"></a>欠采样</h2><h4 id="随机欠采样方法"><a href="#随机欠采样方法" class="headerlink" title="随机欠采样方法"></a>随机欠采样方法</h4><p>减少多数类样本的数量的最简单的方法便是随机剔除多数类的样本，可以事先设置多类数与少数类最终的数量比例ratio，在保留少数类样本不变的情况下，根据ratio随机选择多数类样本。</p>
<p>优点：操作简单，只依赖与样本分布，不依赖任何的距离信息，属于非启发式方法</p>
<p>缺点：会丢失一部分多类数的样本的信息，无法充分的利用已有的信息</p>
<h3 id="Tomek-link-用途"><a href="#Tomek-link-用途" class="headerlink" title="Tomek link 用途"></a>Tomek link 用途</h3><ol>
<li>欠采样：将Tomek link对中属于多数类的样本剔除</li>
<li>数据清洗：及那个Tomek link对中的两个样本都剔除</li>
</ol>
<h3 id="NearMiss方法"><a href="#NearMiss方法" class="headerlink" title="NearMiss方法"></a>NearMiss方法</h3><p>NearMiss方法是利用距离的远近剔除多数类样本的方法，实际操作中也是借助KNN</p>
<p>NearMiss-1:在多数类样本中选择与最近的3个少数类样本的平均距离最小的样本</p>
<p>NearMiss-2:在多数类样本中选择与最远的3个少数类样本的平均距离最小的样本</p>
<p>NearMiss-3:对于每个少数类样本，选择离它最近的给定的数量的多数样本</p>
<p><strong>NearMiss1考虑的是局部的，更倾向于比较集中的少数类附近找到更多的多数类样本。NearMiss2是考虑的是全局的</strong></p>
<p><strong>NearMiss3方法则会使得每一个少数类样本的附近都有足够多的多类样本，显然会使得模型的精确率高，召回率低</strong></p>
<p><strong>一般的情况下，NearMiss-2的效果较好</strong>。</p>
<h3 id="One-Sided-Selection"><a href="#One-Sided-Selection" class="headerlink" title="One-Sided Selection"></a>One-Sided Selection</h3><p>One-Sided Selection利用从上图得到的启发式想法，多数类样本包含四种情况：</p>
<ol>
<li>多数类中的噪声（noise），它们都各自紧贴着一个少数样本</li>
<li>边界样本，此类样本很容易被分错</li>
<li>多余（redundant）样本，因为在训练模型的时候，此类样本没有提供额外的有用信息，其类别信息可以容易地通过其他样本信息得到，此类冗余地样本会提供分类地代价，使得边界曲线移动。</li>
<li>安全样本，对于分类模型有着重要地作用。</li>
</ol>
<p>One-Sided Selection算法地目的是剔除多类数样本中样本中地噪声、边界样本和多余样本，其利用Tomek links剔除多数类样本中的噪声和边界样本，未被1-NN分类器错分的样本则被视为多余的样本，最终得到一个类别分布更为平衡的样本集合。</p>
<h2 id="过采样"><a href="#过采样" class="headerlink" title="过采样"></a>过采样</h2><h3 id="随机过采样"><a href="#随机过采样" class="headerlink" title="随机过采样"></a>随机过采样</h3><p>于前采样对应，增加少数类样本数类最简单的方法便是随机复制少数样本，可以事先设置多数类与少数类最终的数量比例ratio，在保留多数类样本不变的情况下，根据ratio随机复制少数类样本。</p>
<p>优点：操作简单，只依赖于样本的分布，不依赖于任何距离信息，属于非启发式方法</p>
<p>缺点：重复样本过多，容易造成分类器的过拟合</p>
<h3 id="SMOTE-Synthetic-Minority-Over-sampling-Technique-即合成少数类样本的过采样技术"><a href="#SMOTE-Synthetic-Minority-Over-sampling-Technique-即合成少数类样本的过采样技术" class="headerlink" title="SMOTE(Synthetic Minority Over-sampling Technique)即合成少数类样本的过采样技术"></a>SMOTE(Synthetic Minority Over-sampling Technique)即合成少数类样本的过采样技术</h3><p>SMOTE的主要思想是通过一些位置相近的少数类样本中生成新的样本平衡类别的目的，由于不是简单的复制少数类样本，因此可以一定程度上避免分类器过度拟合，</p>
<h3 id="Borderline-SMOTE"><a href="#Borderline-SMOTE" class="headerlink" title="Borderline SMOTE"></a>Borderline SMOTE</h3><p>相对于SMOTE算法对所有少数类样本都是一视同仁的，其利用边界位置的样本信息产生新的样本</p>
<p>Borderline SMOTE-1：从少数类样本集合P中得到k个最近邻样本，在随机选择样本点和xi做随机的线性插值产生新的少数类样本</p>
<p>Borderline SMOTE-2：从少数类样本集合P和多数类样本集合N中分别得到k个最近邻样本<br>Pk和Nk。设定一个比例α，在Pk中选出a比例的样本点和xi作随机的线性插值产生新的少数<br>类样本，方法同Borderline SMOTE-1；在Nk中选出1-a比例的样本点和xi作随机的线性插<br>值产生新的少数类样本，此处的随机数范围选择的是(0,0.5)，即使得产生的新的样本点更靠<br>近少数类样本 </p>
<h2 id="综合采样"><a href="#综合采样" class="headerlink" title="综合采样"></a>综合采样</h2><h3 id="SMOTE-Tomek-links"><a href="#SMOTE-Tomek-links" class="headerlink" title="SMOTE+Tomek links"></a>SMOTE+Tomek links</h3><ol>
<li>利用SMOTE方法生成新的少数类样本，得到扩充后的数据集</li>
<li>剔除数据集中Tomek links对</li>
</ol>
<p>普通SMOTE方法生成的少数类样本是通过线性差值得到的，在平衡类别分布的同时也扩张<br>了少数类的样本空间，产生的问题是可能原本属于多数类样本的空间被少数类“入<br>侵”（invade），容易造成模型的过拟合。<br>Tomek links对寻找的是那种噪声点或者边界点，可以很好地解决“入侵”的问题。 </p>
<h3 id="SMOTE-KNN"><a href="#SMOTE-KNN" class="headerlink" title="SMOTE+KNN"></a>SMOTE+KNN</h3><ol>
<li>利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T；</li>
<li>对T中的每一个样本使用KNN（一般k取3）方法预测，若预测结果和实际类别标签不<br>符，则剔除该样本 </li>
</ol>
<h2 id="Informed-Undersampling"><a href="#Informed-Undersampling" class="headerlink" title="Informed Undersampling"></a>Informed Undersampling</h2><p>是对欠采样的补充</p>
<p>随机欠采样会导致信息的缺失，EasyEnsemble的想法则是多次随机欠采样，尽可能全面地全面包含所有地信息，算法的特点是利用boosting减少偏差（AdaBoost）,bagging减少方差（集成分类器），实际应用中时候可以采用不同的分类器提高分类的效果。</p>
<ol>
<li>随机欠采样方法</li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>不平衡数据处理</tag>
      </tags>
  </entry>
</search>
