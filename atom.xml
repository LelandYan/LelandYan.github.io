<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LelandYan</title>
  
  <subtitle>一个沉默的人</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-03-04T06:20:34.382Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>LelandYan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linear Regression for Classification</title>
    <link href="http://yoursite.com/2020/03/04/2018-12-10-linear_regression-2018/"/>
    <id>http://yoursite.com/2020/03/04/2018-12-10-linear_regression-2018/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:20:34.382Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="多元线性回归问题"><a href="#多元线性回归问题" class="headerlink" title="多元线性回归问题"></a>多元线性回归问题</h1><ol><li><p>代价函数(整体数据集)，损失函数(对于每个数据的误差)</p></li><li><p>learning_rate(从小的尝试比如0.0006)</p></li><li><p><strong>scaling the features(特征缩放)—适用与梯度下降改进 -Mean normalization(均值归一化)</strong></p><ol><li>min-max标准化</li></ol><p>X(每个数据) - U(特征值均值))/(Max-Min)</p><ol><li>Z-score标准化方法</li></ol><p>X(每个数据) - U(特征值均值))/(std)</p><ol><li>归一化</li></ol><p>把数据变成(0,1)或者(1,1)之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速 </p></li><li><p><strong> normal equation(正规方程) or Gradient Descent </strong></p><ol><li>normal equation is suitable for samll features(计算量比较大)</li><li>Gradient descent is suitable for a large number for features(<br>  <img src="/images/feature_scaling.png" alt="">)<br>但是受数据的影响较大，所以对与Gradient descent来说需要进行feature scaling)</li><li><p>normalize equation(正规方程)的公式推导</p><p>代价函数</p><p> ​    <script type="math/tex">J(\theta)=J(\theta_0,\ldots,\theta_n)=\frac {1} {2m} \sum_{i=1}^{m} {(h_\theta(x^{(i)})-y^{(i)})^2}</script></p><p> ​    <script type="math/tex">J(\theta)=\frac {1}{2m}(X\theta-y)^T(X\theta-y)</script></p><p> ​    <script type="math/tex">J(\theta)=\theta^TX^TX\theta-(X\theta)^Ty-y^TX\theta+y^Ty</script></p><p> ​    <script type="math/tex">J(\theta)=\theta^TX^TX\theta-2(X\theta)^Ty+y^Ty</script></p><p> ​    <script type="math/tex">\frac {\partial}{\partial\theta}J(\theta)=2X^TX\theta-2X^Ty=0</script></p><p> ​    <script type="math/tex">\theta=(X^TX)^{-1}X^Ty</script></p></li></ol></li><li><p>矩阵不可逆：</p><p>1.矩阵存在线性相关的特征之值</p><p>2.矩阵所对应的行列式为0</p></li><li><p>homework(python)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line">data = pd.read_table(<span class="string">"ex1data1.txt"</span>, header=<span class="literal">None</span>, delimiter=<span class="string">","</span>, encoding=<span class="string">"gb2312"</span>)</span><br><span class="line">m = len(data)</span><br><span class="line">data_x = np.array(data)[:, <span class="number">0</span>].reshape(m, <span class="number">1</span>)</span><br><span class="line">data_y = np.array(data)[:, <span class="number">1</span>].reshape(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the first task</span></span><br><span class="line">print(np.eye(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the second task</span></span><br><span class="line"><span class="comment"># plt.scatter(data_x, data_y, color='r', marker='x',label="Training Data")</span></span><br><span class="line"><span class="comment"># plt.ylabel('Profit in $10,000s')</span></span><br><span class="line"><span class="comment"># plt.xlabel('Population of City in 10,000s')</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the third task</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    inner = np.power(((X * theta) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner / (<span class="number">2</span> * len(X)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X,y,theta,alpha,epoch)</span>:</span></span><br><span class="line">    cost = np.zeros(epoch)</span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">        temp = theta - (alpha / m) * ((X * theta - y).T * X).T</span><br><span class="line">        theta = temp</span><br><span class="line">        cost[i] = computeCost(X,y,theta)</span><br><span class="line">    <span class="keyword">return</span>  theta,cost</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_equations</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    theta = np.linalg.inv(X.T*X)*X.T*y</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">X = np.matrix(np.concatenate((np.ones((m, <span class="number">1</span>)), data_x), axis=<span class="number">1</span>))</span><br><span class="line">y = np.matrix(data_y)</span><br><span class="line">theta = np.matrix(np.zeros([<span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">epoch = <span class="number">1000</span></span><br><span class="line">final_theta,cost = gradientDescent(X,y,theta,alpha,epoch)</span><br><span class="line">print(final_theta)</span><br><span class="line">final_theta = normal_equations(X,y)</span><br><span class="line">print(final_theta)</span><br><span class="line">x = np.array(list(data_x[:,<span class="number">0</span>]))</span><br><span class="line">f = final_theta[<span class="number">0</span>,<span class="number">0</span>] + final_theta[<span class="number">1</span>,<span class="number">0</span>] * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.scatter(data_x, data_y, color='r', marker='x',label="Training Data")</span></span><br><span class="line"><span class="comment"># plt.ylabel('Profit in $10,000s')</span></span><br><span class="line"><span class="comment"># plt.xlabel('Population of City in 10,000s')</span></span><br><span class="line"><span class="comment"># plt.plot(x,f,label="Prediction")</span></span><br><span class="line"><span class="comment"># plt.legend()</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.plot(np.arange(epoch),cost,'r')</span></span><br><span class="line"><span class="comment"># plt.xlabel("Iteration")</span></span><br><span class="line"><span class="comment"># plt.ylabel("Cost")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">theta0_vals = np.linspace(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">100</span>)</span><br><span class="line">theta1_vals = np.linspace(<span class="number">-1</span>,<span class="number">4</span>,<span class="number">100</span>)</span><br><span class="line">J_vals = np.zeros((len(theta0_vals),len(theta1_vals)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta0_vals)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(theta1_vals)):</span><br><span class="line">        t = np.matrix(np.array([theta0_vals[i],theta1_vals[j]]).reshape((<span class="number">2</span>,<span class="number">1</span>)))</span><br><span class="line">        J_vals[i,j] = computeCost(X,y,t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################## 3D图 #######################################</span></span><br><span class="line"><span class="comment"># fig = plt.figure()</span></span><br><span class="line"><span class="comment"># ax = Axes3D(fig)</span></span><br><span class="line"><span class="comment"># ax.plot_surface(theta0_vals,theta1_vals,J_vals,rstride=1,cstride=1,cmap="rainbow")</span></span><br><span class="line"><span class="comment"># plt.xlabel('theta_0')</span></span><br><span class="line"><span class="comment"># plt.ylabel('theta_1')</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment">######################################## 3D图 #######################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################## 等高线图 #######################################</span></span><br><span class="line"><span class="comment"># plt.figure()</span></span><br><span class="line"><span class="comment"># plt.contourf(theta0_vals, theta1_vals, J_vals, 20, alpha = 0.6, cmap = plt.cm.hot)</span></span><br><span class="line"><span class="comment"># a = plt.contour(theta0_vals, theta1_vals, J_vals, colors = 'black')</span></span><br><span class="line"><span class="comment"># plt.clabel(a,inline=1,fontsize=10)</span></span><br><span class="line"><span class="comment"># plt.plot(theta[0,0],theta[1,0],'r',marker='x')</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment">######################################## 等高线图 #######################################</span></span><br><span class="line"></span><br><span class="line">data2 = pd.read_table(<span class="string">"ex1data2.txt"</span>, header=<span class="literal">None</span>, delimiter=<span class="string">","</span>, encoding=<span class="string">"gb2312"</span>)</span><br><span class="line"></span><br><span class="line">data2 = (data2 - data2.mean()) / data2.std()</span><br><span class="line">m = len(data2)</span><br><span class="line">data_x = np.array(data2)[:, <span class="number">0</span>:<span class="number">2</span>].reshape(m, <span class="number">2</span>)</span><br><span class="line">data_y = np.array(data2)[:, <span class="number">2</span>].reshape(m, <span class="number">1</span>)</span><br><span class="line">data_x = np.concatenate((np.zeros((m,<span class="number">1</span>)),data_x),axis=<span class="number">1</span>)</span><br><span class="line">X = np.matrix(data_x)</span><br><span class="line">y = np.matrix(data_y)</span><br><span class="line">theta = np.matrix(np.zeros((<span class="number">3</span>,<span class="number">1</span>)))</span><br><span class="line">final_theta,cost = gradientDescent(X,y,theta,alpha,epoch)</span><br><span class="line"></span><br><span class="line">plt.plot(np.arange(epoch),cost,<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;多元线性
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
      <category term="multi-regression" scheme="http://yoursite.com/tags/multi-regression/"/>
    
  </entry>
  
  <entry>
    <title>Hypothesis Testing</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-Hypothesis_Testing-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-Hypothesis_Testing-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:18:11.477Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="Hypothesis-Testing"><a href="#Hypothesis-Testing" class="headerlink" title="Hypothesis Testing"></a>Hypothesis Testing</h1><p>是推断统计的最后一步，是根据一定的假设条件由假设条件由样本推断总体的一种方法</p><p>首先提出你的假设</p><p>其实检验假设其实就是假设和检验两步，先提出假设，之后在验证假设时候合理</p><p>4、显著水平<br>总共猜10次，那么是出现7次猜对，可以认为有特殊能力，还是9次猜对之后我才能确认有特殊能力，这是一个较为主观的标准。</p><p>我们一般认为</p><p>P-value&lt;=0.05</p><p>就可以认为假设是不正确的。</p><p>0.05这个标准就是显著水平，当然选择多少作为显著水平也是主观的。</p><p>比如，我们猜奶茶的例子，如果取单侧P值，那么根据我们的计算，如果10次猜对9次：</p><h2 id="P-value-P-9-lt-X-lt-10-0-01-lt-0-05"><a href="#P-value-P-9-lt-X-lt-10-0-01-lt-0-05" class="headerlink" title="P-value=P(9&lt;=X&lt;=10)=0.01&lt;=0.05"></a>P-value=P(9&lt;=X&lt;=10)=0.01&lt;=0.05</h2><p><strong>检验统计量</strong>是用于假设检验计算的统计量。在零假设情况下，这项统计量服从一个给定的概率分布，而这在另一种假设下则不然。从而若检验统计量的值落在上述分布的临界值之外，则可认为前述零假设未必正确。统计学中，用于检验假设量是否正确的量。常用的检验统计量有t统计量，Z统计量等。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Hypo
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Hypothesis Testing" scheme="http://yoursite.com/tags/Hypothesis-Testing/"/>
    
      <category term="数理统计" scheme="http://yoursite.com/tags/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>多元线性回归问题</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-Liner_Regression%20with%20multiple%20variable-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-Liner_Regression%20with%20multiple%20variable-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:33:10.564Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="多元线性回归问题"><a href="#多元线性回归问题" class="headerlink" title="多元线性回归问题"></a>多元线性回归问题</h1><ol><li><p>代价函数(整体数据集)，损失函数(对于每个数据的误差)</p></li><li><p>learning_rate(从小的尝试比如0.0006)</p></li></ol><h3 id="scaling-the-features-特征缩放-—适用与梯度下降改进-Mean-normalization-均值归一化"><a href="#scaling-the-features-特征缩放-—适用与梯度下降改进-Mean-normalization-均值归一化" class="headerlink" title="scaling the features(特征缩放)—适用与梯度下降改进 -Mean normalization(均值归一化)"></a>scaling the features(特征缩放)—适用与梯度下降改进 -Mean normalization(均值归一化)</h3><pre><code>1.min-max标准化X(每个数据) - U(特征值均值))/(Max-Min)2.Z-score标准化方法X(每个数据) - U(特征值均值))/(std)3.归一化把数据变成(0,1)或者(1,1)之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速 </code></pre><h3 id="normal-equation-正规方程-or-Gradient-Descent"><a href="#normal-equation-正规方程-or-Gradient-Descent" class="headerlink" title="normal equation(正规方程) or Gradient Descent"></a>normal equation(正规方程) or Gradient Descent</h3><ol><li><p>normal equation is suitable for samll features(计算量比较大)</p></li><li><p>Gradient descent is suitable for a large number for features(<br> <img src="/images/feature_scaling.png" alt="">)<br> 但是受数据的影响较大，所以对与Gradient descent来说需要进行feature scaling)</p></li><li><p>normalize equation(正规方程)的公式推导 代价函数</p><script type="math/tex; mode=display"> J(\theta)=J(\theta_0,\ldots,\theta_n)=\frac {1} {2m} \sum_{i=1}^{m} {(h_\theta(x^{(i)})-y^{(i)})^2}</script><script type="math/tex; mode=display"> J(\theta)=\frac {1}{2m}(X\theta-y)^T(X\theta-y)</script><script type="math/tex; mode=display">   J(\theta)=\theta^TX^TX\theta-(X\theta)^Ty-y^TX\theta+y^Ty</script><script type="math/tex; mode=display">   J(\theta)=\theta^TX^TX\theta-2(X\theta)^Ty+y^Ty</script><script type="math/tex; mode=display"> \frac {\partial}{\partial\theta}J(\theta)=2X^TX\theta-2X^Ty=0</script><p> ​                                            <script type="math/tex">\theta=(X^TX)^{-1}X^Ty</script></p></li><li><p>矩阵不可逆：</p><p> 1.矩阵存在线性相关的特征之值</p><p> 2.矩阵所对应的行列式为0</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;多元线性
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="multi-regression" scheme="http://yoursite.com/tags/multi-regression/"/>
    
  </entry>
  
  <entry>
    <title>logistic regression(classification)</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-Logistic_Regression_Classification-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-Logistic_Regression_Classification-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:32:16.285Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="logistic-regression-classification"><a href="#logistic-regression-classification" class="headerlink" title="logistic regression(classification)"></a>logistic regression(classification)</h1><p>1.Hypothesis Representation</p><p>​    运用sigmoid function(logistic function)来将函数的值域映射到[0-1]</p><p>​    </p><script type="math/tex; mode=display">sigimoid=h_0(x)=\frac {1}{1+e^-x}</script><p>sigmoid函数图像的为<br><img src="/images/sigmoid.png" alt=""></p><p>2.decision boundary<br><img src="/images/decision_boudary.png" alt=""></p><p><img src="/images/decision_boudary2.png" alt=""></p><ol><li><p>cost function</p><p>这里的代价函数不再使用平方差的均值的方法，因为使用的话，通过非线性函数sigimoid函数，可能变成非凸函数，也就是说，可能存在多个局部最优的解，这将导致梯度下降算法不再有效</p><p><img src="/images/square_functino.png" alt="Image text"></p><p>这里可以更改代价函数为</p><p><img src="/images/cost_functinon.png" alt=""></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;logi
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="logistic regression" scheme="http://yoursite.com/tags/logistic-regression/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程线性回归训练题目一</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-Programming_Exercise_1_Linear_Regression-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-Programming_Exercise_1_Linear_Regression-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:24:14.443Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="李宏毅机器学习课程线性回归训练题目一"><a href="#李宏毅机器学习课程线性回归训练题目一" class="headerlink" title="李宏毅机器学习课程线性回归训练题目一"></a>李宏毅机器学习课程线性回归训练题目一</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line">data = pd.read_table(<span class="string">"ex1data1.txt"</span>, header=<span class="literal">None</span>, delimiter=<span class="string">","</span>, encoding=<span class="string">"gb2312"</span>)</span><br><span class="line">m = len(data)</span><br><span class="line">data_x = np.array(data)[:, <span class="number">0</span>].reshape(m, <span class="number">1</span>)</span><br><span class="line">data_y = np.array(data)[:, <span class="number">1</span>].reshape(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the first task</span></span><br><span class="line">print(np.eye(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the second task</span></span><br><span class="line"><span class="comment"># plt.scatter(data_x, data_y, color='r', marker='x',label="Training Data")</span></span><br><span class="line"><span class="comment"># plt.ylabel('Profit in $10,000s')</span></span><br><span class="line"><span class="comment"># plt.xlabel('Population of City in 10,000s')</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the third task</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    inner = np.power(((X * theta) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner / (<span class="number">2</span> * len(X)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X,y,theta,alpha,epoch)</span>:</span></span><br><span class="line">    cost = np.zeros(epoch)</span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">        temp = theta - (alpha / m) * ((X * theta - y).T * X).T</span><br><span class="line">        theta = temp</span><br><span class="line">        cost[i] = computeCost(X,y,theta)</span><br><span class="line">    <span class="keyword">return</span>  theta,cost</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_equations</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    theta = np.linalg.inv(X.T*X)*X.T*y</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">X = np.matrix(np.concatenate((np.ones((m, <span class="number">1</span>)), data_x), axis=<span class="number">1</span>))</span><br><span class="line">y = np.matrix(data_y)</span><br><span class="line">theta = np.matrix(np.zeros([<span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">epoch = <span class="number">1000</span></span><br><span class="line">final_theta,cost = gradientDescent(X,y,theta,alpha,epoch)</span><br><span class="line">print(final_theta)</span><br><span class="line">final_theta = normal_equations(X,y)</span><br><span class="line">print(final_theta)</span><br><span class="line">x = np.array(list(data_x[:,<span class="number">0</span>]))</span><br><span class="line">f = final_theta[<span class="number">0</span>,<span class="number">0</span>] + final_theta[<span class="number">1</span>,<span class="number">0</span>] * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.scatter(data_x, data_y, color='r', marker='x',label="Training Data")</span></span><br><span class="line"><span class="comment"># plt.ylabel('Profit in $10,000s')</span></span><br><span class="line"><span class="comment"># plt.xlabel('Population of City in 10,000s')</span></span><br><span class="line"><span class="comment"># plt.plot(x,f,label="Prediction")</span></span><br><span class="line"><span class="comment"># plt.legend()</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.plot(np.arange(epoch),cost,'r')</span></span><br><span class="line"><span class="comment"># plt.xlabel("Iteration")</span></span><br><span class="line"><span class="comment"># plt.ylabel("Cost")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">theta0_vals = np.linspace(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">100</span>)</span><br><span class="line">theta1_vals = np.linspace(<span class="number">-1</span>,<span class="number">4</span>,<span class="number">100</span>)</span><br><span class="line">J_vals = np.zeros((len(theta0_vals),len(theta1_vals)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta0_vals)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(theta1_vals)):</span><br><span class="line">        t = np.matrix(np.array([theta0_vals[i],theta1_vals[j]]).reshape((<span class="number">2</span>,<span class="number">1</span>)))</span><br><span class="line">        J_vals[i,j] = computeCost(X,y,t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### # fig = plt.figure()</span></span><br><span class="line"><span class="comment"># ax = Axes3D(fig)</span></span><br><span class="line"><span class="comment"># ax.plot_surface(theta0_vals,theta1_vals,J_vals,rstride=1,cstride=1,cmap="rainbow")</span></span><br><span class="line"><span class="comment"># plt.xlabel('theta_0')</span></span><br><span class="line"><span class="comment"># plt.ylabel('theta_1')</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment">### </span></span><br><span class="line"><span class="comment">### # plt.figure()</span></span><br><span class="line"><span class="comment"># plt.contourf(theta0_vals, theta1_vals, J_vals, 20, alpha = 0.6, cmap = plt.cm.hot)</span></span><br><span class="line"><span class="comment"># a = plt.contour(theta0_vals, theta1_vals, J_vals, colors = 'black')</span></span><br><span class="line"><span class="comment"># plt.clabel(a,inline=1,fontsize=10)</span></span><br><span class="line"><span class="comment"># plt.plot(theta[0,0],theta[1,0],'r',marker='x')</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment">### </span></span><br><span class="line">data2 = pd.read_table(<span class="string">"ex1data2.txt"</span>, header=<span class="literal">None</span>, delimiter=<span class="string">","</span>, encoding=<span class="string">"gb2312"</span>)</span><br><span class="line"></span><br><span class="line">data2 = (data2 - data2.mean()) / data2.std()</span><br><span class="line">m = len(data2)</span><br><span class="line">data_x = np.array(data2)[:, <span class="number">0</span>:<span class="number">2</span>].reshape(m, <span class="number">2</span>)</span><br><span class="line">data_y = np.array(data2)[:, <span class="number">2</span>].reshape(m, <span class="number">1</span>)</span><br><span class="line">data_x = np.concatenate((np.zeros((m,<span class="number">1</span>)),data_x),axis=<span class="number">1</span>)</span><br><span class="line">X = np.matrix(data_x)</span><br><span class="line">y = np.matrix(data_y)</span><br><span class="line">theta = np.matrix(np.zeros((<span class="number">3</span>,<span class="number">1</span>)))</span><br><span class="line">final_theta,cost = gradientDescent(X,y,theta,alpha,epoch)</span><br><span class="line"></span><br><span class="line">plt.plot(np.arange(epoch),cost,<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;李宏毅机
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
      <category term="Axes3D" scheme="http://yoursite.com/tags/Axes3D/"/>
    
  </entry>
  
  <entry>
    <title>AUC、ROC详解</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-auc_roc-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-auc_roc-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:34:15.038Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="AUC、ROC详解"><a href="#AUC、ROC详解" class="headerlink" title="AUC、ROC详解"></a>AUC、ROC详解</h1><p>AUC:一个正例，一个负例，预测为正的概率值比预测为负的概率还要大的可能性</p><p>根据定义：我们最直观的有两种计算auc的方法</p><ol><li>绘制ROC曲线，roc曲线下面的面积就是AUC的值</li><li>假设共有（m+n）个样本，其中正样本m个，负样本n个，共有m<em> n 个样本对，计数，正样本预测为正样本的概率概率大于负样本预测为正样本的概率记为1，累加计数，然后除以（m</em>  n）就是AUC的值</li></ol><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>ROC曲线：接受者操作特征（receiveroperating characteristic），roc曲线上的每个点反应着对同一信号刺激的感受性。</p><p>横轴：负正类率（false postive rate FPR）特异度，划分实例中所有的负例占所有负例的比例 （1-Specificity）</p><p>纵轴：真正类率 （true postive rate TPR）灵敏度，Sensitivity(正类覆盖率)</p><p>2针对一个二分类问题，将实例分成正类(postive)或者负类(negative)。但是实际中分类时，会出现四种情况.</p><p>(1)若一个实例是正类并且被预测为正类，即为真正类(True Postive TP)</p><p>(2)若一个实例是正类，但是被预测成为负类，即为假负类(False Negative FN)</p><p>(3)若一个实例是负类，但是被预测成为正类，即为假正类(False Postive FP)</p><p>(4)若一个实例是负类，但是被预测成为负类，即为真负类(True Negative TN)</p><p>TP:正确的肯定数目</p><p>FN:漏报，没有找到正确匹配的数目</p><p>FP:误报，没有的匹配不正确</p><p>(1)真正类率(True Postive Rate)TPR: TP/(TP+FN),代表分类器预测的正类中实际正实例占所有正实例的比例。Sensitivity</p><p>(2)负正类率(False Postive Rate)FPR: FP/(FP+TN)，代表分类器预测的正类中实际负实例占所有负实例的比例。1-Specificity</p><p>AUC(Area under Curve)：Roc曲线下的面积，介于0.1和1之间。Auc作为数值可以直观的评价分类器的好坏，值越大越好。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;AUC、
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
      <category term="machine learning metrics" scheme="http://yoursite.com/categories/Machine-Learning/machine-learning-metrics/"/>
    
    
  </entry>
  
  <entry>
    <title>随机森林中有关熵概念</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-RandomForest-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-RandomForest-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:29:59.386Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="随机森林中有关熵概念"><a href="#随机森林中有关熵概念" class="headerlink" title="随机森林中有关熵概念"></a>随机森林中有关熵概念</h1><ol><li>使熵概率越大，熵越小,所以是减函数</li><li>使熵可加，所以是log</li><li>熵可以看成为目标函数的期望</li><li>熵：H(X) = -P(x )logP(x)</li><li>条件熵：H(X,Y)-H(X)=H(Y | X)</li></ol><p>2.信息增益</p><ol><li>互信息：H(X)+H(Y)-H(X,Y)=I(X,Y)</li><li>信息增益表示特征a的信息使x的信息的不确定性减少的程度</li><li>特征A对训练数据集D的信息增益g(D,A)=H(D)-H(D |A),也就互信息</li><li>选择信息增益最大的特征作为当前的分类特征</li></ol><p>3.样本不均衡的常用方法</p><ol><li>对多的样本进行欠采样</li><li>对多的样本进行聚类</li><li>对少的数据进行过采样</li><li>随机插值得到新的样本</li></ol><p>4.信息增益率 g(d,a) = g(D,a)-h(a)</p><p>5.gini系数 p(x)（1-p(x)）</p><p>6.决策树的评价</p><p>7.决策树防止过拟合</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;随机森林
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="RandomForest" scheme="http://yoursite.com/tags/RandomForest/"/>
    
      <category term="信息增益" scheme="http://yoursite.com/tags/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A/"/>
    
      <category term="熵" scheme="http://yoursite.com/tags/%E7%86%B5/"/>
    
  </entry>
  
  <entry>
    <title>Classifition metrics</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-classifition_metrics-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-classifition_metrics-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:34:48.880Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="Classifition-metrics"><a href="#Classifition-metrics" class="headerlink" title="Classifition metrics"></a>Classifition metrics</h1><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">confusion_matrix(y_train_5, y_train_pred)</span><br><span class="line">array([[<span class="number">53272</span>, <span class="number">1307</span>],</span><br><span class="line">        [ <span class="number">1077</span>, <span class="number">4344</span>]])</span><br></pre></td></tr></table></figure><h2 id="准确率和召回率"><a href="#准确率和召回率" class="headerlink" title="准确率和召回率"></a>准确率和召回率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>precision_score(y_train_5, y_pred) <span class="comment"># == 4344 / (4344 + 1307)</span></span><br><span class="line"><span class="number">0.76871350203503808</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>recall_score(y_train_5, y_train_pred) <span class="comment"># == 4344 / (4344 + 1077)</span></span><br><span class="line"><span class="number">0.7913669064748201</span></span><br></pre></td></tr></table></figure><h3 id="F1-score"><a href="#F1-score" class="headerlink" title="F1 score"></a>F1 score</h3><p>通常结合准确率和召回率会更加方便，这个指标叫做“F1 值”，特别是当你需要一个简单的方法去比较两个分类器的优劣的时候。F1 值是准确率和召回率的调和平均。普通的平均值平等地看待所有的值，而调和平均会给小的值更大的权重。所以，要想分类器得到一个高的 F1 值，需要召回率和准确率同时高。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f1_score(y_train_5, y_train_pred)</span><br><span class="line"><span class="number">0.78468208092485547</span></span><br></pre></td></tr></table></figure><h3 id="准确率和召回率的关系"><a href="#准确率和召回率的关系" class="headerlink" title="准确率和召回率的关系"></a>准确率和召回率的关系</h3><p>Scikit-Learn 不让你直接设置阈值，但是它给你提供了设置决策分数的方法，这个决策分数可以用来产生预测。它不是调用分类器的<code>predict()</code>方法，而是调用<code>decision_function()</code>方法。这个方法返回每一个样例的分数值，然后基于这个分数值，使用你想要的任何阈值做出预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; y_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_scores</span><br><span class="line">array([ <span class="number">161855.74572176</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>threshold = <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_some_digit_pred = (y_scores &gt; threshold)</span><br><span class="line">array([ <span class="literal">True</span>], dtype=bool)</span><br><span class="line"><span class="string">"""SGDClassifier用了一个等于 0 的阈值，所以前面的代码返回了跟predict()方法一样的结果（都返回了true）。让我们提高这个阈值"""</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>threshold = <span class="number">200000</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_some_digit_pred = (y_scores &gt; threshold)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_some_digit_pred</span><br><span class="line">array([<span class="literal">False</span>], dtype=bool)</span><br><span class="line"></span><br><span class="line"><span class="string">"""使用cross_val_predict()得到每一个样例的分数值，但是这一次指定返回一个决策分数，而不是预测值。"""</span></span><br><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, </span><br><span class="line">                            method=<span class="string">"decision_function"</span>)</span><br><span class="line"><span class="comment"># y_scores返回和样本数量相同的决策分数</span></span><br><span class="line"><span class="keyword">if</span> y_scores.ndim == <span class="number">2</span>:</span><br><span class="line">    y_scores = y_scores[:, <span class="number">1</span>]</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line"></span><br><span class="line">precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)</span><br><span class="line"><span class="comment"># 绘制precisoin和recall曲线</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_recall_vs_threshold</span><span class="params">(precisions, recalls, thresholds)</span>:</span></span><br><span class="line">    plt.plot(thresholds, precisions[:<span class="number">-1</span>], <span class="string">"b--"</span>, label=<span class="string">"Precision"</span>)</span><br><span class="line">    plt.plot(thresholds, recalls[:<span class="number">-1</span>], <span class="string">"g-"</span>, label=<span class="string">"Recall"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Threshold"</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">"upper left"</span>)</span><br><span class="line">    plt.ylim([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plot_precision_recall_vs_threshold(precisions, recalls, thresholds)</span><br><span class="line">plt.show()    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_vs_recall</span><span class="params">(precisions, recalls)</span>:</span></span><br><span class="line">    plt.plot(recalls, precisions, <span class="string">"b-"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Recall"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Precision"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plot_precision_vs_recall(precisions, recalls)</span><br><span class="line">save_fig(<span class="string">"precision_vs_recall_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://hand2st.apachecn.org/images/chapter_3/chapter3.4.jpeg" alt="图3-4 准确率和召回率对决策阈值"></p><p><img src="https://hand2st.apachecn.org/images/chapter_3/chapter3.5.jpeg" alt="图3-5 准确率对召回率"></p><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>受试者工作特征（ROC）曲线是另一个二分类器常用的工具。它非常类似与准确率/召回率曲线，但不是画出准确率对召回率的曲线，ROC 曲线是真正例率（true positive rate，另一个名字叫做召回率）对假正例率（false positive rate, FPR）的曲线。FPR 是反例被错误分成正例的比率。它等于 1 减去真反例率（true negative rate， TNR）。TNR是反例被正确分类的比率。TNR也叫做特异性。所以 ROC 曲线画出召回率对（1 减特异性）的曲线。</p><p><img src="https://hand2st.apachecn.org/images/chapter_3/chapter3.6.jpeg" alt="图3-6 ROC曲线"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line"><span class="string">"""使用cross_val_predict()得到每一个样例的分数值，但是这一次指定返回一个决策分数，而不是预测值。"""</span></span><br><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, </span><br><span class="line">                            method=<span class="string">"decision_function"</span>)</span><br><span class="line"><span class="comment"># y_scores返回和样本数量相同的决策分数</span></span><br><span class="line"><span class="keyword">if</span> y_scores.ndim == <span class="number">2</span>:</span><br><span class="line">    y_scores = y_scores[:, <span class="number">1</span>]</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_roc_curve</span><span class="params">(fpr, tpr, label=None)</span>:</span></span><br><span class="line">    plt.plot(fpr, tpr, linewidth=<span class="number">2</span>, label=label)</span><br><span class="line">    plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'k--'</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>)</span><br><span class="line">plot_roc_curve(fpr, tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>这里同样存在折衷的问题：召回率（TPR）越高，分类器就会产生越多的假正例（FPR）。图中的点线是一个完全随机的分类器生成的 ROC 曲线；一个好的分类器的 ROC 曲线应该尽可能远离这条线（即向左上角方向靠拢）。</p><p>一个比较分类器之间优劣的方法是：测量ROC曲线下的面积（AUC）。一个完美的分类器的 ROC AUC 等于 1，而一个纯随机分类器的 ROC AUC 等于 0.5。Scikit-Learn 提供了一个函数来计算 ROC AUC：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>roc_auc_score(y_train_5, y_scores)</span><br><span class="line"><span class="number">0.97061072797174941</span></span><br></pre></td></tr></table></figure><p>因为 ROC 曲线跟准确率/召回率曲线（或者叫 PR）很类似，你或许会好奇如何决定使用哪一个曲线呢？一个笨拙的规则是，优先使用 PR 曲线当正例很少，或者当你关注假正例多于假反例的时候。其他情况使用 ROC 曲线。举例子，回顾前面的 ROC 曲线和 ROC AUC 数值，你或许认为这个分类器很棒。但是这几乎全是因为只有少数正例（“是 5”），而大部分是反例（“非 5”）。相反，PR 曲线清楚显示出这个分类器还有很大的改善空间（PR 曲线应该尽可能地靠近右上角）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Clas
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning metrics" scheme="http://yoursite.com/tags/Machine-Learning-metrics/"/>
    
  </entry>
  
  <entry>
    <title>sklearn中RandomForestClassifier</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-Sklearn_RandomForest-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-Sklearn_RandomForest-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:26:39.226Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="sklearn中RandomForestClassifier"><a href="#sklearn中RandomForestClassifier" class="headerlink" title="sklearn中RandomForestClassifier"></a>sklearn中RandomForestClassifier</h1><ol><li><p>在scikit-learn中，RandomForest的分类器是RandomForestClassifier,回归类RandomFoestRegressor,需要调参的参数包括两部分，第一部分是Bagging框架的参数，第二部分是CART决策树的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classsklearn.ensemble.RandomForestClassifier(n_estimators=<span class="number">10</span>, criterion=<span class="string">'gini'</span>, max_depth=<span class="literal">None</span>,min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>,max_features=<span class="string">'auto'</span>, max_leaf_nodes=<span class="literal">None</span>, min_impurity_split=<span class="number">1e-07</span>,bootstrap=<span class="literal">True</span>, oob_score=<span class="literal">True</span>, n_jobs=<span class="number">1</span>, random_state=<span class="literal">None</span>, verbose=<span class="number">0</span>,warm_start=<span class="literal">False</span>, class_weight=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></li><li><p>n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数，默认是10。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。对Random Forest来说，增加“子模型数”（n_estimators）可以明显降低整体模型的方差，且不会对子模型的偏差和方差有任何影响。模型的准确度会随着“子模型数”的增加而提高，由于减少的是整体模型方差公式的第二项，故准确度的提高有一个上限。在实际应用中，可以以10为单位，考察取值范围在1至201的调参情况。</p></li><li><p>对比，Random Forest的子模型都拥有较低的偏差，整体模型的训练过程旨在降低方差，故其需要较少的子模型（n_estimators默认值为10）且子模型不为弱模型（max_depth的默认值为None）；Gradient Tree Boosting的子模型都拥有较低的方差，整体模型的训练过程旨在降低偏差，故其需要较多的子模型（n_estimators默认值为100）且子模型为弱模型（max_depth的默认值为3）。</p></li><li><p>bootstrap：默认True，是否有放回的采样。</p></li><li><p>oob_score ：默认识False，即是否采用袋外样本来评估模型的好坏。有放回采样中大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)，这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。个人推荐设置为True，因为袋外分数反应了一个模型拟合后的泛化能力。对单个模型的参数训练，我们知道可以用cross validation（cv）来进行，但是特别消耗时间，而且对于随机森林这种情况也没有大的必要，所以就用这个数据对决策树模型进行验证，算是一个简单的交叉验证，性能消耗小，但是效果不错。</p><h2 id="criterion：-即CART树做划分时对特征的评价标准，分类模型和回归模型的损失函数是不一样的。分类RF对应的CART分类树默认是基尼系数gini-另一个可选择的标准是信息增益entropy，是用来选择节点的最优特征和切分点的两个准则。回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae。一般来说选择默认的标准就已经很好的。"><a href="#criterion：-即CART树做划分时对特征的评价标准，分类模型和回归模型的损失函数是不一样的。分类RF对应的CART分类树默认是基尼系数gini-另一个可选择的标准是信息增益entropy，是用来选择节点的最优特征和切分点的两个准则。回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae。一般来说选择默认的标准就已经很好的。" class="headerlink" title="criterion： 即CART树做划分时对特征的评价标准，分类模型和回归模型的损失函数是不一样的。分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益entropy，是用来选择节点的最优特征和切分点的两个准则。回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae。一般来说选择默认的标准就已经很好的。"></a>criterion： 即CART树做划分时对特征的评价标准，分类模型和回归模型的损失函数是不一样的。分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益entropy，是用来选择节点的最优特征和切分点的两个准则。回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae。一般来说选择默认的标准就已经很好的。</h2></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;skle
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="RandomForest" scheme="http://yoursite.com/tags/RandomForest/"/>
    
      <category term="scikit-learn" scheme="http://yoursite.com/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>损失函数（loss function）</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-cost_function-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-cost_function-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T05:58:37.791Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="损失函数（loss-function）"><a href="#损失函数（loss-function）" class="headerlink" title="损失函数（loss function）"></a>损失函数（loss function）</h1><p>损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是<strong>经验风险函数</strong>的核心部分，也是<strong>结构风险函数</strong>重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子：</p><script type="math/tex; mode=display">\theta^* = \arg \min_\theta \frac{1}{N}{}\sum_{i=1}^{N} L(y_i, f(x_i; \theta)) + \lambda\  \Phi(\theta)</script><p>其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的Φ是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是L1，也可以是L2，或者其他的正则函数。整个式子表示的意思是<strong>找到使目标函数最小时的θθ值</strong>。下面主要列出几种常见的损失函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;损失函数
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>Cross Validation 详解</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-cross_validation-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-cross_validation-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:05:07.978Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="Cross-Validation-详解"><a href="#Cross-Validation-详解" class="headerlink" title="Cross Validation 详解"></a>Cross Validation 详解</h1><p><img src="https://github.com/LelandYan/lelandyan.github.io/raw/master/img/cross_validation.png" alt="Image text"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line"><span class="comment"># cv参数为交叉时候分成几份（k-folds）</span></span><br><span class="line">scores = cross_val_score(knn_clf,x_train,y_train,cv=<span class="number">10</span>)</span><br><span class="line">scores = np.mean(scores)</span><br></pre></td></tr></table></figure><h2 id="交叉验证的目的就是找到最好的超参数所以传入的数据只是train的数据"><a href="#交叉验证的目的就是找到最好的超参数所以传入的数据只是train的数据" class="headerlink" title="交叉验证的目的就是找到最好的超参数所以传入的数据只是train的数据"></a>交叉验证的目的就是找到最好的超参数所以传入的数据只是train的数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="comment"># 事实上网格搜受已经使用了交叉验证了</span></span><br><span class="line">para,_grid = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'weights'</span>:[<span class="string">'uniform'</span>],</span><br><span class="line">        <span class="string">'n_neighbors'</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>)]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'weights'</span>:[<span class="string">'distance'</span>],</span><br><span class="line">        <span class="string">'n_neighbors'</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>)],</span><br><span class="line">        <span class="string">'p'</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>)]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"><span class="comment"># n_jobs表示的处理的计算的核数,verbose可以在运行的程序的时候，可以显示信息</span></span><br><span class="line">knn_clf = KneighborsClassifier()</span><br><span class="line">grid_search = GridSearchCV(knn_clf,param_grid,n_jobs=<span class="number">-1</span>，verbose=<span class="number">2</span>)</span><br><span class="line">grid_search.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># 获取训练集的最好结果</span></span><br><span class="line">grid_search.best_score_</span><br><span class="line"><span class="comment"># 获取训练集的最好超参数</span></span><br><span class="line">grid_search.best_params_</span><br><span class="line"><span class="comment"># 获得最好的分类器</span></span><br><span class="line">grid_search.best_estimator_</span><br></pre></td></tr></table></figure><h3 id="留一法-LOO-CV-虽然准确，但是计算量巨大"><a href="#留一法-LOO-CV-虽然准确，但是计算量巨大" class="headerlink" title="留一法(LOO-CV) 虽然准确，但是计算量巨大"></a>留一法(LOO-CV) 虽然准确，但是计算量巨大</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Cros
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="scikit-learn" scheme="http://yoursite.com/tags/scikit-learn/"/>
    
      <category term="cross validation" scheme="http://yoursite.com/tags/cross-validation/"/>
    
      <category term="validation" scheme="http://yoursite.com/tags/validation/"/>
    
  </entry>
  
  <entry>
    <title>Cross Validation</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-cross_valadation_sklearn-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-cross_valadation_sklearn-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T05:55:03.582Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h1><h2 id="有关交叉验证的sklearn的方法和原理"><a href="#有关交叉验证的sklearn的方法和原理" class="headerlink" title="有关交叉验证的sklearn的方法和原理"></a>有关交叉验证的sklearn的方法和原理</h2><h3 id="cross-val-score"><a href="#cross-val-score" class="headerlink" title="cross_val_score"></a>cross_val_score</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cross_val_score 用训练集来评估模型的好坏，注意交叉验证都是使用的是训练集来进行测验，而不是测试集</span></span><br><span class="line"><span class="comment"># 其返回的是，几折交叉验证就返回几个模型训练的评估分数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment"># 这里的model(),传入的是个模型实例，后传入的两个数据集合，cv表示交叉验证的次数，scoring表示评价方法</span></span><br><span class="line">cross_val_score(model(),X_train,y_train,cv=<span class="number">3</span>,scoring=<span class="string">"accuracy"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面则使用python实现一个cross_val_score函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklean.base <span class="keyword">import</span> clone</span><br><span class="line"><span class="comment"># 将数据集划分为几份</span></span><br><span class="line">skfolds = StratifiedKFold(n_splits=<span class="number">3</span>,random_state=<span class="number">42</span>)</span><br><span class="line"><span class="keyword">for</span> train_index,test_index <span class="keyword">in</span> skfolds.split(X_train,y_train):</span><br><span class="line">    <span class="comment"># 克隆原来模型</span></span><br><span class="line">    clone_clf = clone(model())</span><br><span class="line">    X_train_folds = X_train[trian_index]</span><br><span class="line">    y_train_folds = y_train[train_index]</span><br><span class="line">    X_test_folds = X_train[test_index]</span><br><span class="line">    y_test_folds= y_train[test_index]</span><br><span class="line">    clone_clf.fit(X_train_folds,y_train_folds)</span><br><span class="line">    y_pred = clone_clf.predict(X_test_folds)</span><br><span class="line">    n_correct = sum(y_pred == y_test_folds)</span><br><span class="line">    print(n_correct / len(y_pred) )</span><br></pre></td></tr></table></figure><h3 id="cross-val-predict"><a href="#cross-val-predict" class="headerlink" title="cross_val_predict"></a>cross_val_predict</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">就像 cross_val_score(),cross_val_predict()也使用 K 折交叉验证。它不是返回一个评估分数，而是返回基于每一个测试折做出的一个预测值。这意味着，对于每一个训练集的样例，你得到一个干净的预测（“干净”是说一个模型在训练过程当中没有用到测试集的数据）。"""</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict</span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Cros
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="scikit-learn" scheme="http://yoursite.com/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>Ensemble Learning</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-enseemble_learning-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-enseemble_learning-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T05:57:36.779Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="Ensemble-Learning"><a href="#Ensemble-Learning" class="headerlink" title="Ensemble Learning"></a>Ensemble Learning</h1><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h3 id="集成学习概述"><a href="#集成学习概述" class="headerlink" title="集成学习概述"></a>集成学习概述</h3><p>从下图，我们可以对集成学习的思想做一个概括。对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的</p><p>也就是说，集成学习有两个主要的问题需要解决，第一是如何得到若干个个体学习器，第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器</p><h3 id="集成学习之个体学习器"><a href="#集成学习之个体学习器" class="headerlink" title="集成学习之个体学习器"></a>集成学习之个体学习器</h3><p>上一节我们讲到，集成学习的第一个问题就是如何得到若干个个体学习器。这里我们有两种选择。</p><p>　　　　第一种就是所有的个体学习器都是一个种类的，或者说是同质的。比如都是决策树个体学习器，或者都是神经网络个体学习器。第二种是所有的个体学习器不全是一个种类的，或者说是异质的。比如我们有一个分类问题，对训练集采用支持向量机个体学习器，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。</p><p>　　　　目前来说，同质个体学习器的应用是最广泛的，一般我们常说的集成学习的方法都是指的同质个体学习器。而同质个体学习器使用最多的模型是CART决策树和神经网络。同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类，第一个是个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是boosting系列算法，第二个是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。下面就分别对这两类算法做一个概括总结。</p><h3 id="boostsing集成学习"><a href="#boostsing集成学习" class="headerlink" title="boostsing集成学习"></a>boostsing集成学习</h3><p>　从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p><p>Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)</p><p>这里对Adaboost算法的优缺点做一个总结。</p><p>Adaboost的主要优点有：</p><p>1）Adaboost作为分类器时，分类精度很高</p><p>2）在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。</p><p>3）作为简单的二元分类器时，构造简单，结果可理解。</p><p>4）不容易发生过拟合</p><p>Adaboost的主要缺点有：</p><p>1）对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</p><p>由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。</p><p>最后总结下GBDT的优缺点。</p><p>GBDT主要的优点有：</p><p>1) 可以灵活处理各种类型的数据，包括连续值和离散值。</p><p>2) 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。</p><p>3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</p><p>GBDT的主要缺点有：</p><p>1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</p><h3 id="集成学习之bagging"><a href="#集成学习之bagging" class="headerlink" title="集成学习之bagging"></a>集成学习之bagging</h3><p>Bagging的算法原理和 boosting不同，它的弱学习器之间没有依赖关系，可以并行生成，我们可以用一张图做一个概括如下：从上图可以看出，bagging的个体弱学习器的训练集是通过随机采样得到的。通过T次的随机采样，我们就可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。</p><p>　　　　对于这里的随机采样有必要做进一步的介绍，这里一般采用的是自助采样法（Bootstrap sampling）,即对于m个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。</p><p>　　　　随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。bagging和随机森林算法的原理在后面的文章中会专门来讲。</p><h3 id="集成学习之结合策略"><a href="#集成学习之结合策略" class="headerlink" title="集成学习之结合策略"></a>集成学习之结合策略</h3><ol><li><p>平均法 </p></li><li><p>投票法 （投票法也可以设置权重）</p></li><li><p>学习法 （代表的方法是stacking）当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。</p><p>　　　　在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Ense
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Ensemble Learning" scheme="http://yoursite.com/tags/Ensemble-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Procedure</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-machine_learning_procedure-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-machine_learning_procedure-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T05:59:29.917Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="Machine-Learning-Procedure"><a href="#Machine-Learning-Procedure" class="headerlink" title="Machine Learning Procedure"></a>Machine Learning Procedure</h1><h2 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h2 id="数据预览"><a href="#数据预览" class="headerlink" title="数据预览"></a>数据预览</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = pd.DataFrame(X)</span><br><span class="line"><span class="comment"># 取前10行数据</span></span><br><span class="line">X.head(n=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 取数据中任意的10行</span></span><br><span class="line">X.sample(n=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 查看数据的大体数据分布以及大小</span></span><br><span class="line">X.describe()</span><br></pre></td></tr></table></figure><h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><p>通过箱线图可以发现异常值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X.plot(kind=<span class="string">'box'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>通过条形图可以看出数据结构的分布特征，是否满足正太分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X.hist(figsize=(<span class="number">12</span>,<span class="number">5</span>),xlabelsize=<span class="number">1</span>,ylabelsize=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>通过折线图可以看出数据值的大小密度的分布情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X.plot(kind=<span class="string">"density"</span>,subplot=<span class="literal">True</span>,layout=(<span class="number">4</span>,<span class="number">4</span>),figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>通过特征相关图我们能够知道哪些特征存在明显的相关性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pd.scatter_matrix(X,figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>通过热力图可以更加清晰的看出各个特征之间的关系</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">cax = ax.matshow(X.corr(),vmin=<span class="number">-1</span>,vmax=<span class="number">1</span>,interploation=<span class="string">"none"</span>)</span><br><span class="line">fig.colorbar(cax)</span><br><span class="line"><span class="comment"># 这里为数据特征关联的分布大小</span></span><br><span class="line">ticks = np.arange(<span class="number">0</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">ax.set_xticks(ticks)</span><br><span class="line">ax.set_yticks(ticks)</span><br><span class="line"><span class="comment"># 这里的col_name为特征的名称</span></span><br><span class="line">ax.set_xticklabels(col_name)</span><br><span class="line">ax.set_yticklabels(col_name)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="查找最优模型"><a href="#查找最优模型" class="headerlink" title="查找最优模型"></a>查找最优模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, cross_val_score,GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">models = []</span><br><span class="line">models.append((<span class="string">"AB"</span>, AdaBoostClassifier()))</span><br><span class="line">models.append((<span class="string">"GBM"</span>, GradientBoostingClassifier()))</span><br><span class="line">models.append((<span class="string">"RF"</span>, RandomForestClassifier()))</span><br><span class="line">models.append((<span class="string">"ET"</span>, ExtraTreesClassifier()))</span><br><span class="line">models.append((<span class="string">"SVC"</span>, SVC()))</span><br><span class="line">models.append((<span class="string">"KNN"</span>, KNeighborsClassifier()))</span><br><span class="line">models.append((<span class="string">"LR"</span>, LogisticRegression()))</span><br><span class="line">models.append((<span class="string">"GNB"</span>, GaussianNB()))</span><br><span class="line">models.append((<span class="string">"LDA"</span>, LinearDiscriminantAnalysis()))</span><br><span class="line"></span><br><span class="line">names = []</span><br><span class="line">results = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, model <span class="keyword">in</span> models:</span><br><span class="line">    kfold = KFold(n_splits=<span class="number">5</span>, random_state=<span class="number">42</span>)</span><br><span class="line">    result = cross_val_score(model, X, y, scoring=<span class="string">'accuracy'</span>, cv=kfold)</span><br><span class="line">    names.append(name)</span><br><span class="line">    results.append(result)</span><br><span class="line">    print(<span class="string">"&#123;&#125; Mean:&#123;:.4f&#125;(Std:&#123;:.4f&#125;)"</span>.format(name, result.mean(), result.std()))</span><br></pre></td></tr></table></figure><h2 id="使用Pipeline"><a href="#使用Pipeline" class="headerlink" title="使用Pipeline"></a>使用Pipeline</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">pipeline = []</span><br><span class="line">pipeline.append((<span class="string">"ScalerET"</span>, Pipeline([(<span class="string">"Scaler"</span>,StandardScaler()),</span><br><span class="line"> (<span class="string">"ET"</span>,ExtraTreesClassifier())])))</span><br><span class="line">pipeline.append((<span class="string">"ScalerGBM"</span>, Pipeline([(<span class="string">"Scaler"</span>,StandardScaler()),</span><br><span class="line">   (<span class="string">"GBM"</span>,GradientBoostingClassifier())])))</span><br><span class="line">pipeline.append((<span class="string">"ScalerRF"</span>, Pipeline([(<span class="string">"Scaler"</span>,StandardScaler()),</span><br><span class="line"> (<span class="string">"RF"</span>,RandomForestClassifier())])))</span><br><span class="line"></span><br><span class="line">names = []</span><br><span class="line">results = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, model <span class="keyword">in</span> pipeline:</span><br><span class="line">    kfold = KFold(n_splits=<span class="number">5</span>, random_state=<span class="number">42</span>)</span><br><span class="line">    result = cross_val_score(model, X, y, scoring=<span class="string">'accuracy'</span>, cv=kfold)</span><br><span class="line">    names.append(name)</span><br><span class="line">    results.append(result)</span><br><span class="line">    print(<span class="string">"&#123;&#125; Mean:&#123;:.4f&#125;(Std:&#123;:.4f&#125;)"</span>.format(name, result.mean(), result.std()))</span><br></pre></td></tr></table></figure><h2 id="模型调节"><a href="#模型调节" class="headerlink" title="模型调节"></a>模型调节</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">"C"</span>:[<span class="number">0.1</span>,<span class="number">0.3</span>,<span class="number">0.5</span>,<span class="number">0.7</span>,<span class="number">0.9</span>,<span class="number">1.0</span>,<span class="number">1.3</span>,<span class="number">1.5</span>,<span class="number">1.7</span>,<span class="number">2.0</span>],</span><br><span class="line">    <span class="string">"kernel"</span>:[<span class="string">"linear"</span>,<span class="string">"poly"</span>,<span class="string">"rbf"</span>,<span class="string">"sigmoid"</span>]</span><br><span class="line">&#125;</span><br><span class="line">model = SVC()</span><br><span class="line">kfold = KFold(n_splits=<span class="number">5</span>,random_state=<span class="number">42</span>)</span><br><span class="line">grid = GridSearchCV(estimator=model,param_grid=param_grid,scoring=<span class="string">"accuracy"</span>,cv=kfold)</span><br><span class="line">grid_result = grid.fit(X,y)</span><br><span class="line">print(<span class="string">"Best: &#123;&#125; using &#123;&#125;"</span>.format(grid_result.best_score_,grid_result.best_params_))</span><br><span class="line">means = grid_result.cv_results_[<span class="string">"mean_test_score"</span>]</span><br><span class="line">stds = grid_result.cv_results_[<span class="string">"std_test_score"</span>]</span><br><span class="line">params = grid_result.cv_results_[<span class="string">"params"</span>]</span><br><span class="line"><span class="keyword">for</span> mean,stdev,param <span class="keyword">in</span> zip(means,stds,params):</span><br><span class="line">    print(<span class="string">"&#123;&#125; (&#123;&#125;) with &#123;&#125;"</span>.format(mean,stdev,param))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 使用随机梯度下降法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line">param_distribs = &#123;</span><br><span class="line">        <span class="string">'n_estimators'</span>: randint(low=<span class="number">1</span>, high=<span class="number">200</span>),</span><br><span class="line">        <span class="string">'max_features'</span>: randint(low=<span class="number">1</span>, high=<span class="number">8</span>),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">forest_reg = RandomForestRegressor(random_state=<span class="number">42</span>)</span><br><span class="line">rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,</span><br><span class="line">                                n_iter=<span class="number">10</span>, cv=<span class="number">5</span>, scoring=<span class="string">'neg_mean_squared_error'</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rnd_search.fit(housing_prepared, housing_labels)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># svm 调参</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> expon, reciprocal</span><br><span class="line"></span><br><span class="line"><span class="comment"># see https://docs.scipy.org/doc/scipy/reference/stats.html</span></span><br><span class="line"><span class="comment"># for `expon()` and `reciprocal()` documentation and more probability distribution functions.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: gamma is ignored when kernel is "linear"</span></span><br><span class="line">param_distribs = &#123;</span><br><span class="line">        <span class="string">'kernel'</span>: [<span class="string">'linear'</span>, <span class="string">'rbf'</span>],</span><br><span class="line">        <span class="string">'C'</span>: reciprocal(<span class="number">20</span>, <span class="number">200000</span>),</span><br><span class="line">        <span class="string">'gamma'</span>: expon(scale=<span class="number">1.0</span>),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">svm_reg = SVR()</span><br><span class="line">rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,</span><br><span class="line">                                n_iter=<span class="number">50</span>, cv=<span class="number">5</span>, scoring=<span class="string">'neg_mean_squared_error'</span>,</span><br><span class="line">                                verbose=<span class="number">2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rnd_search.fit(housing_prepared, housing_labels)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Mach
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
      <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>matplotlib防止中文乱码</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-matplotlib_learning-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-matplotlib_learning-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:23:30.910Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="matplotlib防止中文乱码"><a href="#matplotlib防止中文乱码" class="headerlink" title="matplotlib防止中文乱码"></a>matplotlib防止中文乱码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘图时候防止在图像的中文乱码</span></span><br><span class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'simHei'</span>]</span><br><span class="line">mpl.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;matp
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>如何处理多分类问题</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-mult_classifition-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-mult_classifition-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:02:29.794Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="如何处理多分类问题"><a href="#如何处理多分类问题" class="headerlink" title="如何处理多分类问题"></a>如何处理多分类问题</h1><h2 id="有关多分类问题有两种方法"><a href="#有关多分类问题有两种方法" class="headerlink" title="有关多分类问题有两种方法"></a>有关多分类问题有两种方法</h2><p>二分类器只能区分两个类，而多类分类器（也被叫做多项式分类器）可以区分多于两个类。</p><p>一些算法（比如随机森林分类器或者朴素贝叶斯分类器）可以直接处理多类分类问题。其他一些算法（比如 SVM 分类器或者线性分类器）则是严格的二分类器。然后，有许多策略可以让你用二分类器去执行多类分类。</p><p>举例子，创建一个可以将图片分成 10 类（从 0 到 9）的系统的一个方法是：训练10个二分类器，每一个对应一个数字（探测器 0，探测器 1，探测器 2，以此类推）。然后当你想对某张图片进行分类的时候，让每一个分类器对这个图片进行分类，选出决策分数最高的那个分类器。这叫做“一对所有”（OvA）策略（也被叫做“一对其他”）。</p><p>另一个策略是对每一对数字都训练一个二分类器：一个分类器用来处理数字 0 和数字 1，一个用来处理数字 0 和数字 2，一个用来处理数字 1 和 2，以此类推。这叫做“一对一”（OvO）策略。如果有 N 个类。你需要训练<code>N*(N-1)/2</code>个分类器。对于 MNIST 问题，需要训练 45 个二分类器！当你想对一张图片进行分类，你必须将这张图片跑在全部45个二分类器上。然后看哪个类胜出。OvO 策略的主要优点是：每个分类器只需要在训练集的部分数据上面进行训练。这部分数据是它所需要区分的那两个类对应的数据。</p><p>一些算法（比如 SVM 分类器）在训练集的大小上很难扩展，所以对于这些算法，OvO 是比较好的，因为它可以在小的数据集上面可以更多地训练，较之于巨大的数据集而言。但是，对于大部分的二分类器来说，OvA 是更好的选择。</p><p>Scikit-Learn 可以探测出你想使用一个二分类器去完成多分类的任务，它会自动地执行 OvA（除了 SVM 分类器，它使用 OvO）。让我们试一下<code>SGDClassifier</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.fit(X_train, y_train) <span class="comment"># y_train, not y_train_5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.predict([some_digit])</span><br><span class="line">array([ <span class="number">5.</span>])</span><br></pre></td></tr></table></figure><p>很容易。上面的代码在训练集上训练了一个<code>SGDClassifier</code>。这个分类器处理原始的目标class，从 0 到 9（<code>y_train</code>），而不是仅仅探测是否为 5 （<code>y_train_5</code>）。然后它做出一个判断（在这个案例下只有一个正确的数字）。在幕后，Scikit-Learn 实际上训练了 10 个二分类器，每个分类器都产到一张图片的决策数值，选择数值最高的那个类。</p><p>为了证明这是真实的，你可以调用<code>decision_function()</code>方法。不是返回每个样例的一个数值，而是返回 10 个数值，一个数值对应于一个类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_digit_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_digit_scores</span><br><span class="line">array([[<span class="number">-311402.62954431</span>, <span class="number">-363517.28355739</span>, <span class="number">-446449.5306454</span> ,</span><br><span class="line">        <span class="number">-183226.61023518</span>, <span class="number">-414337.15339485</span>, <span class="number">161855.74572176</span>,</span><br><span class="line">        <span class="number">-452576.39616343</span>, <span class="number">-471957.14962573</span>, <span class="number">-518542.33997148</span>,</span><br><span class="line">        <span class="number">-536774.63961222</span>]])</span><br></pre></td></tr></table></figure><p>最高数值是对应于类别 5 ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.argmax(some_digit_scores)</span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.classes_</span><br><span class="line">array([ <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.classes_[<span class="number">5</span>]</span><br><span class="line"><span class="number">5.0</span></span><br></pre></td></tr></table></figure><blockquote><p>一个分类器被训练好了之后，它会保存目标类别列表到它的属性<code>classes_</code> 中去，按照值排序。在本例子当中，在<code>classes_</code> 数组当中的每个类的索引方便地匹配了类本身，比如，索引为 5 的类恰好是类别 5 本身。但通常不会这么幸运。</p></blockquote><p>如果你想强制 Scikit-Learn 使用 OvO 策略或者 OvA 策略，你可以使用<code>OneVsOneClassifier</code>类或者<code>OneVsRestClassifier</code>类。创建一个样例，传递一个二分类器给它的构造函数。举例子，下面的代码会创建一个多类分类器，使用 OvO 策略，基于<code>SGDClassifier</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsOneClassifier</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=<span class="number">42</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ovo_clf.fit(X_train, y_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ovo_clf.predict([some_digit])</span><br><span class="line">array([ <span class="number">5.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(ovo_clf.estimators_)</span><br><span class="line"><span class="number">45</span></span><br></pre></td></tr></table></figure><p>训练一个<code>RandomForestClassifier</code>同样简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_clf.fit(X_train, y_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_clf.predict([some_digit])</span><br><span class="line">array([ <span class="number">5.</span>])</span><br></pre></td></tr></table></figure><p>这次 Scikit-Learn 没有必要去运行 OvO 或者 OvA，因为随机森林分类器能够直接将一个样例分到多个类别。你可以调用<code>predict_proba()</code>，得到样例对应的类别的概率值的列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_clf.predict_proba([some_digit])</span><br><span class="line">array([[ <span class="number">0.1</span>, <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.1</span>, <span class="number">0.</span> , <span class="number">0.8</span>, <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> ]])</span><br></pre></td></tr></table></figure><p>你可以看到这个分类器相当确信它的预测：在数组的索引 5 上的 0.8，意味着这个模型以 80% 的概率估算这张图片代表数字 5。它也认为这个图片可能是数字 0 或者数字 3，分别都是 10% 的几率。</p><p>现在当然你想评估这些分类器。像平常一样，你想使用交叉验证。让我们用<code>cross_val_score()</code>来评估<code>SGDClassifier</code>的精度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(sgd_clf, X_train, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line">array([ <span class="number">0.84063187</span>, <span class="number">0.84899245</span>, <span class="number">0.86652998</span>])</span><br></pre></td></tr></table></figure><p>在所有测试折（test fold）上，它有 84% 的精度。如果你是用一个随机的分类器，你将会得到 10% 的正确率。所以这不是一个坏的分数，但是你可以做的更好。举例子，简单将输入正则化，将会提高精度到 90% 以上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler = StandardScaler()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line">array([ <span class="number">0.91011798</span>, <span class="number">0.90874544</span>, <span class="number">0.906636</span> ])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;如何处理
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="scikit-learn" scheme="http://yoursite.com/tags/scikit-learn/"/>
    
      <category term="multi-classifition" scheme="http://yoursite.com/tags/multi-classifition/"/>
    
  </entry>
  
  <entry>
    <title>Addressing overfitting</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-regulation_overfitting-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-regulation_overfitting-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:29:11.773Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="Addressing-overfitting"><a href="#Addressing-overfitting" class="headerlink" title="Addressing overfitting"></a>Addressing overfitting</h1><h3 id="options"><a href="#options" class="headerlink" title="options:"></a>options:</h3><p>​    1 .Reduce number of features</p><p>​        Manually select which features to keep</p><p>​        Model selection algorithm()</p><p>​    2.Regularization</p><p>​        Keep all the features,but reduce magnitude/value parameters</p><p>​        Works well when we have a lot of    features,each of which contributes a bit ot predicting        </p><p>​    small values for parameters</p><p>​    3. Regularized linear regression<br>   <img src="/images/regulation.png" alt=""></p><ol><li>Gradient descent<br><img src="/images/regularization_gradient.png" alt="Image text"></li></ol><ol><li><p>normalization equation</p><ol><li><p>奇异矩阵的判断方法：</p><p>首先，看这个矩阵是不是方阵（即行数和列数相等的矩阵。若行数和列数不相等，那就谈不上奇异矩阵和非奇异矩阵）。 然后，再看此方阵的行列式| A |是否等于0，若等于0，称矩阵A为奇异矩阵；若不等于0，称矩阵A为非奇异矩阵。 同时，由| A |≠0可知矩阵A可逆，这样可以得出另外一个重要结论:可逆矩阵就是非奇异矩阵，非奇异矩阵也是可逆矩阵。</p></li></ol><p><img src="/images/normalization_regualization.png" alt="Image text"></p></li></ol><p>   <strong>这里一定是可逆的的矩阵，虽然X^TX可能不可逆，但是加入惩罚因子后，其和的行列式不为0，所以其可逆</strong></p><ol><li><p>regularization logistic regression</p><p>logistic regression 和 linear regression 区别在与costFunction和是否使用sigimoid function</p><script type="math/tex; mode=display">J(\theta) = -\frac{ 1 }{ m }[\sum_{ i=1 }^{ m } ({y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log (1-h_\theta(x^{(i)})})]+\frac{k}{2m}\sum_{ i=1 }^{ n }\theta_j^2</script><p><img src="/images/regulariation_logistic_1545615172826.png" alt="Image text"></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Addr
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="logistic regression" scheme="http://yoursite.com/tags/logistic-regression/"/>
    
      <category term="overfitting" scheme="http://yoursite.com/tags/overfitting/"/>
    
      <category term="normalization equation" scheme="http://yoursite.com/tags/normalization-equation/"/>
    
      <category term="regularization" scheme="http://yoursite.com/tags/regularization/"/>
    
  </entry>
  
  <entry>
    <title>矩阵求导</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-matrix_derivative-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-matrix_derivative-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:30:43.645Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h1><ol><li>向量关于标量X的求导<script type="math/tex; mode=display">\left[ \begin{matrix}   y_1 \\   y_2 \\   \vdots \\   y_n  \end{matrix}  \right]</script></li></ol><p>   经过求导后</p><script type="math/tex; mode=display">   \left[    \begin{matrix}     \frac{\partial y_1}{\partial x} \\      \frac{\partial y_2}{\partial x} \\      \vdots \\      \frac{\partial y_n}{\partial x}     \end{matrix}     \right]</script><ol><li><p>矩阵对标量X的求导-和对向量的求导使同理的</p></li><li><p>矩阵对向量X求导</p></li></ol><script type="math/tex; mode=display">   A=\left[   \begin{matrix}    a11      & a12      & \cdots & a1n      \\    a21      & a22      & \cdots & a2n      \\    \vdots & \vdots & \ddots & \vdots \\    am1      & am2      & \cdots & am      \\   \end{matrix}   \right]</script><script type="math/tex; mode=display">   \vec{X}=\begin{Bmatrix}     x_1 \\      x_2 \\      \vdots \\      x_n     \end{Bmatrix} \tag{5}</script><script type="math/tex; mode=display">   \vec{Y} = \vec{A}\cdot\vec{X}=\left[   \begin{matrix}    a11\cdot x1 + a12\cdot\ x2 \cdot + a1n \cdot xn \\    a21\cdot x1 + a22\cdot\ x2 \cdot + a2n \cdot xn \\    \vdots &  \\    am1\cdot x1 + am2\cdot\ x2 \cdot + amn \cdot xn \\   \end{matrix}   \right]</script><script type="math/tex; mode=display">\frac{\partial \vec{Y}}{\partial \vec{X}}=\left[   \begin{matrix}    a11      & a21      & \cdots & am1      \\    a12      & a22      & \cdots & am2      \\    \vdots & \vdots & \ddots & \vdots \\    a1n      & a2n      & \cdots & amn      \\   \end{matrix}   \right]=A^T</script><p><img src="/images/matrix.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;矩阵求导
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="矩阵求导" scheme="http://yoursite.com/tags/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"/>
    
  </entry>
  
  <entry>
    <title>如何处理不平衡的数据</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-sciPy-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-sciPy-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T05:53:40.715Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="2-5-SciPy中稀疏矩阵"><a href="#2-5-SciPy中稀疏矩阵" class="headerlink" title="2.5 SciPy中稀疏矩阵"></a>2.5 SciPy中稀疏矩阵</h1><p>In [3]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import numpy as np</span><br></pre></td></tr></table></figure><h2 id="2-5-1-介绍"><a href="#2-5-1-介绍" class="headerlink" title="2.5.1 介绍"></a>2.5.1 介绍</h2><p>(密集) 矩阵是:</p><ul><li>数据对象</li><li>存储二维值数组的数据结构</li></ul><p>重要特征:</p><ul><li>一次分配所有项目的内存<ul><li>通常是一个连续组块，想一想Numpy数组</li></ul></li><li><em>快速</em>访问个项目(*)</li></ul><h3 id="2-5-1-1-为什么有稀疏矩阵？"><a href="#2-5-1-1-为什么有稀疏矩阵？" class="headerlink" title="2.5.1.1 为什么有稀疏矩阵？"></a>2.5.1.1 为什么有稀疏矩阵？</h3><ul><li>内存，增长是n**2</li><li>小例子（双精度矩阵）:</li></ul><p>In [5]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">x &#x3D; np.linspace(0, 1e6, 10)</span><br><span class="line">plt.plot(x, 8.0 * (x**2) &#x2F; 1e6, lw&#x3D;5)   </span><br><span class="line">plt.xlabel(&#39;size n&#39;)</span><br><span class="line">plt.ylabel(&#39;memory [MB]&#39;)</span><br></pre></td></tr></table></figure><p>Out[5]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;matplotlib.text.Text at 0x105b08dd0&gt;</span><br></pre></td></tr></table></figure><p><img src="https://wizardforcel.gitbooks.io/scipy-lecture-notes/content/img/C3AAAAAElFTkSuQmCC.png" alt="img"></p><h3 id="2-5-1-2-稀疏矩阵-vs-稀疏矩阵存储方案"><a href="#2-5-1-2-稀疏矩阵-vs-稀疏矩阵存储方案" class="headerlink" title="2.5.1.2 稀疏矩阵 vs. 稀疏矩阵存储方案"></a>2.5.1.2 稀疏矩阵 vs. 稀疏矩阵存储方案</h3><ul><li>稀疏矩阵是一个矩阵，巨大多数是空的</li><li>存储所有的0是浪费 -&gt; 只存储非0项目</li><li>想一下<strong>压缩</strong></li><li>有利: 巨大的内存节省</li><li>不利: 依赖实际的存储方案, (*) 通常并不能满足</li></ul><h3 id="2-5-1-3-典型应用"><a href="#2-5-1-3-典型应用" class="headerlink" title="2.5.1.3 典型应用"></a>2.5.1.3 典型应用</h3><ul><li>偏微分方程（PDES）的解<ul><li>有限元素法</li><li>机械工程、电子、物理…</li></ul></li><li>图论<ul><li>（i，j）不是0表示节点i与节点j是联接的</li></ul></li><li>…</li></ul><h3 id="2-5-1-4-先决条件"><a href="#2-5-1-4-先决条件" class="headerlink" title="2.5.1.4 先决条件"></a>2.5.1.4 先决条件</h3><p>最新版本的</p><ul><li><code>numpy</code></li><li><code>scipy</code></li><li><code>matplotlib</code> (可选)</li><li><code>ipython</code> (那些增强很方便)</li></ul><h3 id="2-5-1-5-稀疏结构可视化"><a href="#2-5-1-5-稀疏结构可视化" class="headerlink" title="2.5.1.5 稀疏结构可视化"></a>2.5.1.5 稀疏结构可视化</h3><ul><li>matplotlib中的<code>spy()</code></li><li>样例绘图:</li></ul><p><img src="http://scipy-lectures.github.io/_images/graph.png" alt="img"> <img src="http://scipy-lectures.github.io/_images/graph_g.png" alt="img"> <img src="http://scipy-lectures.github.io/_images/graph_rcm.png" alt="img"></p><h2 id="2-5-2-存储机制"><a href="#2-5-2-存储机制" class="headerlink" title="2.5.2 存储机制"></a>2.5.2 存储机制</h2><ul><li>scipy.sparse中有七类稀疏矩阵:<ol><li>csc_matrix: 压缩列格式</li><li>csr_matrix: 压缩行格式</li><li>bsr_matrix: 块压缩行格式</li><li>lil_matrix: 列表的列表格式</li><li>dok_matrix: 值的字典格式</li><li>coo_matrix: 座标格式 (即 IJV, 三维格式)</li><li>dia_matrix: 对角线格式</li></ol></li><li>每一个类型适用于一些任务</li><li>许多都利用了由Nathan Bell提供的稀疏工具 C ++ 模块</li><li>假设导入了下列模块:</li></ul><p>In [1]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import scipy.sparse as sparse</span><br><span class="line">import matplotlib.pyplot as plt</span><br></pre></td></tr></table></figure><ul><li><p>给Numpy用户的</p><p>warning</p><p>:</p><ul><li>使用’<em>‘的乘是</em>矩阵相乘* (点积)</li><li>并不是Numpy的一部分!<ul><li>向Numpy函数传递一个稀疏矩阵希望一个ndarray/矩阵是没用的</li></ul></li></ul></li></ul><h3 id="2-5-2-1-通用方法"><a href="#2-5-2-1-通用方法" class="headerlink" title="2.5.2.1 通用方法"></a>2.5.2.1 通用方法</h3><ul><li>所有scipy.sparse类都是spmatrix的子类<ul><li>算术操作的默认实现<ul><li>通常转化为CSR</li><li>为了效率而子类覆盖</li></ul></li><li>形状、数据类型设置/获取</li><li>非0索引</li><li>格式转化、与Numpy交互(toarray(), todense())</li><li>…</li></ul></li><li>属性:<ul><li>mtx.A - 与mtx.toarray()相同</li><li>mtx.T - 转置 (与mtx.transpose()相同)</li><li>mtx.H - Hermitian (列举) 转置</li><li>mtx.real - 复矩阵的真部</li><li>mtx.imag - 复矩阵的虚部</li><li>mtx.size - 非零数 (与self.getnnz()相同)</li><li>mtx.shape - 行数和列数 (元组)</li></ul></li><li>数据通常储存在Numpy数组中</li></ul><h3 id="2-5-2-2-稀疏矩阵类"><a href="#2-5-2-2-稀疏矩阵类" class="headerlink" title="2.5.2.2 稀疏矩阵类"></a>2.5.2.2 稀疏矩阵类</h3><h4 id="2-5-2-2-1-对角线格式-DIA"><a href="#2-5-2-2-1-对角线格式-DIA" class="headerlink" title="2.5.2.2.1 对角线格式 (DIA))"></a>2.5.2.2.1 对角线格式 (DIA))</h4><ul><li>非常简单的格式</li><li>形状 (n_diag, length) 的密集Numpy数组的对角线<ul><li>固定长度 -&gt; 当离主对角线比较远时会浪费空间</li><li>_data_matrix的子类 (带数据属性的稀疏矩阵类)</li></ul></li><li>每个对角线的偏移<ul><li>0 是主对角线</li><li>负偏移 = 下面</li><li>正偏移 = 上面</li></ul></li><li>快速矩阵 * 向量 (sparsetools)</li><li>快速方便的关于项目的操作<ul><li>直接操作数据数组 (快速的NumPy机件)</li></ul></li><li>构建器接受 :<ul><li>密集矩阵 (数组)</li><li>稀疏矩阵</li><li>形状元组 (创建空矩阵)</li><li>(数据, 偏移) 元组</li></ul></li><li>没有切片、没有单个项目访问</li><li>用法 :<ul><li>非常专业</li><li>通过有限微分解偏微分方程</li><li>有一个迭代求解器 ##### 2.5.2.2.1.1 示例</li></ul></li><li>创建一些DIA矩阵 :</li></ul><p>In [3]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; np.array([[1, 2, 3, 4]]).repeat(3, axis&#x3D;0)</span><br><span class="line">data</span><br></pre></td></tr></table></figure><p>Out[3]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[1, 2, 3, 4],</span><br><span class="line">       [1, 2, 3, 4],</span><br><span class="line">       [1, 2, 3, 4]])</span><br></pre></td></tr></table></figure><p>In [6]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">offsets &#x3D; np.array([0, -1, 2])</span><br><span class="line">mtx &#x3D; sparse.dia_matrix((data, offsets), shape&#x3D;(4, 4))</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure><p>Out[6]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;4x4 sparse matrix of type &#39;&lt;type &#39;numpy.int64&#39;&gt;&#39;</span><br><span class="line">    with 9 stored elements (3 diagonals) in DIAgonal format&gt;</span><br></pre></td></tr></table></figure><p>In [7]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure><p>Out[7]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">matrix([[1, 0, 3, 0],</span><br><span class="line">        [1, 2, 0, 4],</span><br><span class="line">        [0, 2, 3, 0],</span><br><span class="line">        [0, 0, 3, 4]])</span><br></pre></td></tr></table></figure><p>In [9]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; np.arange(12).reshape((3, 4)) + 1</span><br><span class="line">data</span><br></pre></td></tr></table></figure><p>Out[9]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ 1,  2,  3,  4],</span><br><span class="line">       [ 5,  6,  7,  8],</span><br><span class="line">       [ 9, 10, 11, 12]])</span><br></pre></td></tr></table></figure><p>In [10]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mtx &#x3D; sparse.dia_matrix((data, offsets), shape&#x3D;(4, 4))</span><br><span class="line">mtx.data</span><br></pre></td></tr></table></figure><p>Out[10]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ 1,  2,  3,  4],</span><br><span class="line">       [ 5,  6,  7,  8],</span><br><span class="line">       [ 9, 10, 11, 12]])</span><br></pre></td></tr></table></figure><p>In [11]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.offsets</span><br></pre></td></tr></table></figure><p>Out[11]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ 0, -1,  2], dtype&#x3D;int32)</span><br></pre></td></tr></table></figure><p>In [12]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print mtx</span><br><span class="line">  (0, 0)    1</span><br><span class="line">  (1, 1)    2</span><br><span class="line">  (2, 2)    3</span><br><span class="line">  (3, 3)    4</span><br><span class="line">  (1, 0)    5</span><br><span class="line">  (2, 1)    6</span><br><span class="line">  (3, 2)    7</span><br><span class="line">  (0, 2)    11</span><br><span class="line">  (1, 3)    12</span><br></pre></td></tr></table></figure><p>In [13]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure><p>Out[13]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">matrix([[ 1,  0, 11,  0],</span><br><span class="line">        [ 5,  2,  0, 12],</span><br><span class="line">        [ 0,  6,  3,  0],</span><br><span class="line">        [ 0,  0,  7,  4]])</span><br></pre></td></tr></table></figure><ul><li>机制的解释 :</li></ul><p>偏移: 行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> 2:  9</span><br><span class="line"> 1:  --10------</span><br><span class="line"> 0:  1  . 11  .</span><br><span class="line">-1:  5  2  . 12</span><br><span class="line">-2:  .  6  3  .</span><br><span class="line">-3:  .  .  7  4</span><br><span class="line">     ---------8</span><br></pre></td></tr></table></figure><ul><li>矩阵-向量相乘</li></ul><p>In [15]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vec &#x3D; np.ones((4, ))</span><br><span class="line">vec</span><br></pre></td></tr></table></figure><p>Out[15]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ 1.,  1.,  1.,  1.])</span><br></pre></td></tr></table></figure><p>In [16]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx * vec</span><br></pre></td></tr></table></figure><p>Out[16]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ 12.,  19.,   9.,  11.])</span><br></pre></td></tr></table></figure><p>In [17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.toarray() * vec</span><br></pre></td></tr></table></figure><p>Out[17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">array([[  1.,   0.,  11.,   0.],</span><br><span class="line">       [  5.,   2.,   0.,  12.],</span><br><span class="line">       [  0.,   6.,   3.,   0.],</span><br><span class="line">       [  0.,   0.,   7.,   4.]])</span><br></pre></td></tr></table></figure><h4 id="2-5-2-2-2-列表的列表格式-LIL"><a href="#2-5-2-2-2-列表的列表格式-LIL" class="headerlink" title="2.5.2.2.2 列表的列表格式 (LIL))"></a>2.5.2.2.2 列表的列表格式 (LIL))</h4><ul><li>基于行的联接列表<ul><li>每一行是一个Python列表（排序的）非零元素的列索引</li><li>行存储在Numpy数组中 (dtype=np.object)</li><li>非零值也近似存储</li></ul></li><li>高效增量构建稀疏矩阵</li><li>构建器接受 :<ul><li>密集矩阵 (数组)</li><li>稀疏矩阵</li><li>形状元组 (创建一个空矩阵)</li></ul></li><li>灵活切片、高效改变稀疏结构</li><li>由于是基于行的，算术和行切片慢</li><li>用途 :<ul><li>当稀疏模式并不是已知的逻辑或改变</li><li>例子：从一个文本文件读取稀疏矩阵 ##### 2.5.2.2.2.1 示例</li></ul></li><li>创建一个空的LIL矩阵 :</li></ul><p>In [2]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx &#x3D; sparse.lil_matrix((4, 5))</span><br></pre></td></tr></table></figure><ul><li>准备随机数据:</li></ul><p>In [4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from numpy.random import rand</span><br><span class="line">data &#x3D; np.round(rand(2, 3))</span><br><span class="line">data</span><br></pre></td></tr></table></figure><p>Out[4]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.,  0.,  0.],</span><br><span class="line">       [ 1.,  0.,  0.]])</span><br></pre></td></tr></table></figure><ul><li>使用象征所以分配数据:</li></ul><p>In [6]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mtx[:2, [1, 2, 3]] &#x3D; data</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure><p>Out[6]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;4x5 sparse matrix of type &#39;&lt;type &#39;numpy.float64&#39;&gt;&#39;</span><br><span class="line">    with 3 stored elements in LInked List format&gt;</span><br></pre></td></tr></table></figure><p>In [7]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print mtx</span><br><span class="line">  (0, 1)    1.0</span><br><span class="line">  (0, 3)    1.0</span><br><span class="line">  (1, 2)    1.0</span><br></pre></td></tr></table></figure><p>In [8]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure><p>Out[8]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">matrix([[ 0.,  1.,  0.,  1.,  0.],</span><br><span class="line">        [ 0.,  0.,  1.,  0.,  0.],</span><br><span class="line">        [ 0.,  0.,  0.,  0.,  0.],</span><br><span class="line">        [ 0.,  0.,  0.,  0.,  0.]])</span><br></pre></td></tr></table></figure><p>In [9]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.toarray()</span><br></pre></td></tr></table></figure><p>Out[9]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.,  1.,  0.,  1.,  0.],</span><br><span class="line">       [ 0.,  0.,  1.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  0.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  0.,  0.,  0.]])</span><br></pre></td></tr></table></figure><p>更多的切片和索引:</p><p>In [10]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mtx &#x3D; sparse.lil_matrix([[0, 1, 2, 0], [3, 0, 1, 0], [1, 0, 0, 1]])</span><br><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure><p>Out[10]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">matrix([[0, 1, 2, 0],</span><br><span class="line">        [3, 0, 1, 0],</span><br><span class="line">        [1, 0, 0, 1]])</span><br></pre></td></tr></table></figure><p>In [11]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print mtx</span><br><span class="line">  (0, 1)    1</span><br><span class="line">  (0, 2)    2</span><br><span class="line">  (1, 0)    3</span><br><span class="line">  (1, 2)    1</span><br><span class="line">  (2, 0)    1</span><br><span class="line">  (2, 3)    1</span><br></pre></td></tr></table></figure><p>In [12]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx[:2, :]</span><br></pre></td></tr></table></figure><p>Out[12]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;2x4 sparse matrix of type &#39;&lt;type &#39;numpy.int64&#39;&gt;&#39;</span><br><span class="line">    with 4 stored elements in LInked List format&gt;</span><br></pre></td></tr></table></figure><p>In [13]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx[:2, :].todense()</span><br></pre></td></tr></table></figure><p>Out[13]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">matrix([[0, 1, 2, 0],</span><br><span class="line">        [3, 0, 1, 0]])</span><br></pre></td></tr></table></figure><p>In [14]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx[1:2, [0,2]].todense()</span><br></pre></td></tr></table></figure><p>Out[14]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matrix([[3, 1]])</span><br></pre></td></tr></table></figure><p>In [15]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure><p>Out[15]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">matrix([[0, 1, 2, 0],</span><br><span class="line">        [3, 0, 1, 0],</span><br><span class="line">        [1, 0, 0, 1]])</span><br></pre></td></tr></table></figure><h4 id="2-5-2-2-3-值的字典格式-DOK"><a href="#2-5-2-2-3-值的字典格式-DOK" class="headerlink" title="2.5.2.2.3 值的字典格式 (DOK))"></a>2.5.2.2.3 值的字典格式 (DOK))</h4><ul><li>Python字典的子类<ul><li>键是 (行, 列) 索引元组 (不允许重复的条目)</li><li>值是对应的非零值</li></ul></li><li>高效增量构建稀疏矩阵</li><li>构建器支持:<ul><li>密集矩阵 (数组)</li><li>稀疏矩阵</li><li>形状元组 (创建空矩阵)</li></ul></li><li>高效 O(1) 对单个元素的访问</li><li>灵活索引，改变稀疏结构是高效</li><li>一旦创建完成后可以被高效转换为coo_matrix</li><li>算术很慢 (循环用<code>dict.iteritems()</code>)</li><li>用法:<ul><li>当稀疏模式是未知的假设或改变时</li></ul></li></ul><h5 id="2-5-2-2-3-1-示例"><a href="#2-5-2-2-3-1-示例" class="headerlink" title="2.5.2.2.3.1 示例"></a>2.5.2.2.3.1 示例</h5><ul><li>逐个元素创建一个DOK矩阵:</li></ul><p>In [16]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mtx &#x3D; sparse.dok_matrix((5, 5), dtype&#x3D;np.float64)</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure><p>Out[16]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;5x5 sparse matrix of type &#39;&lt;type &#39;numpy.float64&#39;&gt;&#39;</span><br><span class="line">    with 0 stored elements in Dictionary Of Keys format&gt;</span><br></pre></td></tr></table></figure><p>In [17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for ir in range(5):</span><br><span class="line">    for ic in range(5):</span><br><span class="line">        mtx[ir, ic] &#x3D; 1.0 * (ir !&#x3D; ic)</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure><p>Out[17]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;5x5 sparse matrix of type &#39;&lt;type &#39;numpy.float64&#39;&gt;&#39;</span><br><span class="line">    with 20 stored elements in Dictionary Of Keys format&gt;</span><br></pre></td></tr></table></figure><p>In [18]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure><p>Out[18]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">matrix([[ 0.,  1.,  1.,  1.,  1.],</span><br><span class="line">        [ 1.,  0.,  1.,  1.,  1.],</span><br><span class="line">        [ 1.,  1.,  0.,  1.,  1.],</span><br><span class="line">        [ 1.,  1.,  1.,  0.,  1.],</span><br><span class="line">        [ 1.,  1.,  1.,  1.,  0.]])</span><br></pre></td></tr></table></figure><ul><li>切片与索引:</li></ul><p>In [19]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx[1, 1]</span><br></pre></td></tr></table></figure><p>Out[19]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.0</span><br></pre></td></tr></table></figure><p>In [20]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx[1, 1:3]</span><br></pre></td></tr></table></figure><p>Out[20]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;1x2 sparse matrix of type &#39;&lt;type &#39;numpy.float64&#39;&gt;&#39;</span><br><span class="line">    with 1 stored elements in Dictionary Of Keys format&gt;</span><br></pre></td></tr></table></figure><p>In [21]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx[1, 1:3].todense()</span><br></pre></td></tr></table></figure><p>Out[21]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matrix([[ 0.,  1.]])</span><br></pre></td></tr></table></figure><p>In [22]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx[[2,1], 1:3].todense()</span><br></pre></td></tr></table></figure><p>Out[22]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">matrix([[ 1.,  0.],</span><br><span class="line">        [ 0.,  1.]])</span><br></pre></td></tr></table></figure><h4 id="2-5-2-2-4-座标格式-COO"><a href="#2-5-2-2-4-座标格式-COO" class="headerlink" title="2.5.2.2.4 座标格式 (COO))"></a>2.5.2.2.4 座标格式 (COO))</h4><ul><li>也被称为 ‘ijv’ 或 ‘triplet’ 格式<ul><li>三个NumPy数组: row, col, data</li><li><code>data[i]</code>是在 (row[i], col[i]) 位置的值</li><li>允许重复值</li></ul></li><li><code>\_data\_matrix</code>的子类 (带有<code>data</code>属性的稀疏矩阵类)</li><li>构建稀疏矩阵的高速模式</li><li>构建器接受:<ul><li>密集矩阵 (数组)</li><li>稀疏矩阵</li><li>形状元组 (创建空数组)</li><li><code>(data, ij)</code>元组</li></ul></li><li>与CSR/CSC格式非常快的互相转换</li><li>快速的矩阵 * 向量 (sparsetools)</li><li>快速而简便的逐项操作<ul><li>直接操作数据数组 (快速NumPy机制)</li></ul></li><li>没有切片，没有算术 (直接)</li><li>使用:<ul><li>在各种稀疏格式间的灵活转换</li><li>当转化到其他形式 (通常是 CSR 或 CSC), 重复的条目被加总到一起<ul><li>有限元素矩阵的快速高效创建</li></ul></li></ul></li></ul><h5 id="2-5-2-2-4-1-示例"><a href="#2-5-2-2-4-1-示例" class="headerlink" title="2.5.2.2.4.1 示例"></a>2.5.2.2.4.1 示例</h5><ul><li>创建空的COO矩阵:</li></ul><p>In [23]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mtx &#x3D; sparse.coo_matrix((3, 4), dtype&#x3D;np.int8)</span><br><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure><p>Out[23]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">matrix([[0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0]], dtype&#x3D;int8)</span><br></pre></td></tr></table></figure><ul><li>用 (data, ij) 元组创建:</li></ul><p>In [24]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">row &#x3D; np.array([0, 3, 1, 0])</span><br><span class="line">col &#x3D; np.array([0, 3, 1, 2])</span><br><span class="line">data &#x3D; np.array([4, 5, 7, 9])</span><br><span class="line">mtx &#x3D; sparse.coo_matrix((data, (row, col)), shape&#x3D;(4, 4))</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure><p>Out[24]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;4x4 sparse matrix of type &#39;&lt;type &#39;numpy.int64&#39;&gt;&#39;</span><br><span class="line">    with 4 stored elements in COOrdinate format&gt;</span><br></pre></td></tr></table></figure><p>In [25]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure><p>Out[25]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">matrix([[4, 0, 9, 0],</span><br><span class="line">        [0, 7, 0, 0],</span><br><span class="line">        [0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 5]])</span><br></pre></td></tr></table></figure><h4 id="2-5-2-2-5-压缩稀疏行格式-CSR"><a href="#2-5-2-2-5-压缩稀疏行格式-CSR" class="headerlink" title="2.5.2.2.5 压缩稀疏行格式 (CSR))"></a>2.5.2.2.5 压缩稀疏行格式 (CSR))</h4><ul><li><p>面向行</p><ul><li>三个Numpy数组:</li></ul></li></ul><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">indices</span><br></pre></td></tr></table></figure>,<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">indptr</span><br></pre></td></tr></table></figure>,<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>- `indices`是列索引的数组- `data`是对应的非零值数组- `indptr`指向行开始的所以和数据- 长度是`n_row + 1`, 最后一个项目 = 值数量 = `indices`和`data`的长度- i-th行的非零值是列索引为`indices[indptr[i]:indptr[i+1]]`的`data[indptr[i]:indptr[i+1]]`- 项目 (i, j) 可以通过`data[indptr[i]+k]`, k是j在`indices[indptr[i]:indptr[i+1]]`的位置来访问</code></pre><ul><li><p><code>_cs_matrix</code> (常规 CSR/CSC 功能) 的子类</p></li><li><p><code>_data_matrix</code> (带有<code>data</code>属性的稀疏矩阵类) 的子类</p></li></ul><ul><li><p>快速矩阵向量相乘和其他算术 (sparsetools)</p></li><li><p>构建器接受:</p><ul><li>密集矩阵 (数组)</li><li>稀疏矩阵</li><li>形状元组 (创建空矩阵)</li><li><code>(data, ij)</code> 元组</li><li><code>(data, indices, indptr)</code> 元组</li></ul></li><li><p>高效行切片，面向行的操作</p></li><li><p>较慢的列切片，改变稀疏结构代价昂贵</p></li><li><p>用途:</p><ul><li>实际计算 (大多数线性求解器都支持这个格式)</li></ul></li></ul><h5 id="2-5-2-2-5-1-示例"><a href="#2-5-2-2-5-1-示例" class="headerlink" title="2.5.2.2.5.1 示例"></a>2.5.2.2.5.1 示例</h5><ul><li>创建空的CSR矩阵:</li></ul><p>In [26]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mtx &#x3D; sparse.csr_matrix((3, 4), dtype&#x3D;np.int8)</span><br><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure><p>Out[26]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">matrix([[0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0]], dtype&#x3D;int8)</span><br></pre></td></tr></table></figure><ul><li>用<code>(data, ij)</code>元组创建:</li></ul><p>In [27]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">row &#x3D; np.array([0, 0, 1, 2, 2, 2])</span><br><span class="line">col &#x3D; np.array([0, 2, 2, 0, 1, 2])</span><br><span class="line">data &#x3D; np.array([1, 2, 3, 4, 5, 6])</span><br><span class="line">mtx &#x3D; sparse.csr_matrix((data, (row, col)), shape&#x3D;(3, 3))</span><br><span class="line">mtx</span><br></pre></td></tr></table></figure><p>Out[27]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;3x3 sparse matrix of type &#39;&lt;type &#39;numpy.int64&#39;&gt;&#39;</span><br><span class="line">    with 6 stored elements in Compressed Sparse Row format&gt;</span><br></pre></td></tr></table></figure><p>In [28]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure><p>Out[28]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">matrix([[1, 0, 2],</span><br><span class="line">        [0, 0, 3],</span><br><span class="line">        [4, 5, 6]])</span><br></pre></td></tr></table></figure><p>In [29]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.data</span><br></pre></td></tr></table></figure><p>Out[29]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1, 2, 3, 4, 5, 6])</span><br></pre></td></tr></table></figure><p>In [30]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.indices</span><br></pre></td></tr></table></figure><p>Out[30]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0, 2, 2, 0, 1, 2], dtype&#x3D;int32)</span><br></pre></td></tr></table></figure><p>In [31]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mtx.indptr</span><br></pre></td></tr></table></figure><p>Out[31]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0, 2, 3, 6], dtype&#x3D;int32)</span><br></pre></td></tr></table></figure><p>用<code>(data, indices, indptr)</code>元组创建:</p><p>In [32]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; np.array([1, 2, 3, 4, 5, 6])</span><br><span class="line">indices &#x3D; np.array([0, 2, 2, 0, 1, 2])</span><br><span class="line">indptr &#x3D; np.array([0, 2, 3, 6])</span><br><span class="line">mtx &#x3D; sparse.csr_matrix((data, indices, indptr), shape&#x3D;(3, 3))</span><br><span class="line">mtx.todense()</span><br></pre></td></tr></table></figure><p>Out[32]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">matrix([[1, 0, 2],</span><br><span class="line">        [0, 0, 3],</span><br><span class="line">        [4, 5, 6]])</span><br></pre></td></tr></table></figure><h4 id="2-5-2-2-6-压缩稀疏列格式-CSC"><a href="#2-5-2-2-6-压缩稀疏列格式-CSC" class="headerlink" title="2.5.2.2.6 压缩稀疏列格式 (CSC))"></a>2.5.2.2.6 压缩稀疏列格式 (CSC))</h4><ul><li><p>面向列</p><ul><li><p>三个Numpy数组: <code>indices</code>、<code>indptr</code>、<code>data</code></p></li><li><p><code>indices</code>是行索引的数组</p></li><li><p><code>data</code>是对应的非零值</p></li><li><p><code>indptr</code>指向<code>indices</code>和<code>data</code>开始的列</p></li><li><p>长度是<code>n_col + 1</code>, 最后一个条目 = 值数量 = <code>indices</code>和<code>data</code>的长度</p></li><li><p>第i列的非零值是行索引为<code>indices[indptr[i]:indptr[i+1]]</code>的<code>data[indptr[i]:indptr[i+1]]</code></p></li><li><p>项目 (i, j) 可以作为<code>data[indptr[j]+k]</code>访问, k是i在<code>indices[indptr[j]:indptr[j+1]]</code>的位置</p></li><li><p>```<br>_cs_matrix</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    的子类 (通用的 CSR&#x2F;CSC 功能性)</span><br><span class="line"></span><br><span class="line">    - &#96;_data_matrix&#96;的子类 (带有&#96;data&#96;属性的稀疏矩阵类)</span><br><span class="line"></span><br><span class="line">- 快速的矩阵和向量相乘及其他数学 (sparsetools)</span><br><span class="line"></span><br><span class="line">- 构建器接受：</span><br><span class="line"></span><br><span class="line">  - 密集矩阵 (数组)</span><br><span class="line">  - 稀疏矩阵</span><br><span class="line">  - 形状元组 (创建空矩阵)</span><br><span class="line">  - &#96;(data, ij)&#96;元组</span><br><span class="line">  - &#96;(data, indices, indptr)&#96;元组</span><br><span class="line"></span><br><span class="line">- 高效列切片、面向列的操作</span><br><span class="line"></span><br><span class="line">- 较慢的行切片、改变稀疏结构代价昂贵</span><br><span class="line"></span><br><span class="line">- 用途:</span><br><span class="line"></span><br><span class="line">  - 实际计算 (巨大多数线性求解器支持这个格式)</span><br><span class="line"></span><br><span class="line">##### 2.5.2.2.6.1 示例</span><br><span class="line"></span><br><span class="line">- 创建空CSC矩阵:</span><br><span class="line"></span><br><span class="line">In [33]:</span><br></pre></td></tr></table></figure><p>mtx = sparse.csc_matrix((3, 4), dtype=np.int8)<br>mtx.todense()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[33]:</span><br></pre></td></tr></table></figure><p>matrix([[0, 0, 0, 0],</p><pre><code>[0, 0, 0, 0],[0, 0, 0, 0]], dtype=int8)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 用&#96;(data, ij)&#96;元组创建:</span><br><span class="line"></span><br><span class="line">In [34]:</span><br></pre></td></tr></table></figure><p>row = np.array([0, 0, 1, 2, 2, 2])<br>col = np.array([0, 2, 2, 0, 1, 2])<br>data = np.array([1, 2, 3, 4, 5, 6])<br>mtx = sparse.csc_matrix((data, (row, col)), shape=(3, 3))<br>mtx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[34]:</span><br></pre></td></tr></table></figure><p><3x3 sparse matrix of type '<type 'numpy.int64'>'with 6 stored elements in Compressed Sparse Column format></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">In [35]:</span><br></pre></td></tr></table></figure><p>mtx.todense()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[35]:</span><br></pre></td></tr></table></figure><p>matrix([[1, 0, 2],</p><pre><code>[0, 0, 3],[4, 5, 6]])</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">In [36]:</span><br></pre></td></tr></table></figure><p>mtx.data</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[36]:</span><br></pre></td></tr></table></figure><p>array([1, 4, 5, 2, 3, 6])</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">In [37]:</span><br></pre></td></tr></table></figure><p>mtx.indices</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[37]:</span><br></pre></td></tr></table></figure><p>array([0, 2, 2, 0, 1, 2], dtype=int32)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">In [38]:</span><br></pre></td></tr></table></figure><p>mtx.indptr</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[38]:</span><br></pre></td></tr></table></figure><p>array([0, 2, 3, 6], dtype=int32)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 用&#96;(data, indices, indptr)&#96;元组创建:</span><br><span class="line"></span><br><span class="line">In [39]:</span><br></pre></td></tr></table></figure><p>data = np.array([1, 4, 5, 2, 3, 6])<br>indices = np.array([0, 2, 2, 0, 1, 2])<br>indptr = np.array([0, 2, 3, 6])<br>mtx = sparse.csc_matrix((data, indices, indptr), shape=(3, 3))<br>mtx.todense()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[39]:</span><br></pre></td></tr></table></figure><p>matrix([[1, 0, 2],</p><pre><code>[0, 0, 3],[4, 5, 6]])</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.5.2.2.7 块压缩行格式 (BSR))</span><br><span class="line"></span><br><span class="line">- 本质上，CSR带有密集的固定形状的子矩阵而不是纯量的项目</span><br><span class="line"></span><br><span class="line">  - 块大小&#96;(R, C)&#96;必须可以整除矩阵的形状&#96;(M, N)&#96;</span><br><span class="line"></span><br><span class="line">  - 三个Numpy数组:</span><br></pre></td></tr></table></figure><p>indices</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">、</span><br></pre></td></tr></table></figure><p>indptr</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">、</span><br></pre></td></tr></table></figure><p>data</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    - &#96;indices&#96;是每个块列索引的数组</span><br><span class="line">    - &#96;data&#96;是形状为(nnz, R, C)对应的非零值</span><br><span class="line">    - ...</span><br><span class="line"></span><br><span class="line">  - &#96;_cs_matrix&#96;的子类 (通用的CSR&#x2F;CSC功能性)</span><br><span class="line"></span><br><span class="line">  - &#96;_data_matrix&#96;的子类 (带有&#96;data&#96;属性的稀疏矩阵类)</span><br><span class="line"></span><br><span class="line">- 快速矩阵向量相乘和其他的算术 (sparsetools)</span><br><span class="line"></span><br><span class="line">- 构建器接受:</span><br><span class="line"></span><br><span class="line">  - 密集矩阵 (数组)</span><br><span class="line">  - 稀疏矩阵</span><br><span class="line">  - 形状元组 (创建空的矩阵)</span><br><span class="line">  - &#96;(data, ij)&#96;元组</span><br><span class="line">  - &#96;(data, indices, indptr)&#96;元组</span><br><span class="line"></span><br><span class="line">- 许多对于带有密集子矩阵的稀疏矩阵算术操作比CSR更高效很多</span><br><span class="line"></span><br><span class="line">- 用途:</span><br><span class="line"></span><br><span class="line">  - 类似CSR</span><br><span class="line">  - 有限元素向量值离散化 ##### 2.5.2.2.7.1 示例</span><br><span class="line"></span><br><span class="line">- 创建空的&#96;(1, 1)&#96;块大小的（类似CSR...）的BSR矩阵:</span><br><span class="line"></span><br><span class="line">In [40]:</span><br></pre></td></tr></table></figure><p>mtx = sparse.bsr_matrix((3, 4), dtype=np.int8)<br>mtx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[40]:</span><br></pre></td></tr></table></figure><p><3x4 sparse matrix of type '<type 'numpy.int8'>'with 0 stored elements (blocksize = 1x1) in Block Sparse Row format></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">In [41]:</span><br></pre></td></tr></table></figure><p>mtx.todense()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[41]:</span><br></pre></td></tr></table></figure><p>matrix([[0, 0, 0, 0],</p><pre><code>[0, 0, 0, 0],[0, 0, 0, 0]], dtype=int8)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 创建块大小&#96;(3, 2)&#96;的空BSR矩阵:</span><br><span class="line"></span><br><span class="line">In [42]:</span><br></pre></td></tr></table></figure><p>mtx = sparse.bsr_matrix((3, 4), blocksize=(3, 2), dtype=np.int8)<br>mtx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[42]:</span><br></pre></td></tr></table></figure><3x4 sparse matrix of type '<type 'numpy.int8'>'with 0 stored elements (blocksize = 3x2) in Block Sparse Row format></li></ul></li><li><p>一个bug?</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 用&#96;(1, 1)&#96;块大小 (类似 CSR...)&#96;(data, ij)&#96;的元组创建:</span><br><span class="line"></span><br><span class="line">In [43]:</span><br></pre></td></tr></table></figure><p>row = np.array([0, 0, 1, 2, 2, 2])<br>col = np.array([0, 2, 2, 0, 1, 2])<br>data = np.array([1, 2, 3, 4, 5, 6])<br>mtx = sparse.bsr_matrix((data, (row, col)), shape=(3, 3))<br>mtx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[43]:</span><br></pre></td></tr></table></figure><p><3x3 sparse matrix of type '<type 'numpy.int64'>'  with 6 stored elements (blocksize = 1x1) in Block Sparse Row format></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">In [44]:</span><br></pre></td></tr></table></figure><p>mtx.todense()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[44]:</span><br></pre></td></tr></table></figure><p>matrix([[1, 0, 2],</p><pre><code>  [0, 0, 3],  [4, 5, 6]])</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">In [45]:</span><br></pre></td></tr></table></figure><p>mtx.indices</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[45]:</span><br></pre></td></tr></table></figure><p>array([0, 2, 2, 0, 1, 2], dtype=int32)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">In [46]:</span><br></pre></td></tr></table></figure><p>mtx.indptr</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[46]:</span><br></pre></td></tr></table></figure><p>array([0, 2, 3, 6], dtype=int32)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 用&#96;(2, 1)&#96;块大小&#96;(data, indices, indptr)&#96;的元组创建:</span><br><span class="line"></span><br><span class="line">In [47]:</span><br></pre></td></tr></table></figure><p>indptr = np.array([0, 2, 3, 6])<br>indices = np.array([0, 2, 2, 0, 1, 2])<br>data = np.array([1, 2, 3, 4, 5, 6]).repeat(4).reshape(6, 2, 2)<br>mtx = sparse.bsr_matrix((data, indices, indptr), shape=(6, 6))<br>mtx.todense()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[47]:</span><br></pre></td></tr></table></figure><p>matrix([[1, 1, 0, 0, 2, 2],</p><pre><code>  [1, 1, 0, 0, 2, 2],  [0, 0, 0, 0, 3, 3],  [0, 0, 0, 0, 3, 3],  [4, 4, 5, 5, 6, 6],  [4, 4, 5, 5, 6, 6]])</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">In [48]:</span><br></pre></td></tr></table></figure><p>data</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Out[48]:</span><br></pre></td></tr></table></figure><p>array([[[1, 1],</p><pre><code>  [1, 1]], [[2, 2],  [2, 2]], [[3, 3],  [3, 3]], [[4, 4],  [4, 4]], [[5, 5],  [5, 5]], [[6, 6],  [6, 6]]])</code></pre><p>```</p></li></ul><h3 id="2-5-2-3-总结"><a href="#2-5-2-3-总结" class="headerlink" title="2.5.2.3 总结"></a>2.5.2.3 总结</h3><p>存储机制的总结</p><div class="table-container"><table><thead><tr><th>格式</th><th>矩阵 * 向量</th><th>提取项目</th><th>灵活提取</th><th>设置项目</th><th>灵活设置</th><th>求解器</th><th>备注</th></tr></thead><tbody><tr><td>DIA</td><td>sparsetools</td><td>.</td><td>.</td><td>.</td><td>.</td><td>迭代</td><td>有数据数组，专门化</td></tr><tr><td>LIL</td><td>通过 CSR</td><td>是</td><td>是</td><td>是</td><td>是</td><td>迭代</td><td>通过CSR的算术, 增量构建</td></tr><tr><td>DOK</td><td>python</td><td>是</td><td>只有一个轴</td><td>是</td><td>是</td><td>迭代</td><td><code>O(1)</code>条目访问, 增量构建</td></tr><tr><td>COO</td><td>sparsetools</td><td>.</td><td>.</td><td>.</td><td>.</td><td>迭代</td><td>有数据数组, 便利的快速转换</td></tr><tr><td>CSR</td><td>sparsetools</td><td>是</td><td>是</td><td>慢</td><td>.</td><td>任何</td><td>有数据数组, 快速以行为主的操作</td></tr><tr><td>CSC</td><td>sparsetools</td><td>是</td><td>是</td><td>慢</td><td>.</td><td>任何</td><td>有数据数组, 快速以列为主的操作</td></tr><tr><td>BSR</td><td>sparsetools</td><td>.</td><td>.</td><td>.</td><td>.</td><td>专门化</td><td>有数据数组，专门化</td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;2-5-
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
      <category term="scikit-learn" scheme="http://yoursite.com/tags/scikit-learn/"/>
    
      <category term="scipy" scheme="http://yoursite.com/tags/scipy/"/>
    
  </entry>
  
  <entry>
    <title>sklearn的非数值进行数值性编码与sklearn的库进行准确率的计算</title>
    <link href="http://yoursite.com/2020/03/04/2020-3-3-skearn_learning-2020/"/>
    <id>http://yoursite.com/2020/03/04/2020-3-3-skearn_learning-2020/</id>
    <published>2020-03-04T04:08:25.000Z</published>
    <updated>2020-03-04T06:24:57.736Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hey"><a href="#Hey" class="headerlink" title="Hey"></a>Hey</h1><blockquote><p>Machine Learning notes</p></blockquote><h1 id="skearn的非数值进行数值性编码与sklearn的库进行准确率的计算"><a href="#skearn的非数值进行数值性编码与sklearn的库进行准确率的计算" class="headerlink" title="skearn的非数值进行数值性编码与sklearn的库进行准确率的计算"></a>skearn的非数值进行数值性编码与sklearn的库进行准确率的计算</h1><p>1.可以使用python自带的skearn对result的非数值进行数值性编码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">y = LabelEncoder().fit_transform(data[<span class="number">4</span>])</span><br></pre></td></tr></table></figure><p>2.可以使用sklearn的库进行准确率的计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_socre</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=<span class="number">0.7</span>, random_state=<span class="number">1</span>)</span><br><span class="line">model = DecisionTreeClassifier(criterion=<span class="string">'entorpy'</span>)</span><br><span class="line">model.fit(x_train,y_train)</span><br><span class="line">y_test_hat = model.predict(x_test)</span><br><span class="line"><span class="comment"># 第一种方法</span></span><br><span class="line">print(<span class="string">"accuracy_score"</span>,accuracy_socre(y_test_hat,y_test))</span><br><span class="line"><span class="comment"># 第二种方法</span></span><br><span class="line">y_test = y_test.reshape(<span class="number">-1</span>)<span class="comment"># 变成一维</span></span><br><span class="line">result = (y_test_hat==y_test)</span><br><span class="line">acc = np.mean(result)</span><br><span class="line">print(<span class="string">f"准确率:<span class="subst">&#123;<span class="number">100</span>*acc&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hey&quot;&gt;&lt;a href=&quot;#Hey&quot; class=&quot;headerlink&quot; title=&quot;Hey&quot;&gt;&lt;/a&gt;Hey&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Machine Learning notes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;skea
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="scikit-learn" scheme="http://yoursite.com/tags/scikit-learn/"/>
    
      <category term="encoding" scheme="http://yoursite.com/tags/encoding/"/>
    
  </entry>
  
</feed>
